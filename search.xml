<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Python爬虫（五）多线程基本原理</title>
    <url>/posts/4cff7d80.html</url>
    <content><![CDATA[<p>我们知道，在一台计算机中，我们可以同时打开许多软件，比如同时浏览网页、听音乐、打字等等，看似非常正常。但仔细想想，为什么计算机可以做到这么多软件同时运行呢？这就涉及到计算机中的两个重要概念：多进程和多线程了。</p>
<p>同样，在编写爬虫程序的时候，为了提高爬取效率，我们可能想同时运行多个爬虫任务。这里同样需要涉及多进程和多线程的知识。</p>
<p>本课时，我们就先来了解一下多线程的基本原理，以及在 Python 中如何实现多线程。</p>
<h1 id="多线程的含义"><a href="#多线程的含义" class="headerlink" title="多线程的含义"></a>多线程的含义</h1><p>说起多线程，就不得不先说什么是线程。然而想要弄明白什么是线程，又不得不先说什么是进程。</p>
<p>进程我们可以理解为是一个<strong>可以独立运行的程序单位</strong>，比如打开一个浏览器，这就开启了一个浏览器进程；打开一个文本编辑器，这就开启了一个文本编辑器进程。但一个进程中是可以同时处理很多事情的，比如在浏览器中，我们可以在多个选项卡中打开多个页面，有的页面在播放音乐，有的页面在播放视频，有的网页在播放动画，它们可以同时运行，互不干扰。为什么能同时做到同时运行这么多的任务呢？这里就需要引出线程的概念了，其实这一个个任务，实际上就对应着一个个线程的执行。</p>
<p>而进程呢？它就是<strong>线程的集合</strong>，进程就是由一个或多个线程构成的，<strong>线程是操作系统进行运算调度的最小单位</strong>，是进程中的一个最小运行单元。比如上面所说的浏览器进程，其中的播放音乐就是一个线程，播放视频也是一个线程，当然其中还有很多其他的线程在同时运行，这些线程的并发或并行执行最后使得整个浏览器可以同时运行这么多的任务。</p>
<p>了解了线程的概念，多线程就很容易理解了，<strong>多线程就是一个进程中同时执行多个线程</strong>，前面所说的浏览器的情景就是典型的多线程执行。</p>
<h1 id="并发和并行"><a href="#并发和并行" class="headerlink" title="并发和并行"></a>并发和并行</h1><p>说到多进程和多线程，这里就需要再讲解两个概念，那就是并发和并行。我们知道，一个程序在计算机中运行，其底层是处理器通过运行一条条的指令来实现的。</p>
<p><strong>并发</strong>，英文叫作 concurrency。它是指同一时刻只能有一条指令执行，但是多个线程的对应的指令被快速轮换地执行。比如一个处理器，它先执行线程 A 的指令一段时间，再执行线程 B 的指令一段时间，再切回到线程 A 执行一段时间。</p>
<p>由于处理器执行指令的速度和切换的速度非常非常快，人完全感知不到计算机在这个过程中有多个线程切换上下文执行的操作，这就使得宏观上看起来多个线程在同时运行。但<strong>微观上只是这个处理器在连续不断地在多个线程之间切换和执行</strong>，每个线程的执行一定会占用这个处理器一个时间片段，<strong>同一时刻，其实只有一个线程在执行</strong>。</p>
<p><strong>并行</strong>，英文叫作 parallel。它是指<strong>同一时刻，有多条指令在多个处理器上同时执行，并行必须要依赖于多个处理器</strong>。不论是从宏观上还是微观上，多个线程都是在同一时刻一起执行的。</p>
<p>并行只能在多处理器系统中存在，<strong>如果我们的计算机处理器只有一个核，那就不可能实现并行</strong>。而并发在单处理器和多处理器系统中都是可以存在的，因为仅靠一个核，就可以实现并发。</p>
<p>举个例子，比如系统处理器需要同时运行多个线程。如果系统处理器只有一个核，那它只能通过并发的方式来运行这些线程。如果系统处理器有多个核，当一个核在执行一个线程时，另一个核可以执行另一个线程，这样这两个线程就实现了并行执行，当然其他的线程也可能和另外的线程处在同一个核上执行，它们之间就是并发执行。具体的执行方式，就取决于操作系统的调度了。</p>
<h1 id="多线程适用场景"><a href="#多线程适用场景" class="headerlink" title="多线程适用场景"></a>多线程适用场景</h1><p>在一个程序进程中，有一些操作是比较耗时或者需要等待的，比如等待数据库的查询结果的返回，等待网页结果的响应。如果使用单线程，处理器必须要等到这些操作完成之后才能继续往下执行其他操作，而这个线程在等待的过程中，处理器明显是可以来执行其他的操作的。<strong>如果使用多线程，处理器就可以在某个线程等待的时候，去执行其他的线程，从而从整体上提高执行效率</strong>。</p>
<p>像上述场景，线程在执行过程中很多情况下是需要等待的。比如网络爬虫就是一个非常典型的例子，爬虫在向服务器发起请求之后，<strong>有一段时间必须要等待服务器的响应返回</strong>，这种任务就属于 IO 密集型任务。对于这种任务，如果我们启用多线程，处理器就可以在某个线程等待的过程中去处理其他的任务，从而提高整体的爬取效率。</p>
<p>但并不是所有的任务都是 IO 密集型任务，还有一种任务叫作<strong>计算密集型任务</strong>，也可以称之为 CPU 密集型任务。顾名思义，就是任务的运行一直需要处理器的参与。此时如果我们开启了多线程，一个处理器从一个计算密集型任务切换到切换到另一个计算密集型任务上去，处理器依然不会停下来，始终会忙于计算，这样并不会节省总体的时间，因为需要处理的任务的计算总量是不变的。如果线程数目过多，反而还会在线程切换的过程中多耗费一些时间，整体效率会变低。</p>
<p>所以，<strong>如果任务不全是计算密集型任务，我们可以使用多线程来提高程序整体的执行效率</strong>。尤其对于网络爬虫这种 IO 密集型任务来说，使用多线程会大大提高程序整体的爬取效率。</p>
<h1 id="Python-实现多线程"><a href="#Python-实现多线程" class="headerlink" title="Python 实现多线程"></a>Python 实现多线程</h1><p>在 Python 中，实现多线程的模块叫作 <code>threading</code>，是 Python 自带的模块。下面我们来了解下使用 <code>threading</code> 实现多线程的方法。</p>
<h2 id="Thread-直接创建子线程"><a href="#Thread-直接创建子线程" class="headerlink" title="Thread 直接创建子线程"></a>Thread 直接创建子线程</h2><p>首先，我们可以使用 <code>Thread</code> 类来创建一个线程，创建时需要指定 target 参数为运行的方法名称，如果被调用的方法需要传入额外的参数，则可以通过 <code>Thread</code> 的 <code>args</code> 参数来指定。示例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">target</span><span class="params">(second)</span>:</span></span><br><span class="line">    print(<span class="string">f'Threading <span class="subst">&#123;threading.current_thread().name&#125;</span> is running'</span>)</span><br><span class="line">    print(<span class="string">f'Threading <span class="subst">&#123;threading.current_thread().name&#125;</span> sleep <span class="subst">&#123;second&#125;</span>s'</span>)</span><br><span class="line">    time.sleep(second)</span><br><span class="line">    print(<span class="string">f'Threading <span class="subst">&#123;threading.current_thread().name&#125;</span> is ended'</span>)</span><br><span class="line">print(<span class="string">f'Threading <span class="subst">&#123;threading.current_thread().name&#125;</span> is running'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">5</span>]:</span><br><span class="line">    thread = threading.Thread(target=target, args=[i])</span><br><span class="line">    thread.start()</span><br><span class="line">print(<span class="string">f'Threading <span class="subst">&#123;threading.current_thread().name&#125;</span> is ended'</span>)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Threading MainThread is running</span><br><span class="line">Threading Thread-1 is running</span><br><span class="line">Threading Thread-1 sleep 1s</span><br><span class="line">Threading Thread-2 is running</span><br><span class="line">Threading Thread-2 sleep 5s</span><br><span class="line">Threading MainThread is ended</span><br><span class="line">Threading Thread-1 is ended</span><br><span class="line">Threading Thread-2 is ended</span><br></pre></td></tr></table></figure>
<p>在这里我们首先声明了一个方法，叫作 <code>target</code>，它接收一个参数为 <code>second</code>，通过方法的实现可以发现，这个方法其实就是执行了一个 <code>time.sleep</code> 休眠操作，<code>second</code> 参数就是休眠秒数，其前后都 <code>print</code> 了一些内容，其中线程的名字我们通过 <code>threading.current_thread().name</code> 来获取出来，如果是主线程的话，其值就是 <code>MainThread</code>，如果是子线程的话，其值就是 <code>Thread-*</code>。</p>
<p>然后我们通过 <code>Thead</code> 类新建了两个线程，<code>target</code> 参数就是刚才我们所定义的方法名，<code>args</code> 以列表的形式传递。两次循环中，这里 i 分别就是 1 和 5，这样两个线程就分别休眠 1 秒和 5 秒，声明完成之后，我们调用 <code>start</code> 方法即可开始线程的运行。</p>
<p>观察结果我们可以发现，这里一共产生了三个线程，分别是主线程 <code>MainThread</code> 和两个子线程 <code>Thread-1</code>、<code>Thread-2</code>。另外我们观察到，主线程首先运行结束，紧接着 <code>Thread-1</code>、<code>Thread-2</code>才接连运行结束，分别间隔了 1 秒和 4 秒。这说明主线程并没有等待子线程运行完毕才结束运行，而是直接退出了，有点不符合常理。</p>
<p>如果我们想要主线程等待子线程运行完毕之后才退出，可以让每个子线程对象都调用下 <code>join</code> 方法，实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">threads = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">5</span>]:</span><br><span class="line">    thread = threading.Thread(target=target, args=[i])</span><br><span class="line">    threads.append(thread)</span><br><span class="line">    thread.start()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> thread <span class="keyword">in</span> threads:</span><br><span class="line">    thread.join()</span><br></pre></td></tr></table></figure>
<p>这样，主线程必须等待子线程都运行结束，主线程才继续运行并结束。</p>
<h2 id="继承-Thread-类创建子线程"><a href="#继承-Thread-类创建子线程" class="headerlink" title="继承 Thread 类创建子线程"></a>继承 Thread 类创建子线程</h2><p>另外，我们也可以通过继承 <code>Thread</code> 类的方式创建一个线程，该线程需要执行的方法写在类的 <code>run</code> 方法里面即可。上面的例子的等价改写为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span><span class="params">(threading.Thread)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, second)</span>:</span></span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line">        self.second = second</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">f'Threading <span class="subst">&#123;threading.current_thread().name&#125;</span> is running'</span>)</span><br><span class="line">        print(<span class="string">f'Threading <span class="subst">&#123;threading.current_thread().name&#125;</span> sleep <span class="subst">&#123;self.second&#125;</span>s'</span>)</span><br><span class="line">        time.sleep(self.second)</span><br><span class="line">        print(<span class="string">f'Threading <span class="subst">&#123;threading.current_thread().name&#125;</span> is ended'</span>)</span><br><span class="line">        </span><br><span class="line">print(<span class="string">f'Threading <span class="subst">&#123;threading.current_thread().name&#125;</span> is running'</span>)</span><br><span class="line">threads = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">5</span>]:</span><br><span class="line">    thread = MyThread(i)</span><br><span class="line">    threads.append(thread)</span><br><span class="line">    thread.start()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> thread <span class="keyword">in</span> threads:</span><br><span class="line">    thread.join()</span><br><span class="line">print(<span class="string">f'Threading <span class="subst">&#123;threading.current_thread().name&#125;</span> is ended'</span>)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Threading MainThread <span class="keyword">is</span> running </span><br><span class="line">Threading Thread<span class="number">-1</span> <span class="keyword">is</span> running </span><br><span class="line">Threading Thread<span class="number">-1</span> sleep <span class="number">1</span>s </span><br><span class="line">Threading Thread<span class="number">-2</span> <span class="keyword">is</span> running </span><br><span class="line">Threading Thread<span class="number">-2</span> sleep <span class="number">5</span>s </span><br><span class="line">Threading Thread<span class="number">-1</span> <span class="keyword">is</span> ended </span><br><span class="line">Threading Thread<span class="number">-2</span> <span class="keyword">is</span> ended </span><br><span class="line">Threading MainThread <span class="keyword">is</span> ended</span><br></pre></td></tr></table></figure>
<p>可以看到，两种实现方式，其运行效果是相同的。</p>
<h2 id="守护线程"><a href="#守护线程" class="headerlink" title="守护线程"></a>守护线程</h2><p>在线程中有一个叫作守护线程的概念，如果一个线程被设置为守护线程，那么意味着这个线程是“不重要”的，这意味着，如果主线程结束了而该守护线程还没有运行完，那么它将会被强制结束。在 Python 中我们可以通过 <code>setDaemon</code> 方法来将某个线程设置为守护线程。</p>
<p>示例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">target</span><span class="params">(second)</span>:</span></span><br><span class="line">    print(<span class="string">f'Threading <span class="subst">&#123;threading.current_thread().name&#125;</span> is running'</span>)</span><br><span class="line">    print(<span class="string">f'Threading <span class="subst">&#123;threading.current_thread().name&#125;</span> sleep <span class="subst">&#123;second&#125;</span>s'</span>)</span><br><span class="line">    time.sleep(second)</span><br><span class="line">    print(<span class="string">f'Threading <span class="subst">&#123;threading.current_thread().name&#125;</span> is ended'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'Threading <span class="subst">&#123;threading.current_thread().name&#125;</span> is running'</span>)</span><br><span class="line">t1 = threading.Thread(target=target, args=[<span class="number">2</span>])</span><br><span class="line">t1.start()</span><br><span class="line">t2 = threading.Thread(target=target, args=[<span class="number">5</span>])</span><br><span class="line">t2.setDaemon(<span class="literal">True</span>)</span><br><span class="line">t2.start()</span><br><span class="line">print(<span class="string">f'Threading <span class="subst">&#123;threading.current_thread().name&#125;</span> is ended'</span>)</span><br></pre></td></tr></table></figure>
<p>在这里我们通过 <code>setDaemon</code> 方法将 <code>t2</code> 设置为了守护线程，这样主线程在运行完毕时，<code>t2</code> 线程会随着线程的结束而结束。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Threading MainThread is running </span><br><span class="line">Threading Thread-1 is running </span><br><span class="line">Threading Thread-1 sleep 2s </span><br><span class="line">Threading Thread-2 is running </span><br><span class="line">Threading Thread-2 sleep 5s </span><br><span class="line">Threading MainThread is ended </span><br><span class="line">Threading Thread-1 is ended</span><br></pre></td></tr></table></figure>
<p>可以看到，我们没有看到 <code>Thread-2</code> 打印退出的消息，<code>Thread-2</code> 随着主线程的退出而退出了。</p>
<p>不过细心的你可能会发现，这里并没有调用 <code>join</code> 方法，如果我们让 <code>t1</code> 和 <code>t2</code> 都调用<code>join</code>方法，主线程就会仍然等待各个子线程执行完毕再退出，不论其是否是守护线程。</p>
<h1 id="互斥锁"><a href="#互斥锁" class="headerlink" title="互斥锁"></a>互斥锁</h1><p>在一个进程中的多个线程是共享资源的，比如在一个进程中，有一个全局变量 <code>count</code> 用来计数，现在我们声明多个线程，每个线程运行时都给 <code>count</code> 加 1，让我们来看看效果如何，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span><span class="params">(threading.Thread)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">global</span> count</span><br><span class="line">        temp = count + <span class="number">1</span></span><br><span class="line">        time.sleep(<span class="number">0.001</span>)</span><br><span class="line">        count = temp</span><br><span class="line"></span><br><span class="line">threads = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    thread = MyThread()</span><br><span class="line">    thread.start()</span><br><span class="line">    threads.append(thread)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> thread <span class="keyword">in</span> threads:</span><br><span class="line">    thread.join()</span><br><span class="line">print(<span class="string">f'Final count: <span class="subst">&#123;count&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>
<p>在这里，我们声明了 1000 个线程，每个线程都是现取到当前的全局变量 <code>count</code> 值，然后休眠一小段时间，然后对 <code>count</code> 赋予新的值。</p>
<p>那这样，按照常理来说，最终的 <code>count</code>值应该为 1000。但其实不然，我们来运行一下看看。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Final count: 69</span><br></pre></td></tr></table></figure>
<p>最后的结果居然只有 69，而且多次运行或者换个环境运行结果是不同的。</p>
<p>这是为什么呢？因为 <code>count</code> 这个值是共享的，每个线程都可以在执行 temp = count 这行代码时拿到当前 count 的值，但是这些线程中的一些线程可能是并发或者并行执行的，这就导致不同的线程拿到的可能是同一个 <code>count</code> 值，最后导致有些线程的 <code>count</code> 的加 1 操作并没有生效，导致最后的结果偏小。</p>
<p>所以，如果多个线程同时对某个数据进行读取或修改，就会出现不可预料的结果。为了避免这种情况，我们需要对多个线程进行同步，要实现同步，我们可以对需要操作的数据进行加锁保护，这里就需要用到 <code>threading.Lock</code> 了。</p>
<p>加锁保护是什么意思呢？就是说，某个线程在对数据进行操作前，需要先加锁，这样其他的线程发现被加锁了之后，就无法继续向下执行，会一直等待锁被释放，只有加锁的线程把锁释放了，其他的线程才能继续加锁并对数据做修改，修改完了再释放锁。这样可以确保同一时间只有一个线程操作数据，多个线程不会再同时读取和修改同一个数据，这样最后的运行结果就是对的了。</p>
<p>我们可以将代码修改为如下内容：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span><span class="params">(threading.Thread)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">global</span> count</span><br><span class="line">        lock.acquire()</span><br><span class="line">        temp = count + <span class="number">1</span></span><br><span class="line">        time.sleep(<span class="number">0.001</span>)</span><br><span class="line">        count = temp</span><br><span class="line">        lock.release()</span><br><span class="line"></span><br><span class="line">lock = threading.Lock()</span><br><span class="line">threads = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    thread = MyThread()</span><br><span class="line">    thread.start()</span><br><span class="line">    threads.append(thread)</span><br><span class="line"><span class="keyword">for</span> thread <span class="keyword">in</span> threads:</span><br><span class="line">    thread.join()</span><br><span class="line">print(<span class="string">f'Final count: <span class="subst">&#123;count&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>
<p>在这里我们声明了一个 <code>lock</code> 对象，其实就是 <code>threading.Lock</code> 的一个实例，然后在 <code>run</code> 方法里面，获取 <code>count</code> 前先加锁，修改完 <code>count</code> 之后再释放锁，这样多个线程就不会同时获取和修改 <code>count</code> 的值了。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Final count: 1000</span><br></pre></td></tr></table></figure>
<p>这样运行结果就正常了。</p>
<p>关于 Python 多线程的内容，这里暂且先介绍这些，关于 <code>theading</code> 更多的使用方法，如信号量、队列等，可以参考官方文档：<a href="https://docs.python.org/zh-cn/3.7/library/threading.html#module-threading。" target="_blank" rel="noopener">https://docs.python.org/zh-cn/3.7/library/threading.html#module-threading。</a></p>
<h1 id="Python-多线程的问题"><a href="#Python-多线程的问题" class="headerlink" title="Python 多线程的问题"></a>Python 多线程的问题</h1><p>由于 Python 中 GIL 的限制，导致不论是在单核还是多核条件下，在同一时刻只能运行一个线程，导致 Python 多线程无法发挥多核并行的优势。</p>
<p>GIL 全称为 Global Interpreter Lock，中文翻译为全局解释器锁，其最初设计是出于数据安全而考虑的。</p>
<p>在 Python 多线程下，每个线程的执行方式如下：</p>
<ul>
<li>获取 <code>GIL</code></li>
<li>执行对应线程的代码</li>
<li>释放 <code>GIL</code></li>
</ul>
<p>可见，某个线程想要执行，必须先拿到 GIL，我们可以把 GIL 看作是通行证，并且在一个 Python 进程中，GIL 只有一个。拿不到通行证的线程，就不允许执行。这样就会导致，即使是多核条件下，一个 Python 进程下的多个线程，同一时刻也只能执行一个线程。</p>
<p>不过对于爬虫这种 IO 密集型任务来说，这个问题影响并不大。而对于计算密集型任务来说，由于 GIL 的存在，多线程总体的运行效率相比可能反而比单线程更低。</p>
]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫（四）Session 与 Cookies</title>
    <url>/posts/8c4e03b4.html</url>
    <content><![CDATA[<p>我们在浏览网站的过程中，经常会遇到需要登录的情况，而有些网页只有登录之后才可以访问，而且登录之后可以连续访问很多次网站，但是有时候过一段时间就需要重新登录。</p>
<p>还有一些网站，在打开浏览器时就自动登录了，而且很长时间都不会失效，这种情况又是为什么？其实这里面涉及 <code>Session</code> 和 <code>Cookies</code> 的相关知识，本节就来揭开它们的神秘面纱。</p>
<h1 id="静态网页和动态网页"><a href="#静态网页和动态网页" class="headerlink" title="静态网页和动态网页"></a>静态网页和动态网页</h1><p>在开始介绍它们之前，我们需要先了解一下静态网页和动态网页的概念。这里还是前面的示例代码，内容如下：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>This is a Demo<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"container"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"wrapper"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">h2</span> <span class="attr">class</span>=<span class="string">"title"</span>&gt;</span>Hello World<span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"text"</span>&gt;</span>Hello, this is a paragraph.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>这是最基本的 <code>HTML</code> 代码，我们将其保存为一个 .html 文件，然后把它放在某台具有固定公网 IP 的主机上，主机上装上 Apache 或 Nginx 等服务器，这样这台主机就可以作为服务器了，其他人便可以通过访问服务器看到这个页面，这就搭建了一个最简单的网站。</p>
<p>这种网页的内容是 <code>HTML</code> 代码编写的，文字、图片等内容均通过写好的 <code>HTML</code> 代码来指定，这种页面叫作静态网页。它加载速度快，编写简单，但是存在很大的缺陷，如可维护性差，不能根据 <code>URL</code> 灵活多变地显示内容等。例如，我们想要给这个网页的 <code>URL</code> 传入一个 <code>name</code> 参数，让其在网页中显示出来，是无法做到的。</p>
<p>因此，动态网页应运而生，它可以动态解析 <code>URL</code> 中参数的变化，<strong>关联数据库并动态呈现不同的页面内容</strong>，非常灵活多变。我们现在遇到的大多数网站都是动态网站，它们不再是一个简单的 HTML，而是可能由 JSP、PHP、Python 等语言编写的，其功能比静态网页强大和丰富太多了。</p>
<p>此外，<strong>动态网站还可以实现用户登录和注册的功能</strong>。再回到开头来看提到的问题，很多页面是需要登录之后才可以查看的。按照一般的逻辑来说，输入用户名和密码登录之后，肯定是拿到了一种类似凭证的东西，有了它，我们才能保持登录状态，才能访问登录之后才能看到的页面。</p>
<p>那么，这种神秘的凭证到底是什么呢？其实它就是 <code>Session</code> 和 <code>Cookies</code> 共同产生的结果，下面我们来一探究竟。</p>
<h1 id="无状态-HTTP"><a href="#无状态-HTTP" class="headerlink" title="无状态 HTTP"></a>无状态 HTTP</h1><p>在了解<code>Session</code> 和 <code>Cookies</code>之前，我们还需要了解 HTTP 的一个特点，叫作无状态。</p>
<p>HTTP 的无状态是指 HTTP 协议对事务处理是没有记忆能力的，也就是说服务器不知道客户端是什么状态。</p>
<p>当我们向服务器发送请求后，服务器解析此请求，然后返回对应的响应，服务器负责完成这个过程，而且这个过程是完全独立的，服务器不会记录前后状态的变化，也就是缺少状态记录。</p>
<p>这意味着如果后续需要处理前面的信息，则必须重传，这也导致需要额外传递一些前面的重复请求，才能获取后续响应，然而这种效果显然不是我们想要的。为了保持前后状态，我们肯定不能将前面的请求全部重传一次，这太浪费资源了，对于这种需要用户登录的页面来说，更是棘手。</p>
<p>这时两个用于保持 HTTP 连接状态的技术就出现了，它们分别是 <code>Session</code> 和 <code>Cookies</code>。<strong><code>Session</code> 在服务端，也就是网站的服务器，用来保存用户的 <code>Session</code> 信息；<code>Cookies</code> 在客户端，也可以理解为浏览器端</strong>，有了 Cookies，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别 <code>Cookies</code> 并鉴定出是哪个用户，然后再判断用户是否是登录状态，进而返回对应的响应。</p>
<p>我们可以理解为 <code>Cookies</code> 里面保存了登录的凭证，有了它，只需要在下次请求携带 <code>Cookies</code> 发送请求而不必重新输入用户名、密码等信息重新登录了。</p>
<p>因此在爬虫中，有时候处理需要登录才能访问的页面时，我们一般会直接将登录成功后获取的 <code>Cookies</code> 放在请求头里面直接请求，而不必重新模拟登录。</p>
<p>好了，了解 <code>Session</code> 和 <code>Cookies</code> 的概念之后，我们在来详细剖析它们的原理。</p>
<h1 id="Session"><a href="#Session" class="headerlink" title="Session"></a><code>Session</code></h1><p><code>Session</code>，中文称之为会话，其本身的含义是指有始有终的一系列动作 / 消息。比如，打电话时，从拿起电话拨号到挂断电话这中间的一系列过程可以称为一个 <code>Session</code>。</p>
<p>而在 Web 中，<code>Session</code> 对象用来存储特定用户 <code>Session</code> 所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 <code>Session</code> 对象中的变量将不会丢失，而是在整个用户 <code>Session</code> 中一直存在下去。当用户请求来自应用程序的 Web 页时，如果该用户还没有 <code>Session</code>，则 Web 服务器将自动创建一个 <code>Session</code> 对象。当 <code>Session</code> 过期或被放弃后，服务器将终止该 <code>Session</code>。</p>
<h1 id="Cookies"><a href="#Cookies" class="headerlink" title="Cookies"></a><code>Cookies</code></h1><p><code>Cookies</code> 指某些网站为了辨别用户身份、进行 <code>Session</code> 跟踪而存储在用户本地终端上的数据。</p>
<h2 id="Session-维持"><a href="#Session-维持" class="headerlink" title="Session 维持"></a>Session 维持</h2><p>那么，我们怎样利用 <code>Cookies</code>  保持状态呢？当客户端第一次请求服务器时，服务器会返回一个响应头中带有 Set-Cookie 字段的响应给客户端，用来标记是哪一个用户，客户端浏览器会把 <code>Cookies</code> 保存起来。当浏览器下一次再请求该网站时，浏览器会把此 <code>Cookies</code> 放到请求头一起提交给服务器，<code>Cookies</code>  携带了 <code>Session</code> ID 信息，服务器检查该 <code>Cookies</code> 即可找到对应的 <code>Session</code> 是什么，然后再判断 <code>Session</code> 来以此来辨认用户状态。</p>
<p>在成功登录某个网站时，服务器会告诉客户端设置哪些 <code>Cookies</code> 信息，在后续访问页面时客户端会把 <code>Cookies</code> 发送给服务器，服务器再找到对应的 <code>Session</code> 加以判断。如果 <code>Session</code> 中的某些设置登录状态的变量是有效的，那就证明用户处于登录状态，此时返回登录之后才可以查看的网页内容，浏览器再进行解析便可以看到了。</p>
<p>反之，如果传给服务器的 <code>Cookies</code> 是无效的，或者 <code>Session</code> 已经过期了，我们将不能继续访问页面，此时可能会收到错误的响应或者跳转到登录页面重新登录。</p>
<p>所以，<code>Cookies</code>和 <code>Session</code> 需要配合，一个处于客户端，一个处于服务端，二者共同协作，就实现了登录 <code>Session</code> 控制。</p>
<p><img src="/../Pic/Spider/cookie.jpg" style="zoom:57%;"></p>
<h2 id="属性结构"><a href="#属性结构" class="headerlink" title="属性结构"></a>属性结构</h2><p>接下来，我们来看看 <code>Cookies</code> 都有哪些内容。这里以知乎为例，在浏览器开发者工具中打开 <code>Application</code> 选项卡，然后在左侧会有一个<code>Storage</code> 部分，最后一项即为 <code>Cookies</code>，将其点开，如图所示，这些就是 <code>Cookies</code>。</p>
<p><img src="/../Pic/Spider/storage.jpg" style="zoom:80%;"></p>
<p>可以看到，这里有很多条目，其中每个条目可以称为 <code>Cookie</code>。它有如下几个属性。</p>
<ul>
<li><code>Name</code>，即该<code>Cookie</code>的名称。Cookie 一旦创建，名称便不可更改。</li>
<li><code>Value</code>，即该<code>Cookie</code>的值。如果值为 Unicode 字符，需要为字符编码。如果值为二进制数据，则需要使用 BASE64 编码。</li>
<li><code>Max Age</code>，即该<code>Cookie</code>失效的时间，单位秒，也常和 Expires 一起使用，通过它可以计算出其有效时间。Max Age 如果为正数，则该<code>Cookie</code>在 Max Age 秒之后失效。如果为负数，则关闭浏览器时<code>Cookie</code>即失效，浏览器也不会以任何形式保存该 <code>Cookie</code>。</li>
<li><code>Path</code>，即该<code>Cookie</code>的使用路径。如果设置为 /path/，则只有路径为 /path/ 的页面可以访问该 <code>Cookie</code>。如果设置为 /，则本域名下的所有页面都可以访问该 <code>Cookie</code>。</li>
<li><code>Domain</code>，即可以访问该<code>Cookie</code>的域名。例如如果设置为 .zhihu.com，则所有以 zhihu.com，结尾的域名都可以访问该 <code>Cookie</code>。</li>
<li><code>Size</code> 字段，即此<code>Cookie</code>的大小。</li>
<li><code>Http</code> 字段，即<code>Cookie</code>的 <code>httponly</code> 属性。若此属性为 true，则只有在 HTTP Headers 中会带有此<code>Cookie</code>的信息，而不能通过 <code>document.cookie</code> 来访问此 <code>Cookie</code>。</li>
<li><code>Secure</code>，即该<code>Cookie</code>是否仅被使用安全协议传输。安全协议。安全协议有 HTTPS、SSL 等，在网络上传输数据之前先将数据加密。默认为 <code>false</code>。</li>
</ul>
<h2 id="会话Cookie和持久-Cookie"><a href="#会话Cookie和持久-Cookie" class="headerlink" title="会话Cookie和持久 Cookie"></a>会话<code>Cookie</code>和持久 Cookie</h2><p>从表面意思来说，会话<code>Cookie</code>就是把<code>Cookie</code>放在浏览器内存里，浏览器在关闭之后该<code>Cookie</code>即失效；持久<code>Cookie</code>则会保存到客户端的硬盘中，下次还可以继续使用，用于长久保持用户登录状态。</p>
<p>其实严格来说，没有会话<code>Cookie</code>和持久<code>Cookie</code>之 分，只是由<code>Cookie</code>的 <code>Max Age</code> 或 <code>Expires</code> 字段决定了过期的时间。</p>
<p>因此，一些持久化登录的网站其实就是把<code>Cookie</code>的有效时间和<code>Session</code>有效期设置得比较长，下次我们再访问页面时仍然携带之前的 <code>Cookie</code>，就可以直接保持登录状态。</p>
<h1 id="常见误区"><a href="#常见误区" class="headerlink" title="常见误区"></a>常见误区</h1><p>在谈论<code>Session</code>机制的时候，常常听到这样一种误解 ——“只要关闭浏览器，<code>Session</code> 就消失了”。可以想象一下会员卡的例子，除非顾客主动对店家提出销卡，否则店家绝对不会轻易删除顾客的资料。对<code>Session</code>来说，也是一样，除非程序通知服务器删除一个 <code>Session</code>，否则服务器会一直保留。比如，程序一般都是在我们做注销操作时才去删除 <code>Session</code>。</p>
<p>但是当我们关闭浏览器时，浏览器不会主动在关闭之前通知服务器它将要关闭，所以服务器根本不会有机会知道浏览器已经关闭。之所以会有这种错觉，是因为大部分网站都使用会话<code>Cookie</code>来保存<code>Session ID</code> 信息，而关闭浏览器后 <code>Cookies</code> 就消失了，再次连接服务器时，也就无法找到原来的<code>Session</code>了。如果服务器设置的 <code>Cookies</code> 保存到硬盘上，或者使用某种手段改写浏览器发出的 <code>HTTP</code> 请求头，把原来的 <code>Cookies</code> 发送给服务器，则再次打开浏览器，仍然能够找到原来的<code>Session ID</code>，依旧还是可以保持登录状态的。</p>
<p>而且恰恰是由于关闭浏览器不会导致<code>Session</code>被删除，这就需要服务器为<code>Session</code>设置一个失效时间，当距离客户端上一次使用<code>Session</code>的时间超过这个失效时间时，服务器就可以认为客户端已经停止了活动，才会把<code>Session</code>删除以节省存储空间。</p>
]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫（三）爬虫的基本原理</title>
    <url>/posts/22139870.html</url>
    <content><![CDATA[<p>我们可以把互联网比作一张大网，而爬虫（即网络爬虫）便是在网上爬行的蜘蛛。如果把网的节点比作一个个网页，爬虫爬到这就相当于访问了该页面，获取了其信息。可以把节点间的连线比作网页与网页之间的链接关系，这样蜘蛛通过一个节点后，可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，网站的数据就可以被抓取下来了。</p>
<h1 id="爬虫概述"><a href="#爬虫概述" class="headerlink" title="爬虫概述"></a>爬虫概述</h1><p>简单来说，爬虫就是获取网页并提取和保存信息的自动化程序，下面概要介绍一下。</p>
<h2 id="获取网页"><a href="#获取网页" class="headerlink" title="获取网页"></a>获取网页</h2><p>爬虫首先要做的工作就是获取网页，这里就是获取网页的源代码。</p>
<p>源代码里包含了网页的部分有用信息，所以只要把源代码获取下来，就可以从中提取想要的信息了。</p>
<p>前面讲了请求和响应的概念，向网站的服务器发送一个请求，返回的响应体便是网页源代码。所以，最关键的部分就是构造一个请求并发送给服务器，然后接收到响应并将其解析出来，那么这个流程怎样实现呢？总不能手工去截取网页源码吧？</p>
<p>不用担心，Python 提供了许多库来帮助我们实现这个操作，如 <code>urllib</code>、<code>requests</code> 等。我们可以用这些库来帮助我们实现 HTTP 请求操作，请求和响应都可以用类库提供的数据结构来表示，得到响应之后只需要解析数据结构中的 <code>Body</code> 部分即可，即得到网页的源代码，这样我们可以用程序来实现获取网页的过程了。</p>
<h2 id="提取信息"><a href="#提取信息" class="headerlink" title="提取信息"></a>提取信息</h2><p>获取网页源代码后，接下来就是分析网页源代码，从中提取我们想要的数据。首先，最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错。</p>
<p>另外，由于网页的结构有一定的规则，所以还有一些根据网页节点属性、CSS 选择器或 XPath 来提取网页信息的库，如 <code>Beautiful Soup</code>、<code>pyquery</code>、<code>lxml</code> 等。使用这些库，我们可以高效快速地从中提取网页信息，如节点的属性、文本值等。</p>
<p>提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得条理清晰，以便我们后续处理和分析数据。</p>
<h2 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h2><p>提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为 TXT 文本或 JSON 文本，也可以保存到数据库，如 MySQL 和 MongoDB 等，还可保存至远程服务器，如借助<code>SFTP</code> 进行操作等。</p>
<h2 id="自动化程序"><a href="#自动化程序" class="headerlink" title="自动化程序"></a>自动化程序</h2><p>说到自动化程序，意思是说爬虫可以代替人来完成这些操作。首先，我们手工当然可以提取这些信息，但是当量特别大或者想快速获取大量数据的话，肯定还是要借助程序。爬虫就是代替我们来完成这份爬取工作的自动化程序，它可以在抓取过程中进行各种异常处理、错误重试等操作，确保爬取持续高效地运行。</p>
<h1 id="能抓怎样的数据"><a href="#能抓怎样的数据" class="headerlink" title="能抓怎样的数据"></a>能抓怎样的数据</h1><p>在网页中我们能看到各种各样的信息，最常见的便是常规网页，它们对应着 HTML 代码，而最常抓取的便是 HTML 源代码。</p>
<p>另外，可能有些网页返回的不是 HTML 代码，而是一个 JSON 字符串（其中 API 接口大多采用这样的形式），这种格式的数据方便传输和解析，它们同样可以抓取，而且数据提取更加方便。</p>
<p>此外，我们还可以看到各种二进制数据，如图片、视频和音频等。利用爬虫，我们可以将这些二进制数据抓取下来，然后保存成对应的文件名。</p>
<p>另外，还可以看到各种扩展名的文件，如 CSS、JavaScript 和配置文件等，这些其实也是最普通的文件，只要在浏览器里面可以访问到，就可以将其抓取下来。</p>
<p>上述内容其实都对应各自的 URL，是基于 HTTP 或 HTTPS 协议的，只要是这种数据，爬虫都可以抓取。</p>
<h2 id="JavaScript-渲染页面"><a href="#JavaScript-渲染页面" class="headerlink" title="JavaScript 渲染页面"></a>JavaScript 渲染页面</h2><p>有时候，我们在用 urllib 或 requests 抓取网页时，得到的源代码实际和浏览器中看到的不一样。</p>
<p>这是一个非常常见的问题。现在网页越来越多地采用 Ajax、前端模块化工具来构建，整个网页可能都是由 JavaScript 渲染出来的，也就是说原始的 HTML 代码就是一个空壳，例如：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>This is a Demo<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"container"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"app.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>在浏览器中打开这个页面时，首先会加载这个 HTML 内容，接着浏览器会发现其中引入了一个 <code>app.js</code> 文件，然后便会接着去请求这个文件，获取到该文件后，便会执行其中的 JavaScript 代码，而 JavaScript 则会改变 HTML 中的节点，向其添加内容，最后得到完整的页面。</p>
<p>但是在用 <code>urllib</code> 或 <code>requests</code> 等库请求当前页面时，我们得到的只是这个 HTML 代码，它不会帮助我们去继续加载这个 JavaScript 文件，这样也就看不到浏览器中的内容了。</p>
<p>这也解释了为什么有时我们得到的源代码和浏览器中看到的不一样。</p>
<p>因此，使用基本 HTTP 请求库得到的源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，我们可以分析其后台 <code>Ajax</code> 接口，也可使用 <code>Selenium</code>、<code>Splash</code> 这样的库来实现模拟 JavaScript 渲染。</p>
<p>后面，我们会详细介绍如何采集 JavaScript 渲染的网页。本节介绍了爬虫的一些基本原理，这可以帮助我们在后面编写爬虫时更加得心应手。</p>
]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫（二）Web网页基础</title>
    <url>/posts/503970b4.html</url>
    <content><![CDATA[<h1 id="网页的组成"><a href="#网页的组成" class="headerlink" title="网页的组成"></a>网页的组成</h1><p>首先，我们来了解网页的基本组成，网页可以分为三大部分：HTML、CSS 和 JavaScript。</p>
<p>如果把网页比作一个人的话，HTML 相当于骨架，JavaScript 相当于肌肉，CSS 相当于皮肤，三者结合起来才能形成一个完整的网页。下面我们来分别介绍一下这三部分的功能。</p>
<h2 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h2><p>HTML 是用来描述网页的一种语言，其全称叫作 Hyper Text Markup Language，即超文本标记语言。</p>
<p>我们浏览的网页包括文字、按钮、图片和视频等各种复杂的元素，其基础架构就是 HTML。不同类型的元素通过不同类型的标签来表示，如图片用 <code>img</code> 标签表示，视频用 <code>video</code> 标签表示，段落用 <code>p</code> 标签表示，它们之间的布局又常通过布局标签 div 嵌套组合而成，各种标签通过不同的排列和嵌套就可以形成网页的框架。</p>
<p>我们在 Chrome 浏览器中打开百度，右击并选择 “检查” 项（或按 F12 键），打开开发者模式，这时在 Elements 选项卡中即可看到网页的源代码，如图所示。</p>
<p><img src="/../Pic/Spider/baidu_2.png" style="zoom:67%;"></p>
<p>这就是 HTML，整个网页就是由各种标签嵌套组合而成的。这些标签定义的节点元素相互嵌套和组合形成了复杂的层次关系，就形成了网页的架构。</p>
<h2 id="CSS"><a href="#CSS" class="headerlink" title="CSS"></a>CSS</h2><p>虽然 HTML 定义了网页的结构，但是只有 HTML 页面的布局并不美观，可能只是简单的节点元素的排列，为了让网页看起来更好看一些，这里就需要借助 CSS 了。</p>
<p>CSS，全称叫作 Cascading Style Sheets，即层叠样式表。“层叠” 是指当在 HTML 中引用了数个样式文件，并且样式发生冲突时，浏览器能依据层叠顺序处理。“样式” 指网页中文字大小、颜色、元素间距、排列等格式。</p>
<p>CSS 是目前唯一的网页页面排版样式标准，有了它的帮助，页面才会变得更为美观。</p>
<p>图的右侧即为 CSS，例如：</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-id">#head_wrapper</span><span class="selector-class">.s-ps-islite</span> <span class="selector-class">.s-p-top</span> &#123;</span><br><span class="line"></span><br><span class="line">   <span class="attribute">position</span>: absolute;</span><br><span class="line"></span><br><span class="line">   <span class="attribute">bottom</span>: <span class="number">40px</span>;</span><br><span class="line"></span><br><span class="line">   <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line"></span><br><span class="line">   <span class="attribute">height</span>: <span class="number">181px</span>;</span><br></pre></td></tr></table></figure>
<p>这就是一个 CSS 样式。大括号前面是一个 CSS 选择器。此选择器的作用是首先选中 <code>id</code> 为 <code>head_wrapper</code> 且 <code>class</code> 为 <code>s-ps-islite</code> 的节点，然后再选中其内部的 <code>class</code> 为 <code>s-p-top</code> 的节点。</p>
<p>大括号内部写的就是一条条样式规则，例如 <code>position</code> 指定了这个元素的布局方式为绝对布局，<code>bottom</code> 指定元素的下边距为 40 像素，<code>width</code> 指定了宽度为 100% 占满父元素，<code>height</code> 则指定了元素的高度。</p>
<p>也就是说，我们将位置、宽度、高度等样式配置统一写成这样的形式，然后用大括号括起来，接着在开头再加上 CSS 选择器，这就代表这个样式对 CSS 选择器选中的元素生效，元素就会根据此样式来展示了。</p>
<p>在网页中，一般会统一定义整个网页的样式规则，并写入 CSS 文件中（其后缀为 <code>css</code>）。在 HTML 中，只需要用 <code>link</code> 标签即可引入写好的 CSS 文件，这样整个页面就会变得美观、优雅。</p>
<h2 id="JavaScript"><a href="#JavaScript" class="headerlink" title="JavaScript"></a>JavaScript</h2><p>JavaScript，简称 JS，是一种脚本语言。HTML 和 CSS 配合使用，提供给用户的只是一种静态信息，缺乏交互性。我们在网页里可能会看到一些交互和动画效果，如下载进度条、提示框、轮播图等，这通常就是 JavaScript 的功劳。它的出现使得用户与信息之间不只是一种浏览与显示的关系，而是实现了一种实时、动态、交互的页面功能。</p>
<p>JavaScript 通常也是以单独的文件形式加载的，后缀为 js，在 HTML 中通过 script 标签即可引入，例如：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line">&lt;script src=<span class="string">"jquery-2.1.0.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></table></figure>
<p>综上所述，HTML 定义了网页的内容和结构，CSS 描述了网页的布局，JavaScript 定义了网页的行为。</p>
<h1 id="网页的结构"><a href="#网页的结构" class="headerlink" title="网页的结构"></a>网页的结构</h1><p>了解了网页的基本组成，我们再用一个例子来感受下 HTML 的基本结构。新建一个文本文件，名称可以自取，后缀为 <code>html</code>，内容如下：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>This is a Demo<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"container"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"wrapper"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">h2</span> <span class="attr">class</span>=<span class="string">"title"</span>&gt;</span>Hello World<span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"text"</span>&gt;</span>Hello, this is a paragraph.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br><span class="line">这就是一个最简单的 HTML 实例。开头用 DOCTYPE 定义了文档类型，其次最外层是 html 标签，最后还有对应的结束标签来表示闭合，其内部是 head 标签和 body 标签，分别代表网页头和网页体，它们也需要结束标签。</span><br></pre></td></tr></table></figure>
<p><code>head</code> 标签内定义了一些页面的配置和引用，如：<code>&lt;meta charset=&quot;UTF-8&quot;&gt;</code>，它指定了网页的编码为 UTF-8。<code>title</code> 标签则定义了网页的标题，会显示在网页的选项卡中，不会显示在正文中。body 标签内则是在网页正文中显示的内容。</p>
<p><code>div</code> 标签定义了网页中的区块，它的 <code>id</code> 是 <code>container</code>，这是一个非常常用的属性，且 <code>id</code> 的内容在网页中是唯一的，我们可以通过它来获取这个区块。然后在此区块内又有一个 <code>div</code> 标签，它的 <code>class</code> 为 <code>wrapper</code>，这也是一个非常常用的属性，经常与 CSS 配合使用来设定样式。</p>
<p>然后此区块内部又有一个 <code>h2</code> 标签，这代表一个二级标题。另外，还有一个 <code>p</code> 标签，这代表一个段落。在这两者中直接写入相应的内容即可在网页中呈现出来，它们也有各自的 <code>class</code> 属性。</p>
<p>将代码保存后，在浏览器中打开该文件，可以看到如图所示的内容。</p>
<p><img src="/../Pic/Spider/demo.png" alt></p>
<p>可以看到，在选项卡上显示了 <code>This is a Demo</code> 字样，这是我们在 <code>head</code> 中的 <code>title</code> 里定义的文字。而网页正文是 <code>body</code> 标签内部定义的各个元素生成的，可以看到这里显示了二级标题和段落。</p>
<p>这个实例便是网页的一般结构。一个网页的标准形式是 <code>html</code> 标签内嵌套 <code>head</code> 和 <code>body</code> 标签，<code>head</code> 内定义网页的配置和引用，<code>body</code> 内定义网页的正文。</p>
<h1 id="节点树及节点间的关系"><a href="#节点树及节点间的关系" class="headerlink" title="节点树及节点间的关系"></a>节点树及节点间的关系</h1><p>在 HTML 中，所有标签定义的内容都是节点，它们构成了一个 <code>HTML DOM</code> 树。</p>
<p>我们先看下什么是 DOM。<code>DOM</code> 是 W3C（万维网联盟）的标准，其英文全称 Document Object Model，即文档对象模型。它定义了访问 HTML 和 XML 文档的标准：</p>
<p>W3C 文档对象模型（<code>DOM</code>）是中立于平台和语言的接口，它允许程序和脚本动态地访问和更新文档的内容、结构和样式。</p>
<p><code>W3C DOM</code> 标准被分为 3 个不同的部分：</p>
<ul>
<li>核心 <code>DOM</code> - 针对任何结构化文档的标准模型</li>
<li><code>XML DOM</code> - 针对 XML 文档的标准模型</li>
<li><code>HTML DOM</code> - 针对 HTML 文档的标准模型</li>
</ul>
<p>根据 W3C 的 <code>HTML DOM</code> 标准，HTML 文档中的所有内容都是节点：</p>
<ul>
<li>整个文档是一个文档节点</li>
<li>每个 HTML 元素是元素节点</li>
<li>HTML 元素内的文本是文本节点</li>
<li>每个 HTML 属性是属性节点</li>
<li>注释是注释节点</li>
</ul>
<p><code>HTML DOM</code> 将 HTML 文档视作树结构，这种结构被称为节点树，如图所示。</p>
<p><img src="/../Pic/Spider/tree.jpg" style="zoom:90%;"></p>
<p>通过 <code>HTML DOM</code>，树中的所有节点均可通过 <code>JavaScript</code> 访问，所有 <code>HTML</code> 节点元素均可被修改，也可以被创建或删除。</p>
<p>节点树中的节点彼此拥有层级关系。我们常用父（<code>parent</code>）、子（<code>child</code>）和兄弟（<code>sibling</code>）等术语描述这些关系。父节点拥有子节点，同级的子节点被称为兄弟节点。</p>
<p>在节点树中，顶端节点称为根（<code>root</code>）。除了根节点之外，每个节点都有父节点，同时可拥有任意数量的子节点或兄弟节点。图中展示了节点树以及节点之间的关系。</p>
<p>本段参考 W3SCHOOL，链接：<a href="http://www.w3school.com.cn/htmldom/dom_nodes.asp。" target="_blank" rel="noopener">http://www.w3school.com.cn/htmldom/dom_nodes.asp。</a></p>
<h1 id="选择器"><a href="#选择器" class="headerlink" title="选择器"></a>选择器</h1><p>我们知道网页由一个个节点组成，CSS 选择器会根据不同的节点设置不同的样式规则，那么怎样来定位节点呢？</p>
<p>在 CSS 中，我们使用 CSS 选择器来定位节点。例如，上例中 <code>div</code> 节点的 <code>id</code> 为 <code>container</code>，那么就可以表示为 <code>#container</code>，其中 <code>#</code> 开头代表选择 <code>id</code>，其后紧跟 <code>id</code> 的名称。</p>
<p>另外，如果我们想选择 <code>class</code>为 <code>wrapper</code> 的节点，便可以使用 <code>.wrapper</code>，这里以点“<code>.</code>”开头代表选择 <code>class</code>，其后紧跟 <code>class</code> 的名称。另外，还有一种选择方式，那就是根据标签名筛选，例如想选择二级标题，直接用 <code>h2</code> 即可。这是最常用的 3 种表示，分别是根据 <code>id</code>、<code>class</code>、标签名筛选，请牢记它们的写法。</p>
<p>另外，CSS 选择器还支持嵌套选择，各个选择器之间加上空格分隔开便可以代表嵌套关系，如 <code>#container .wrapper p</code> 则代表先选择 id 为 <code>container</code> 的节点，然后选中其内部的 <code>class</code> 为 <code>wrapper</code> 的节点，然后再进一步选中其内部的 <code>p</code> 节点。</p>
<p>另外，如果不加空格，则代表并列关系，如 <code>div#container .wrapper p.text</code> 代表先选择 <code>id</code> 为 <code>container</code> 的 <code>div</code> 节点，然后选中其内部的 <code>class</code>为 <code>wrapper</code> 的节点，再进一步选中其内部的 <code>class</code> 为 <code>text</code> 的 <code>p</code>节点。这就是 CSS 选择器，其筛选功能还是非常强大的。</p>
<p>另外，CSS 选择器还有一些其他语法规则，具体如表所示。</p>
<p><img src="/../Pic/Spider/select1.png" style="zoom:67%;"></p>
<p><img src="/../Pic/Spider/select2.png" style="zoom:67%;"></p>
<p><img src="/../Pic/Spider/select3.png" style="zoom:67%;"></p>
<p>另外，还有一种比较常用的选择器是 XPath，这种选择方式后面会详细介绍。</p>
]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫（一）掌握HTTP基本原理</title>
    <url>/posts/8c9d019c.html</url>
    <content><![CDATA[<h1 id="URL和URI"><a href="#URL和URI" class="headerlink" title="URL和URI"></a>URL和URI</h1><ul>
<li>URL（Uniform Resource Identifier）：统一资源定位符</li>
<li>URI（Uniform Resource Locator）：统一资源标识符</li>
</ul>
<p>举例来说，<a href="https://github.com/favicon.ico" target="_blank" rel="noopener">https://github.com/favicon.ico</a> ，它是一个 URL，也是一个 URI。即有这样的一个图标资源，我们用 URL/URI 来唯一指定了它的访问方式，这其中包括了访问协议 HTTPS、访问路径（即根目录）和资源名称 <code>favicon.ico</code>。通过这样一个链接，我们便可以从互联网上找到这个资源，这就是 URL/URI。</p>
<p>URL 是 URI 的子集，也就是说每个 URL 都是 URI，但不是每个 URI 都是 URL。那么，什么样的 URI 不是 URL 呢？URI 还包括一个子类叫作 URN，它的全称为 Universal Resource Name，即统一资源名称。</p>
<p>URN 只命名资源而不指定如何定位资源，比如 <code>urn:isbn:0451450523</code> 指定了一本书的 ISBN，可以唯一标识这本书，但是没有指定到哪里定位这本书，这就是 URN。URL、URN 和 URI 的关系可以用图表示。</p>
<p><img src="/../Pic/Spider/url.jpg" style="zoom:33%;"></p>
<p>但是在目前的互联网，URN 的使用非常少，几乎所有的 URI 都是 URL，所以一般的网页链接我们可以称之为 URL，也可以称之为 URI。</p>
<h1 id="超文本"><a href="#超文本" class="headerlink" title="超文本"></a>超文本</h1><p>接下来，我们再了解一个概念 —— 超文本，其英文名称叫作 Hypertext，我们在浏览器里看到的网页就是超文本解析而成的，其网页源代码是一系列 HTML 代码，里面包含了一系列标签，比如 <code>img</code> 显示图片，<code>p</code> 指定显示段落等。浏览器解析这些标签后，便形成了我们平常看到的网页，而网页的源代码 HTML 就可以称作超文本。</p>
<p>例如，我们在 Chrome 浏览器里面打开任意一个页面，如淘宝首页，右击任一地方并选择 “检查” 项（或者直接按快捷键 F12），即可打开浏览器的开发者工具，这时在 Elements 选项卡即可看到当前网页的源代码，这些源代码都是超文本，如图所示。</p>
<p><img src="/../Pic/Spider/taobao.png" alt></p>
<h1 id="HTTP-和-HTTPS"><a href="#HTTP-和-HTTPS" class="headerlink" title="HTTP 和 HTTPS"></a>HTTP 和 HTTPS</h1><p>在淘宝的首页 <a href="https://www.taobao.com/中，URL" target="_blank" rel="noopener">https://www.taobao.com/中，URL</a> 的开头会有 <code>http</code> 或 <code>https</code>，这个就是访问资源需要的协议类型，有时我们还会看到 <code>ftp</code>、<code>sftp</code>、<code>smb</code> 开头的 URL，那么这里的 <code>ftp</code>、<code>sftp</code>、<code>smb</code> 都是指的协议类型。在爬虫中，我们抓取的页面通常就是 <code>http</code> 或 <code>https</code> 协议的，我们在这里首先来了解一下这两个协议的含义。</p>
<p>HTTP 的全称是 Hyper Text Transfer Protocol，中文名叫作<strong>超文本传输协议</strong>，HTTP 协议是用于从网络传输超文本数据到本地浏览器的传送协议，它能保证高效而准确地传送超文本文档。HTTP 由万维网协会（World Wide Web Consortium）和 Internet 工作小组 IETF（Internet Engineering Task Force）共同合作制定的规范，目前广泛使用的是 HTTP 1.1 版本。</p>
<p>HTTPS 的全称是 Hyper Text Transfer Protocol over Secure Socket Layer，是以安全为目标的 HTTP 通道，简单讲是 HTTP 的安全版，即 HTTP 下加入 SSL 层，简称为 HTTPS。</p>
<p><strong>HTTPS 的安全基础是 SSL</strong>，因此通过它传输的内容都是经过 SSL 加密的，它的主要作用可以分为两种：</p>
<ul>
<li>建立一个信息安全通道，来保证数据传输的安全。</li>
<li>确认网站的真实性，凡是使用了 HTTPS 的网站，都可以通过点击浏览器地址栏的锁头标志来查看网站认证之后的真实信息，也可以通过 CA 机构颁发的安全签章来查询。</li>
</ul>
<p>现在越来越多的网站和 App 都已经向 HTTPS 方向发展。例如：</p>
<ul>
<li>苹果公司强制所有 iOS App 在 2017 年 1 月 1 日 前全部改为使用 HTTPS 加密，否则 App 就无法在应用商店上架。<br>谷歌从 2017 年 1 月推出的 Chrome 56 开始，对未进行 HTTPS 加密的网址链接亮出风险提示，即在地址栏的显著位置提醒用户 “此网页不安全”。</li>
<li>腾讯微信小程序的官方需求文档要求后台使用 HTTPS 请求进行网络通信，不满足条件的域名和协议无法请求。</li>
</ul>
<p>因此，HTTPS 已经已经是大势所趋。</p>
<h1 id="HTTP-请求过程"><a href="#HTTP-请求过程" class="headerlink" title="HTTP 请求过程"></a>HTTP 请求过程</h1><p>我们在浏览器中输入一个 URL，回车之后便可以在浏览器中观察到页面内容。实际上，<strong>这个过程是浏览器向网站所在的服务器发送了一个请求，网站服务器接收到这个请求后进行处理和解析，然后返回对应的响应，接着传回给浏览器</strong>。响应里包含了页面的源代码等内容，浏览器再对其进行解析，便将网页呈现了出来，传输模型如图所示。</p>
<p><img src="/../Pic/Spider/requests1.jpg" style="zoom: 33%;"></p>
<p>此处客户端即代表我们自己的 PC 或手机浏览器，服务器即要访问的网站所在的服务器。</p>
<p>为了更直观地说明这个过程，这里用 Chrome 浏览器的开发者模式下的 Network 监听组件来做下演示，它可以显示访问当前请求网页时发生的所有网络请求和响应。</p>
<p>打开 Chrome 浏览器，右击并选择 “检查” 项，即可打开浏览器的开发者工具。这里访问百度 <a href="http://www.baidu.com/" target="_blank" rel="noopener">http://www.baidu.com/</a> ，输入该 URL 后回车，观察这个过程中发生了怎样的网络请求。可以看到，在 <code>Network</code> 页面下方出现了一个个的条目，其中一个条目就代表一次发送请求和接收响应的过程，如图所示。</p>
<p><img src="/../Pic/Spider/baidu.png" style="zoom:33%;"></p>
<ul>
<li>第一列 <code>Name</code>：请求的名称，一般会将 URL 的最后一部分内容当作名称。</li>
<li>第二列 <code>Status</code>：响应的状态码，这里显示为 200，代表响应是正常的。通过状态码，我们可以判断发送了请求之后是否得到了正常的响应。</li>
<li>第三列 <code>Type</code>：请求的文档类型。这里为 <code>document</code>，代表我们这次请求的是一个<code>HTML</code> 文档，内容就是一些 HTML 代码。</li>
<li>第四列<code>Initiator</code>：请求源。用来标记请求是由哪个对象或进程发起的。</li>
<li>第五列 <code>Size</code>：从服务器下载的文件和请求的资源大小。如果是从缓存中取得的资源，则该列会显示 from cache。</li>
<li>第六列 <code>Time</code>：发起请求到获取响应所用的总时间。</li>
<li>第七列 <code>Waterfall</code>：网络请求的可视化瀑布流。<br>我们点击这个条目即可看到其更详细的信息，如图所示。</li>
</ul>
<p><img src="/../Pic/Spider/network1.jpg" style="zoom:90%;"></p>
<p>首先是 General 部分，Request URL 为请求的 URL，Request Method 为请求的方法，Status Code 为响应状态码，Remote Address 为远程服务器的地址和端口，Referrer Policy 为 Referrer 判别策略。</p>
<p>再继续往下，可以看到，有 Response Headers 和 Request Headers，这分别代表响应头和请求头。请求头里带有许多请求信息，例如浏览器标识、Cookies、Host 等信息，这是请求的一部分，服务器会根据请求头内的信息判断请求是否合法，进而作出对应的响应。图中看到的 Response Headers 就是响应的一部分，例如其中包含了服务器的类型、文档类型、日期等信息，浏览器接受到响应后，会解析响应内容，进而呈现网页内容。</p>
<p>下面我们分别来介绍一下请求和响应都包含哪些内容。</p>
<h1 id="请求"><a href="#请求" class="headerlink" title="请求"></a>请求</h1><p>请求，由客户端向服务端发出，可以分为 4 部分内容：请求方法（Request Method）、请求的网址（Request URL）、请求头（Request Headers）、请求体（Request Body）。</p>
<h2 id="请求方法"><a href="#请求方法" class="headerlink" title="请求方法"></a>请求方法</h2><p>常见的请求方法有两种：<code>GET</code> 和 <code>POST</code>。</p>
<p>在浏览器中直接输入 URL 并回车，这便发起了一个 <code>GET</code> 请求，请求的参数会直接包含到 URL 里。例如，在百度中搜索 Python，这就是一个 GET 请求，链接为 <a href="https://www.baidu.com/s?wd=Python" target="_blank" rel="noopener">https://www.baidu.com/s?wd=Python</a> ，其中 URL 中包含了请求的参数信息，这里参数 <code>wd</code> 表示要搜寻的关键字。POST 请求大多在表单提交时发起。比如，对于一个登录表单，输入用户名和密码后，点击 “登录” 按钮，这通常会发起一个 POST 请求，其数据通常以表单的形式传输，而不会体现在 URL 中。</p>
<p><strong>GET 和 POST 请求方法有如下区别。</strong></p>
<ul>
<li>GET 请求中的参数包含在 URL 里面，数据可以在 URL 中看到，而 POST 请求的 URL 不会包含这些数据，数据都是通过表单形式传输的，会包含在请求体中。</li>
<li>GET 请求提交的数据最多只有 1024 字节，而 POST 请求没有限制。</li>
</ul>
<p>一般来说，登录时，需要提交用户名和密码，其中包含了敏感信息，使用 GET 方式请求的话，密码就会暴露在 URL 里面，造成密码泄露，所以这里最好以 POST 方式发送。上传文件时，由于文件内容比较大，也会选用 POST 方式。</p>
<p>我们平常遇到的绝大部分请求都是 GET 或 POST 请求，另外还有一些请求方法，如 <code>HEAD</code>、<code>PUT</code>、<code>DELETE</code>、<code>OPTIONS</code>、<code>CONNECT</code>、<code>TRACE</code> 等，我们简单将其总结为下表。</p>
<p><img src="/../Pic/Spider/post1.png" style="zoom:80%;"></p>
<p>请求的网址，即统一资源定位符 URL，它可以唯一确定我们想请求的资源。</p>
<h2 id="请求头"><a href="#请求头" class="headerlink" title="请求头"></a>请求头</h2><p>请求头，用来说明服务器要使用的附加信息，比较重要的信息有 <code>Cookie</code>、<code>Referer</code>、<code>User-Agent</code> 等。下面简要说明一些常用的头信息。</p>
<ul>
<li><code>Accept</code>：请求报头域，用于指定客户端可接受哪些类型的信息。</li>
<li><code>Accept-Language</code>：指定客户端可接受的语言类型。</li>
<li><code>Accept-Encoding</code>：指定客户端可接受的内容编码。</li>
<li><code>Host</code>：用于指定请求资源的主机 IP 和端口号，其内容为请求 URL 的原始服务器或网关的位置。从 HTTP 1.1 版本开始，请求必须包含此内容。</li>
<li><code>Cookie</code>：也常用复数形式 Cookies，这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据。它的主要功能是维持当前访问会话。例如，我们输入用户名和密码成功登录某个网站后，服务器会用会话保存登录状态信息，后面我们每次刷新或请求该站点的其他页面时，会发现都是登录状态，这就是 Cookies 的功劳。Cookies 里有信息标识了我们所对应的服务器的会话，每次浏览器在请求该站点的页面时，都会在请求头中加上 Cookies 并将其发送给服务器，服务器通过 Cookies 识别出是我们自己，并且查出当前状态是登录状态，所以返回结果就是登录之后才能看到的网页内容。</li>
<li><code>Referer</code>：此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相应的处理，如做来源统计、防盗链处理等。</li>
<li><code>User-Agent</code>：简称 UA，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本、浏览器及版本等信息。在做爬虫时加上此信息，可以伪装为浏览器；如果不加，很可能会被识别出为爬虫。</li>
<li><code>Content-Type</code>：也叫互联网媒体类型（Internet Media Type）或者 MIME 类型，在 HTTP 协议消息头中，它用来表示具体请求中的媒体类型信息。例如，text/html 代表 HTML 格式，image/gif 代表 GIF 图片，application/json 代表 JSON 类型，更多对应关系可以查看此对照表：<a href="http://tool.oschina.net/commons。" target="_blank" rel="noopener">http://tool.oschina.net/commons。</a></li>
</ul>
<p>因此，请求头是请求的重要组成部分，在写爬虫时，大部分情况下都需要设定请求头。</p>
<h2 id="请求体"><a href="#请求体" class="headerlink" title="请求体"></a>请求体</h2><p>请求体一般承载的内容是 <code>POST</code> 请求中的表单数据，而对于<code>GET</code> 请求，请求体则为空。</p>
<p>例如，这里我登录 GitHub 时捕获到的请求和响应如图所示。</p>
<p><img src="/../Pic/Spider/github.jpg" style="zoom:67%;"></p>
<p>登录之前，我们填写了用户名和密码信息，提交时这些内容就会以表单数据的形式提交给服务器，此时需要注意 Request Headers 中指定 <code>Content-Type</code> 为 application/x-www-form-urlencoded。只有设置 Content-Type 为 application/x-www-form-urlencoded，才会以表单数据的形式提交。另外，我们也可以将 Content-Type 设置为 application/json 来提交 JSON 数据，或者设置为 multipart/form-data 来上传文件。</p>
<p>表格中列出了 <code>Content-Type</code> 和 <code>POST</code> 提交数据方式的关系。</p>
<p><img src="/../Pic/Spider/content.png" style="zoom:80%;"></p>
<p>在爬虫中，如果要构造 POST 请求，需要使用正确的 <code>Content-Type</code>，并了解各种请求库的各个参数设置时使用的是哪种 Content-Type，不然可能会导致 POST 提交后无法正常响应。</p>
<h1 id="响应"><a href="#响应" class="headerlink" title="响应"></a>响应</h1><p>响应，由服务端返回给客户端，可以分为三部分：响应状态码（Response Status Code）、响应头（Response Headers）和响应体（Response Body）。</p>
<h2 id="响应状态码"><a href="#响应状态码" class="headerlink" title="响应状态码"></a>响应状态码</h2><p>响应状态码表示服务器的响应状态，如 200 代表服务器正常响应，404 代表页面未找到，500 代表服务器内部发生错误。在爬虫中，我们可以根据状态码来判断服务器响应状态，如状态码为 200，则证明成功返回数据，再进行进一步的处理，否则直接忽略。下表列出了常见的错误代码及错误原因。</p>
<p><img src="/../Pic/Spider/response.png" style="zoom:67%;"></p>
<p>响应头包含了服务器对请求的应答信息，如 Content-Type、Server、Set-Cookie 等。下面简要说明一些常用的响应头信息。</p>
<ul>
<li><code>Date</code>：标识响应产生的时间。</li>
<li><code>Last-Modified</code>：指定资源的最后修改时间。</li>
<li><code>Content-Encoding</code>：指定响应内容的编码。</li>
<li><code>Server</code>：包含服务器的信息，比如名称、版本号等。</li>
<li><code>Content-Type</code>：文档类型，指定返回的数据类型是什么，如 <code>text/html</code> 代表返回 <code>HTML</code> 文档，<code>application/x-javascript</code> 则代表返回 <code>JavaScript</code>文件，<code>image/jpeg</code> 则代表返回图片。</li>
<li><code>Set-Cookie</code>：设置 <code>Cookies</code>。响应头中的 <code>Set-Cookie</code> 告诉浏览器需要将此内容放在 <code>Cookies</code> 中，下次请求携带 <code>Cookies</code> 请求。</li>
<li><code>Expires</code>：指定响应的过期时间，可以使代理服务器或浏览器将加载的内容更新到缓存中。如果再次访问时，就可以直接从缓存中加载，降低服务器负载，缩短加载时间。</li>
</ul>
<h2 id="响应体"><a href="#响应体" class="headerlink" title="响应体"></a>响应体</h2><p>最重要的当属响应体的内容了。响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的 <code>HTML</code> 代码；请求一张图片时，它的响应体就是图片的二进制数据。我们做爬虫请求网页后，要解析的内容就是响应体，如图所示。</p>
<p><img src="/../Pic/Spider/response_body.jpg" style="zoom:67%;"></p>
<p>在浏览器开发者工具中点击 Preview，就可以看到网页的源代码，也就是响应体的内容，它是解析的目标。</p>
<p>在做爬虫时，我们主要通过响应体得到网页的源代码、JSON 数据等，然后从中做相应内容的提取。</p>
]]></content>
      <categories>
        <category>Spider</category>
      </categories>
  </entry>
  <entry>
    <title>Python深度学习（三）深度学习用于文本和序列</title>
    <url>/posts/a98b7769.html</url>
    <content><![CDATA[<p>本节将介绍使用深度学习模型处理文本（可以将其理解为单词序列或字符序列）、时间序列和一般的序列数据。用于处理序列的两种基本的深度学习算法分别是循环神经网络（recurrent neural network）和一维卷积神经网络（1D convnet），后者是二维卷积神经网络的一维版本。下面将讨论这两种方法。<br>这些算法的应用包括：</p>
<ul>
<li>文档分类和时间序列分类，比如识别文章的主题或书的作者；</li>
<li>时间序列对比，比如估测两个文档或两支股票行情的相关程度；</li>
<li>序列到序列的学习，比如将英语翻译成法语；</li>
<li>情感分析，比如将推文或电影评论的情感划分为正面或负面；</li>
<li>时间序列预测，比如根据某地最近的天气数据来预测未来天气。</li>
</ul>
<p>本节的示例重点讨论两个小任务：一个是IMDB数据集的情感分析，这个任务前面介绍过；另一个是温度预测。但这两个任务中所使用的技术可以应用于上面列出来的所有应用。</p>
<h1 id="一、处理文本数据"><a href="#一、处理文本数据" class="headerlink" title="一、处理文本数据"></a>一、处理文本数据</h1><p>文本是最常用的序列数据之一，可以理解为字符序列或单词序列，但最常见的是单词级处理。后面几节介绍的深度学习序列处理模型都可以根据文本生成基本形式的自然语言理解，并可用于文档分类、情感分析、作者识别甚至问答（QA，在有限的语境下）等应用。当然，请记住，这些深度学习模型都没有像人类一样真正地理解文本，而只是映射出书面语言的统计结构，但这足以解决许多简单的文本任务。深度学习用于自然语言处理是将模式识别应用于单词、句子和段落，这与计算机视觉是将模式识别应用于像素大致相同。</p>
<p>与其他所有神经网络一样，深度学习模型不会接收原始文本作为输入，它只能处理数值张量。文本<strong>向量化</strong>（vectorize）是指将文本转换为数值张量的过程。它有多种实现方法。</p>
<ul>
<li>将文本分割为单词，并将每个单词转换为一个向量。</li>
<li>将文本分割为字符，并将每个字符转换为一个向量。</li>
<li>提取单词或字符的<code>n-gram</code>，并将每个<code>n-gram</code>转换为一个向量。<code>n-gram</code>是多个连续单词或字符的集合（<code>n-gram</code>之间可重叠）。</li>
</ul>
<p>将文本分解而成的单元（单词、字符或<code>n-gram</code>）叫作标记（token），将文本分解成标记的过程叫作分词（tokenization）。所有文本向量化过程都是应用某种分词方案，然后将数值向量与生成的标记相关联。这些向量组合成序列张量，被输入到深度神经网络中。将向量与标记相关联的方法有很多种。本节将介绍两种主要方法：对标记做<code>one-hot</code>编码（one-hot encoding）与标记嵌入［token embedding，通常只用于单词，叫作词嵌入（word embedding）］。<br>本节剩余内容将解释这些方法，并介绍如何使用这些方法，将原始文本转换为可以输入到<code>Keras</code>网络中的<code>Numpy</code>张量。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\从文本到标记再到向量.png" width="500" height="500" alt="从文本到标记再到向量" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">从文本到标记再到向量</div>
</center>

<h2 id="1-单词和字符的one-hot-编码"><a href="#1-单词和字符的one-hot-编码" class="headerlink" title="1. 单词和字符的one-hot 编码"></a>1. 单词和字符的one-hot 编码</h2><p>one-hot编码是将标记转换为向量的最常用、最基本的方法。在IMDB和路透社两个例子中，你已经用过这种方法（都是处理单词）。它将每个单词与一个唯一的整数索引相关联，然后将这个整数索引 $i$ 转换为长度为 $N$ 的二进制向量（$N$ 是词表大小），这个向量只有第 $i$ 个元素是1，其余元素都为0。<br>当然，也可以进行字符级的one-hot编码。为了让你完全理解什么是<code>one-hot</code>编码以及如何实现<code>one-hot</code>编码，下面给出了两个简单示例，一个是单词级的<code>one-hot</code>编码，另一个是字符级的<code>one-hot</code>编码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 单词级的one-hot 编码（简单示例）</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">samples = [<span class="string">'The cat sat on the mat.'</span>, <span class="string">'The dog ate my homework.'</span>]</span><br><span class="line">token_index = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> sample <span class="keyword">in</span> samples:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sample.split():</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> token_index:</span><br><span class="line">            token_index[word] = len(token_index) + <span class="number">1</span></span><br><span class="line">max_length = <span class="number">10</span></span><br><span class="line">results = np.zeros(shape=(len(samples),max_length,max(token_index.values()) + <span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(samples):</span><br><span class="line">    <span class="keyword">for</span> j, word <span class="keyword">in</span> list(enumerate(sample.split()))[:max_length]:</span><br><span class="line">        index = token_index.get(word)</span><br><span class="line">        results[i, j, index] = <span class="number">1.</span></span><br></pre></td></tr></table></figure>
<pre><code>array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],

       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 字符级的one-hot 编码（简单示例）</span></span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line">samples = [<span class="string">'The cat sat on the mat.'</span>, <span class="string">'The dog ate my homework.'</span>]</span><br><span class="line">characters = string.printable</span><br><span class="line">token_index = dict(zip(range(<span class="number">1</span>, len(characters) + <span class="number">1</span>), characters))</span><br><span class="line">max_length = <span class="number">50</span></span><br><span class="line">results = np.zeros((len(samples), max_length, max(token_index.keys()) + <span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(samples):</span><br><span class="line">    <span class="keyword">for</span> j, character <span class="keyword">in</span> enumerate(sample):</span><br><span class="line">        index = token_index.get(character)</span><br><span class="line">        results[i, j, index] = <span class="number">1.</span></span><br></pre></td></tr></table></figure>
<pre><code>array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]])
</code></pre><p>注意，Keras 的内置函数可以对原始文本数据进行单词级或字符级的<code>one-hot</code>编码。你应该使用这些函数，因为它们实现了许多重要的特性，比如从字符串中去除特殊字符、只考虑数据集中前 $N$ 个最常见的单词（这是一种常用的限制，以避免处理非常大的输入向量空间）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用Keras实现单词级的one-hot编码</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line">samples = [<span class="string">'The cat sat on the mat.'</span>, <span class="string">'The dog ate my homework.'</span>]</span><br><span class="line">tokenizer = Tokenizer(num_words=<span class="number">1000</span>)</span><br><span class="line">tokenizer.fit_on_texts(samples)</span><br><span class="line">sequences = tokenizer.texts_to_sequences(samples)</span><br><span class="line">one_hot_results = tokenizer.texts_to_matrix(samples, mode=<span class="string">'binary'</span>)</span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line">print(<span class="string">'Found %s unique tokens.'</span> % len(word_index))</span><br></pre></td></tr></table></figure>
<p>one-hot 编码的一种变体是所谓的one-hot散列技巧（one-hot hashing trick），如果词表中唯一标记的数量太大而无法直接处理，就可以使用这种技巧。这种方法没有为每个单词显式分配一个索引并将这些索引保存在一个字典中，而是将单词散列编码为固定长度的向量，通常用一个非常简单的散列函数来实现。这种方法的主要优点在于，它避免了维护一个显式的单词索引，从而节省内存并允许数据的在线编码（在读取完所有数据之前，你就可以立刻生成标记向量）。</p>
<p>这种方法有一个缺点，就是可能会出现散列冲突（hash collision），即两个不同的单词可能具有相同的散列值，随后任何机器学习模型观察这些散列值，都无法区分它们所对应的单词。如果散列空间的维度远大于需要散列的唯一标记的个数，散列冲突的可能性会减小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用散列技巧的单词级的one-hot编码（简单示例）</span></span><br><span class="line">samples = [<span class="string">'The cat sat on the mat.'</span>, <span class="string">'The dog ate my homework.'</span>]</span><br><span class="line">dimensionality = <span class="number">1000</span></span><br><span class="line">max_length = <span class="number">10</span></span><br><span class="line">results = np.zeros((len(samples), max_length, dimensionality))</span><br><span class="line"><span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(samples):</span><br><span class="line">    <span class="keyword">for</span> j, word <span class="keyword">in</span> list(enumerate(sample.split()))[:max_length]:</span><br><span class="line">        index = abs(hash(word)) % dimensionality</span><br><span class="line">        results[i, j, index] = <span class="number">1.</span></span><br></pre></td></tr></table></figure>
<pre><code>array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]])
</code></pre><h2 id="2-使用词嵌入"><a href="#2-使用词嵌入" class="headerlink" title="2. 使用词嵌入"></a>2. 使用词嵌入</h2><p>将单词与向量相关联还有另一种常用的强大方法，就是使用密集的词向量（word vector），也叫词嵌入（word embedding）。one-hot 编码得到的向量是二进制的、稀疏的（绝大部分元素都是0）、维度很高的（维度大小等于词表中的单词个数），而词嵌入是低维的浮点数向量（即密集向量，与稀疏向量相对）。与one-hot 编码得到的词向量不同，词嵌入是从数据中学习得到的。常见的词向量维度是256、512 或1024（处理非常大的词表时）。与此相对，onehot编码的词向量维度通常为20 000 或更高（对应包含20 000 个标记的词表）。因此，词向量可以将更多的信息塞入更低的维度中。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\encoding_1.png" width="300" height="300" alt="one-hot 编码或one-hot 散列得到的词表示是稀疏的、高维的、硬编码的，
而词嵌入是密集的、相对低维的，而且是从数据中学习得到的" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">one-hot 编码或one-hot 散列得到的词表示是稀疏的、高维的、硬编码的，
而词嵌入是密集的、相对低维的，而且是从数据中学习得到的</div>
</center>

<p>获取词嵌入有两种方法。</p>
<ul>
<li>在完成主任务（比如文档分类或情感预测）的同时学习词嵌入。在这种情况下，一开始是随机的词向量，然后对这些词向量进行学习，其学习方式与学习神经网络的权重相同</li>
<li>在不同于待解决问题的机器学习任务上预计算好词嵌入，然后将其加载到模型中。这些词嵌入叫作预训练词嵌入（pretrained word embedding）</li>
</ul>
<p><strong>1. 利用Embedding 层学习词嵌入</strong></p>
<p>要将一个词与一个密集向量相关联，最简单的方法就是随机选择向量。这种方法的问题在于，得到的嵌入空间没有任何结构。例如，accurate 和exact 两个词的嵌入可能完全不同，尽管它们在大多数句子里都是可以互换的。深度神经网络很难对这种杂乱的、非结构化的嵌入空间进行学习。</p>
<p>说得更抽象一点，词向量之间的几何关系应该表示这些词之间的语义关系。词嵌入的作用应该是将人类的语言映射到几何空间中。例如，在一个合理的嵌入空间中，同义词应该被嵌入到相似的词向量中，一般来说，任意两个词向量之间的几何距离（比如L2 距离）应该和这两个词的语义距离有关（表示不同事物的词被嵌入到相隔很远的点，而相关的词则更加靠近）。除了距离，你可能还希望嵌入空间中的特定方向也是有意义的。为了更清楚地说明这一点，我们来看一个具体示例。</p>
<p>在下图中，四个词被嵌入在二维平面上，这四个词分别是cat（猫）、dog（狗）、wolf（狼）和tiger（虎）。对于我们这里选择的向量表示，这些词之间的某些语义关系可以被编码为几何变换。例如，从cat 到tiger 的向量与从dog 到wolf 的向量相等，这个向量可以被解释为“从宠物到野生动物”向量。同样，从dog 到cat 的向量与从wolf 到tiger 的向量也相等，它可以被解释为“从犬科到猫科”向量。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\encoding_2.png" width="200" height="200" alt="词嵌入空间的简单示例" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">词嵌入空间的简单示例</div>
</center>

<p>在真实的词嵌入空间中，常见的有意义的几何变换的例子包括“性别”向量和“复数”向量。例如，将king（国王）向量加上female（女性）向量，得到的是queen（女王）向量。将king（国王）向量加上plural（复数）向量，得到的是kings 向量。词嵌入空间通常具有几千个这种可解释的、并且可能很有用的向量。</p>
<p>有没有一个理想的词嵌入空间，可以完美地映射人类语言，并可用于所有自然语言处理任务？可能有，但我们尚未发现。此外，也不存在人类语言（human language）这种东西。世界上有许多种不同的语言，而且它们不是同构的，因为语言是特定文化和特定环境的反射。但从更实际的角度来说，一个好的词嵌入空间在很大程度上取决于你的任务。英语电影评论情感分析模型的完美词嵌入空间，可能不同于英语法律文档分类模型的完美词嵌入空间，因为某些语义关系的重要性因任务而异。</p>
<p>因此，合理的做法是对每个新任务都学习一个新的嵌入空间。幸运的是，反向传播让这种学习变得很简单，而Keras 使其变得更简单。我们要做的就是学习一个层的权重，这个层就是Embedding 层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将一个Embedding 层实例化</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Embedding</span><br><span class="line">embedding_layer = Embedding(<span class="number">1000</span>, <span class="number">64</span>)</span><br></pre></td></tr></table></figure>
<p>最好将Embedding层理解为一个字典，将整数索引（表示特定单词）映射为密集向量。它接收整数作为输入，并在内部字典中查找这些整数，然后返回相关联的向量。Embedding 层实际上是一种字典查找</p>
<center>
    <img src="\Pic\DeepLearning_Pic\encoding_3.png" width="400" height="400" alt="Embedding层" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Embedding层</div>
</center>

<p>Embedding 层的输入是一个二维整数张量，其形状为(samples, sequence_length)，每个元素是一个整数序列。它能够嵌入长度可变的序列，例如，对于前一个例子中的Embedding 层，你可以输入形状为(32, 10)（32 个长度为10 的序列组成的批量）或(64,15)（64 个长度为15 的序列组成的批量）的批量。不过一批数据中的所有序列必须具有相同的长度（因为需要将它们打包成一个张量），所以较短的序列应该用0 填充，较长的序列应该被截断。</p>
<p>这个Embedding层返回一个形状为$(samples, sequence_length, embedding_dimensionality)$的三维浮点数张量。然后可以用RNN 层或一维卷积层来处理这个三维张量（二者都会在后面介绍）。</p>
<p>将一个Embedding层实例化时，它的权重（即标记向量的内部字典）最开始是随机的，与其他层一样。在训练过程中，利用反向传播来逐渐调节这些词向量，改变空间结构以便下游模型可以利用。一旦训练完成，嵌入空间将会展示大量结构，这种结构专门针对训练模型所要解决的问题。</p>
<p>我们将这个想法应用于你熟悉的IMDB 电影评论情感预测任务。首先，我们需要快速准备数据。将电影评论限制为前10 000个最常见的单词（第一次处理这个数据集时就是这么做的），然后将评论长度限制为只有20个单词。对于这10 000个单词，网络将对每个词都学习一个8维嵌入，将输入的整数序列（二维整数张量）转换为嵌入序列（三维浮点数张量），然后将这个张量展平为二维，最后在上面训练一个Dense层用于分类。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载IMDB数据，准备用于Embedding层</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> preprocessing</span><br><span class="line">max_features = <span class="number">10000</span></span><br><span class="line">maxlen = <span class="number">20</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = imdb.load_data(</span><br><span class="line">num_words=max_features)</span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在IMDB数据上使用Embedding层和分类器</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Flatten, Dense, Embedding</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(<span class="number">10000</span>, <span class="number">8</span>, input_length=maxlen))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'acc'</span>])</span><br><span class="line">model.summary()</span><br><span class="line">history = model.fit(x_train, y_train,</span><br><span class="line">epochs=<span class="number">10</span>,</span><br><span class="line">batch_size=<span class="number">32</span>,</span><br><span class="line">validation_split=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_2 (Embedding)      (None, 20, 8)             80000     
_________________________________________________________________
flatten_1 (Flatten)          (None, 160)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 161       
=================================================================
Total params: 80,161
Trainable params: 80,161
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>得到的验证精度约为76%，考虑到仅查看每条评论的前20个单词，这个结果还是相当不错的。但请注意，仅仅将嵌入序列展开并在上面训练一个Dense层，会导致模型对输入序列中的每个单词单独处理，而没有考虑单词之间的关系和句子结构（举个例子，这个模型可能会将thismovie is a bomb 和this movie is the bomb 两条都归为负面评论a）。更好的做法是在嵌入序列上添加循环层或一维卷积层，将每个序列作为整体来学习特征。这也是接下来几节的重点。</p>
<p><strong>2. 使用预训练的词嵌入</strong><br>有时可用的训练数据很少，以至于只用手头数据无法学习适合特定任务的词嵌入。那么应该怎么办？</p>
<p>你可以从预计算的嵌入空间中加载嵌入向量（你知道这个嵌入空间是高度结构化的，并且具有有用的属性，即抓住了语言结构的一般特点），而不是在解决问题的同时学习词嵌入。在自然语言处理中使用预训练的词嵌入，其背后的原理与在图像分类中使用预训练的卷积神经网络是一样的：没有足够的数据来自己学习真正强大的特征，但你需要的特征应该是非常通用的，比如常见的视觉特征或语义特征。在这种情况下，重复使用在其他问题上学到的特征，这种做<br>法是有道理的。</p>
<p>这种词嵌入通常是利用词频统计计算得出的（观察哪些词共同出现在句子或文档中），用到的技术很多，有些涉及神经网络，有些则不涉及。Bengio等人在21 世纪初首先研究了一种思路，就是用无监督的方法计算一个密集的低维词嵌入空间，但直到最有名且最成功的词嵌入方案之一word2vec 算法发布之后，这一思路才开始在研究领域和工业应用中取得成功。word2vec算法由Google的Tomas Mikolov于2013 年开发，其维度抓住了特定的语义属性，比如性别。有许多预计算的词嵌入数据库， 你都可以下载并在Keras的Embedding层中使用。word2vec就是其中之一。另一个常用的是GloVe（global vectors for word representation，词表示全局向量），由斯坦福大学的研究人员于2014 年开发。这种嵌入方法基于对词共现统计矩阵进行因式分解。其开发者已经公开了数百万个英文标记的预计算嵌入，它们都是从维基百科数据和Common Crawl 数据得到的。</p>
<p>我们来看一下如何在Keras 模型中使用GloVe嵌入。同样的方法也适用于word2vec 嵌入或其他词嵌入数据库。这个例子还可以改进前面刚刚介绍过的文本分词技术，即从原始文本开始，一步步进行处理。</p>
<h2 id="3-从原始文本到词嵌入"><a href="#3-从原始文本到词嵌入" class="headerlink" title="3. 从原始文本到词嵌入"></a>3. 从原始文本到词嵌入</h2><p>本节的模型与之前刚刚见过的那个类似：将句子嵌入到向量序列中，然后将其展平，最后在上面训练一个Dense层。但此处将使用预训练的词嵌入。此外，我们将从头开始，先下载IMDB 原始文本数据，而不是使用Keras内置的已经预先分词的IMDB 数据。</p>
<blockquote>
<p>下载IMDB 数据的原始文本</p>
</blockquote>
<p>首先，打开<a href="http://mng.bz/0tIo" target="_blank" rel="noopener">http://mng.bz/0tIo</a> ，下载原始IMDB 数据集并解压。接下来，我们将训练评论转换成字符串列表，每个字符串对应一条评论。你也可以将评论标签（正面/ 负面）转换成labels列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 处理IMDB原始数据的标签</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">imdb_dir = <span class="string">'data/aclImdb'</span></span><br><span class="line">train_dir = os.path.join(imdb_dir, <span class="string">'train'</span>)</span><br><span class="line">labels = []</span><br><span class="line">texts = []</span><br><span class="line"><span class="keyword">for</span> label_type <span class="keyword">in</span> [<span class="string">'neg'</span>, <span class="string">'pos'</span>]:</span><br><span class="line">    dir_name = os.path.join(train_dir, label_type)</span><br><span class="line">    <span class="keyword">for</span> fname <span class="keyword">in</span> os.listdir(dir_name):</span><br><span class="line">        <span class="keyword">if</span> fname[<span class="number">-4</span>:] == <span class="string">'.txt'</span>:</span><br><span class="line">            f = open(os.path.join(dir_name, fname))</span><br><span class="line">            texts.append(f.read())</span><br><span class="line">            f.close()</span><br><span class="line">            <span class="keyword">if</span> label_type == <span class="string">'neg'</span>:</span><br><span class="line">                labels.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                labels.append(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>对数据进行分词</p>
</blockquote>
<p>利用本节前面介绍过的概念，我们对文本进行分词，并将其划分为训练集和验证集。因为预训练的词嵌入对训练数据很少的问题特别有用（否则，针对于具体任务的嵌入可能效果更好），所以我们又添加了以下限制：将训练数据限定为前200个样本。因此，你需要在读取200个样本之后学习对电影评论进行分类。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对IMDB原始数据的文本进行分词</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">maxlen = <span class="number">100</span></span><br><span class="line">training_samples = <span class="number">200</span></span><br><span class="line">validation_samples = <span class="number">10000</span></span><br><span class="line">max_words = <span class="number">10000</span></span><br><span class="line">tokenizer = Tokenizer(num_words=max_words)</span><br><span class="line">tokenizer.fit_on_texts(texts)</span><br><span class="line">sequences = tokenizer.texts_to_sequences(texts)</span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line">print(<span class="string">'Found %s unique tokens.'</span> % len(word_index))</span><br><span class="line">data = pad_sequences(sequences, maxlen=maxlen)</span><br><span class="line">labels = np.asarray(labels)</span><br><span class="line">print(<span class="string">'Shape of data tensor:'</span>, data.shape)</span><br><span class="line">print(<span class="string">'Shape of label tensor:'</span>, labels.shape)</span><br><span class="line">indices = np.arange(data.shape[<span class="number">0</span>])</span><br><span class="line">np.random.shuffle(indices)</span><br><span class="line">data = data[indices]</span><br><span class="line">labels = labels[indices]</span><br><span class="line">x_train = data[:training_samples]</span><br><span class="line">y_train = labels[:training_samples]</span><br><span class="line">x_val = data[training_samples: training_samples + validation_samples]</span><br><span class="line">y_val = labels[training_samples: training_samples + validation_samples]</span><br></pre></td></tr></table></figure>
<pre><code>Found 88582 unique tokens.
Shape of data tensor: (25000, 100)
Shape of label tensor: (25000,)
</code></pre><blockquote>
<p>下载GloVe词嵌入</p>
</blockquote>
<p>打开<a href="https://nlp.stanford.edu/projects/glove" target="_blank" rel="noopener">https://nlp.stanford.edu/projects/glove</a> ，下载2014年英文维基百科的预计算嵌入。这是一个822 MB的压缩文件，文件名是glove.6B.zip，里面包含400 000 个单词（或非单词的标记）的100 维嵌入向量，解压文件。</p>
<blockquote>
<p>对嵌入进行预处理</p>
</blockquote>
<p>我们对解压后的文件（一个.txt 文件）进行解析，构建一个将单词（字符串）映射为其向量表示（数值向量）的索引。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 解析GloVe词嵌入文件</span></span><br><span class="line">glove_dir = <span class="string">'model/glove.6B'</span></span><br><span class="line">embeddings_index = &#123;&#125;</span><br><span class="line">f = open(os.path.join(glove_dir, <span class="string">'glove.6B.100d.txt'</span>))</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">    values = line.split()</span><br><span class="line">    word = values[<span class="number">0</span>]</span><br><span class="line">    coefs = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">'float32'</span>)</span><br><span class="line">    embeddings_index[word] = coefs</span><br><span class="line">f.close()</span><br><span class="line">print(<span class="string">'Found %s word vectors.'</span> % len(embeddings_index))</span><br></pre></td></tr></table></figure>
<pre><code>Found 400000 word vectors.
</code></pre><p>接下来，需要构建一个可以加载到<code>Embedding</code>层中的嵌入矩阵。它必须是一个形状为<code>(max_words, embedding_dim)</code> 的矩阵，对于单词索引（在分词时构建）中索引为$i$的单词，这个矩阵的元素$i$就是这个单词对应的<code>embedding_dim</code>维向量。注意，索引0不应该代表任何单词或标记，它只是一个占位符。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准备GloVe词嵌入矩阵</span></span><br><span class="line">embedding_dim = <span class="number">100</span></span><br><span class="line">embedding_matrix = np.zeros((max_words, embedding_dim))</span><br><span class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> word_index.items():</span><br><span class="line">    <span class="keyword">if</span> i &lt; max_words:</span><br><span class="line">        embedding_vector = embeddings_index.get(word)</span><br><span class="line">        <span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            embedding_matrix[i] = embedding_vector</span><br></pre></td></tr></table></figure>
<blockquote>
<p>定义模型</p>
</blockquote>
<p>我们将使用与前面相同的模型架构。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Embedding, Flatten, Dense</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(max_words, embedding_dim, input_length=maxlen))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_2&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_3 (Embedding)      (None, 100, 100)          1000000   
_________________________________________________________________
flatten_2 (Flatten)          (None, 10000)             0         
_________________________________________________________________
dense_2 (Dense)              (None, 32)                320032    
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 33        
=================================================================
Total params: 1,320,065
Trainable params: 1,320,065
Non-trainable params: 0
_________________________________________________________________
</code></pre><blockquote>
<p>在模型中加载GloVe嵌入</p>
</blockquote>
<p>Embedding层只有一个权重矩阵，是一个二维的浮点数矩阵，其中每个元素$i$是与索引$i$相关联的词向量。将准备好的<code>GloVe</code>矩阵加载到<code>Embedding</code>层中，即模型的第一层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将预训练的词嵌入加载到Embedding层中</span></span><br><span class="line">model.layers[<span class="number">0</span>].set_weights([embedding_matrix])</span><br><span class="line">model.layers[<span class="number">0</span>].trainable = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>此外，需要冻结<code>Embedding</code>层（即将其<code>trainable</code>属性设为<code>False</code>），其原理和预训练的卷积神经网络特征相同，你已经很熟悉了。如果一个模型的一部分是经过预训练的（如<code>Embedding</code>层），而另一部分是随机初始化的（如分类器），那么在训练期间不应该更新预训练的部分，以避免丢失它们所保存的信息。随机初始化的层会引起较大的梯度更新，会破坏已经学到的特征。</p>
<blockquote>
<p>训练模型与评估模型</p>
</blockquote>
<p>下面编译并训练模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'acc'</span>])</span><br><span class="line">history = model.fit(x_train, y_train,</span><br><span class="line">            epochs=<span class="number">10</span>,</span><br><span class="line">            batch_size=<span class="number">32</span>,</span><br><span class="line">            validation_data=(x_val, y_val))</span><br><span class="line">model.save_weights(<span class="string">'model/pre_trained_glove_model.h5'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">acc = history.history[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_acc'</span>]</span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line">epochs = range(<span class="number">1</span>, len(acc) + <span class="number">1</span>)</span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_29_11.png" width="400" height="400" alt="b" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_30_0.png" width="400" height="400" alt="b" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>



<p>模型很快就开始过拟合，考虑到训练样本很少，这一点也不奇怪。出于同样的原因，验证精度的波动很大，但似乎达到了接近60%。</p>
<p>你也可以在不加载预训练词嵌入、也不冻结嵌入层的情况下训练相同的模型。在这种情况下，你将会学到针对任务的输入标记的嵌入。如果有大量的可用数据，这种方法通常比预训练词嵌入更加强大，但本例只有200个训练样本。我们来试一下这种方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在不使用预训练词嵌入的情况下，训练相同的模型</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Embedding, Flatten, Dense</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(max_words, embedding_dim, input_length=maxlen))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.summary()</span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">        loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">        metrics=[<span class="string">'acc'</span>])</span><br><span class="line">history = model.fit(x_train, y_train,</span><br><span class="line">        epochs=<span class="number">10</span>,</span><br><span class="line">        batch_size=<span class="number">32</span>,</span><br><span class="line">        validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_3&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_4 (Embedding)      (None, 100, 100)          1000000   
_________________________________________________________________
flatten_3 (Flatten)          (None, 10000)             0         
_________________________________________________________________
dense_4 (Dense)              (None, 32)                320032    
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 33        
=================================================================
Total params: 1,320,065
Trainable params: 1,320,065
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>验证精度停留在50% 多一点。因此，在本例中，预训练词嵌入的性能要优于与任务一起学习的嵌入。如果增加样本数量，情况将很快发生变化，你可以把它作为一个练习。</p>
<p>最后，我们在测试数据上评估模型。首先，你需要对测试数据进行分词。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_dir = os.path.join(imdb_dir, <span class="string">'test'</span>)</span><br><span class="line">labels = []</span><br><span class="line">texts = []</span><br><span class="line"><span class="keyword">for</span> label_type <span class="keyword">in</span> [<span class="string">'neg'</span>, <span class="string">'pos'</span>]:</span><br><span class="line">    dir_name = os.path.join(test_dir, label_type)</span><br><span class="line">    <span class="keyword">for</span> fname <span class="keyword">in</span> sorted(os.listdir(dir_name)):</span><br><span class="line">        <span class="keyword">if</span> fname[<span class="number">-4</span>:] == <span class="string">'.txt'</span>:</span><br><span class="line">            f = open(os.path.join(dir_name, fname))</span><br><span class="line">            texts.append(f.read())</span><br><span class="line">            f.close()</span><br><span class="line">            <span class="keyword">if</span> label_type == <span class="string">'neg'</span>:</span><br><span class="line">                labels.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                labels.append(<span class="number">1</span>)</span><br><span class="line">sequences = tokenizer.texts_to_sequences(texts)</span><br><span class="line">x_test = pad_sequences(sequences, maxlen=maxlen)</span><br><span class="line">y_test = np.asarray(labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.load_weights(<span class="string">'model/pre_trained_glove_model.h5'</span>)</span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<pre><code>25000/25000 [==============================] - 1s 44us/step
[0.7739053187370301, 0.5541599988937378]
</code></pre><p>我们只用了很少的训练样本，得到这样的结果很不容易。</p>
<h1 id="二、理解循环神经网络"><a href="#二、理解循环神经网络" class="headerlink" title="二、理解循环神经网络"></a>二、理解循环神经网络</h1><p>目前你见过的所有神经网络（比如密集连接网络和卷积神经网络）都有一个主要特点，那就是它们都没有记忆。它们单独处理每个输入，在输入与输入之间没有保存任何状态。对于这样的网络，要想处理数据点的序列或时间序列，你需要向网络同时展示整个序列，即将序列转换成单个数据点。例如，你在IMDB 示例中就是这么做的：将全部电影评论转换为一个大向量，然后一次性处理。这种网络叫作前馈网络（feedforward network）。</p>
<p>与此相反，当你在阅读这个句子时，你是一个词一个词地阅读（或者说，眼睛一次扫视一次扫视地阅读），同时会记住之前的内容。这让你能够动态理解这个句子所传达的含义。生物智能以渐进的方式处理信息，同时保存一个关于所处理内容的内部模型，这个模型是根据过去的信息构建的，并随着新信息的进入而不断更新。</p>
<p>循环神经网络（RNN，recurrent neural network）采用同样的原理，不过是一个极其简化的版本：它处理序列的方式是，遍历所有序列元素，并保存一个状态（state），其中包含与已查看内容相关的信息。实际上，RNN 是一类具有内部环的神经网络。在处理两个不同的独立序列（比如两条不同的IMDB 评论）之间，RNN 状态会被重置，因此，你仍可以将一个序列看作单个数据点，即网络的单个输入。真正改变的是，数据点不再是在单个步骤中进行处理，相反，网络内部会对序列元素进行遍历。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\rnn1.png" width="200" height="200" alt="循环网络：带有环的网络" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">循环网络：带有环的网络</div>
</center>

<p>为了将环（loop）和状态的概念解释清楚，我们用Numpy来实现一个简单RNN的前向传递。这个RNN的输入是一个张量序列，我们将其编码成大小为$(timesteps, input_features)$的二维张量。它对时间步（timestep）进行遍历，在每个时间步，它考虑$t$时刻的当前状态与$t$时刻的输入［形状为$(input_ features,)$］，对二者计算得到$t$时刻的输出。然后，我们<strong>将下一个时间步的状态设置为上一个时间步的输出</strong>。对于第一个时间步，上一个时间步的输出没有定义，所以它没有当前状态。因此，你需要将状态初始化为一个全零向量，这叫作网络的初始状态（initial state）。</p>
<p>RNN的伪代码如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">state_t = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> input_t <span class="keyword">in</span> input_sequence:</span><br><span class="line">    output_t = f(input_t, state_t)</span><br><span class="line">    state_t = output_t</span><br></pre></td></tr></table></figure>
<p>你甚至可以给出具体的函数$f$：从输入和状态到输出的变换，其参数包括两个矩阵（$W$和$U$）和一个偏置向量。它类似于前馈网络中密集连接层所做的变换，因此更详细的RNN伪代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">state_t = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> input_t <span class="keyword">in</span> input_sequence:</span><br><span class="line">    output_t = activation(dot(W, input_t) + dot(U, state_t) + b)</span><br><span class="line">    state_t = output_t</span><br></pre></td></tr></table></figure></p>
<p>为了将这些概念的含义解释得更加清楚，我们为简单RNN的前向传播编写一个简单的Numpy实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">timesteps = <span class="number">100</span></span><br><span class="line">input_features = <span class="number">32</span></span><br><span class="line">output_features = <span class="number">64</span></span><br><span class="line">inputs = np.random.random((timesteps, input_features))</span><br><span class="line">state_t = np.zeros((output_features,))</span><br><span class="line">W = np.random.random((output_features, input_features))</span><br><span class="line">U = np.random.random((output_features, output_features))</span><br><span class="line">b = np.random.random((output_features,))</span><br><span class="line">successive_outputs = []</span><br><span class="line"><span class="keyword">for</span> input_t <span class="keyword">in</span> inputs:</span><br><span class="line">    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)</span><br><span class="line">    successive_outputs.append(output_t)</span><br><span class="line">    state_t = output_t</span><br><span class="line">final_output_sequence = np.stack(successive_outputs, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>总之，RNN 是一个for 循环，它重复使用循环前一次迭代的计算结果，仅此而已。当然，你可以构建许多不同的RNN，它们都满足上述定义。这个例子只是最简单的RNN表述之一。RNN的特征在于其时间步函数，比如前面例子中的这个函数</p>
<h2 id="1-Keras-中的循环层"><a href="#1-Keras-中的循环层" class="headerlink" title="1. Keras 中的循环层"></a>1. Keras 中的循环层</h2><p>上面Numpy 的简单实现，对应一个实际的Keras 层，即SimpleRNN层。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> SimpleRNN</span><br></pre></td></tr></table></figure><br>二者有一点小小的区别：SimpleRNN层能够像其他Keras层一样处理序列批量，而不是像Numpy示例那样只能处理单个序列。因此，它接收形状为<code>(batch_size, timesteps,input_features)</code>的输入，而不是<code>(timesteps, input_features)</code>。</p>
<p>与Keras 中的所有循环层一样，SimpleRNN可以在两种不同的模式下运行：一种是返回每个时间步连续输出的完整序列，即形状为<code>(batch_size, timesteps, output_features)</code>的三维张量；另一种是只返回每个输入序列的最终输出，即形状为<code>(batch_size, output_features)</code>的二维张量。这两种模式由return_sequences 这个构造函数参数来控制。我们来看一个使用SimpleRNN的例子，它只返回最后一个时间步的输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Embedding, SimpleRNN</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(<span class="number">10000</span>, <span class="number">32</span>))</span><br><span class="line">model.add(SimpleRNN(<span class="number">32</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_4&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_5 (Embedding)      (None, None, 32)          320000    
_________________________________________________________________
simple_rnn_1 (SimpleRNN)     (None, 32)                2080      
=================================================================
Total params: 322,080
Trainable params: 322,080
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>下面这个例子返回完整的状态序列</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(<span class="number">10000</span>, <span class="number">32</span>))</span><br><span class="line">model.add(SimpleRNN(<span class="number">32</span>, return_sequences=<span class="literal">True</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_5&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_6 (Embedding)      (None, None, 32)          320000    
_________________________________________________________________
simple_rnn_2 (SimpleRNN)     (None, None, 32)          2080      
=================================================================
Total params: 322,080
Trainable params: 322,080
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>为了提高网络的表示能力，将多个循环层逐个堆叠有时也是很有用的。在这种情况下，你需要让所有中间层都返回完整的输出序列。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(<span class="number">10000</span>, <span class="number">32</span>))</span><br><span class="line">model.add(SimpleRNN(<span class="number">32</span>, return_sequences=<span class="literal">True</span>))</span><br><span class="line">model.add(SimpleRNN(<span class="number">32</span>, return_sequences=<span class="literal">True</span>))</span><br><span class="line">model.add(SimpleRNN(<span class="number">32</span>, return_sequences=<span class="literal">True</span>))</span><br><span class="line">model.add(SimpleRNN(<span class="number">32</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_6&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_7 (Embedding)      (None, None, 32)          320000    
_________________________________________________________________
simple_rnn_3 (SimpleRNN)     (None, None, 32)          2080      
_________________________________________________________________
simple_rnn_4 (SimpleRNN)     (None, None, 32)          2080      
_________________________________________________________________
simple_rnn_5 (SimpleRNN)     (None, None, 32)          2080      
_________________________________________________________________
simple_rnn_6 (SimpleRNN)     (None, 32)                2080      
=================================================================
Total params: 328,320
Trainable params: 328,320
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>接下来，我们将这个模型应用于IMDB电影评论分类问题。首先，对数据进行预处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line">max_features = <span class="number">10000</span></span><br><span class="line">maxlen = <span class="number">500</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">print(<span class="string">'Loading data...'</span>)</span><br><span class="line">(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)</span><br><span class="line">print(len(input_train), <span class="string">'train sequences'</span>)</span><br><span class="line">print(len(input_test), <span class="string">'test sequences'</span>)</span><br><span class="line">print(<span class="string">'Pad sequences (samples x time)'</span>)</span><br><span class="line">input_train = sequence.pad_sequences(input_train, maxlen=maxlen)</span><br><span class="line">input_test = sequence.pad_sequences(input_test, maxlen=maxlen)</span><br><span class="line">print(<span class="string">'input_train shape:'</span>, input_train.shape)</span><br><span class="line">print(<span class="string">'input_test shape:'</span>, input_test.shape)</span><br></pre></td></tr></table></figure>
<pre><code>Loading data...
25000 train sequences
25000 test sequences
Pad sequences (samples x time)
input_train shape: (25000, 500)
input_test shape: (25000, 500)
</code></pre><p>我们用一个Embedding层和一个SimpleRNN层来训练一个简单的循环网络</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(max_features, <span class="number">32</span>))</span><br><span class="line">model.add(SimpleRNN(<span class="number">32</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'acc'</span>])</span><br><span class="line">history = model.fit(input_train, y_train,</span><br><span class="line">                epochs=<span class="number">10</span>,</span><br><span class="line">                batch_size=<span class="number">128</span>,</span><br><span class="line">                validation_split=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Train on 20000 samples, validate on 5000 samples
Epoch 1/10
20000/20000 [==============================] - 17s 832us/step - loss: 0.6298 - acc: 0.6252 - val_loss: 0.5190 - val_acc: 0.7454
Epoch 2/10
20000/20000 [==============================] - 17s 832us/step - loss: 0.4073 - acc: 0.8216 - val_loss: 0.3733 - val_acc: 0.8438
Epoch 3/10
20000/20000 [==============================] - 17s 844us/step - loss: 0.2946 - acc: 0.8804 - val_loss: 0.3948 - val_acc: 0.8318
Epoch 4/10
20000/20000 [==============================] - 17s 856us/step - loss: 0.2396 - acc: 0.9059 - val_loss: 0.4578 - val_acc: 0.8388
Epoch 5/10
20000/20000 [==============================] - 17s 870us/step - loss: 0.2003 - acc: 0.9233 - val_loss: 0.4395 - val_acc: 0.8168
Epoch 6/10
20000/20000 [==============================] - 18s 887us/step - loss: 0.1601 - acc: 0.9406 - val_loss: 0.5268 - val_acc: 0.8204
Epoch 7/10
20000/20000 [==============================] - 17s 874us/step - loss: 0.1124 - acc: 0.9612 - val_loss: 0.5143 - val_acc: 0.8048
Epoch 8/10
20000/20000 [==============================] - 18s 914us/step - loss: 0.0923 - acc: 0.9682 - val_loss: 0.4945 - val_acc: 0.8362
Epoch 9/10
20000/20000 [==============================] - 18s 877us/step - loss: 0.0507 - acc: 0.9839 - val_loss: 0.5826 - val_acc: 0.8092
Epoch 10/10
20000/20000 [==============================] - 18s 879us/step - loss: 0.0540 - acc: 0.9821 - val_loss: 0.5969 - val_acc: 0.8182
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">acc = history.history[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_acc'</span>]</span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line">epochs = range(<span class="number">1</span>, len(acc) + <span class="number">1</span>)</span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_49_0.png" width="400" height="400" alt="b" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_49_1.png" width="400" height="400" alt="b" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<p>处理这个数据集的第一个简单方法得到的测试精度是88%。不幸的是，与这个基准相比，这个小型循环网络的表现并不好（验证精度只有85%）。问题的部分原因在于，输入只考虑了前500 个单词，而不是整个序列，因此，RNN获得的信息比前面的基准模型更少。另一部分原因在于，SimpleRNN不擅长处理长序列，比如文本。其他类型的循环层的表现要好得多。我们来看几个更高级的循环层。</p>
<h2 id="2-理解LSTM层和GRU层"><a href="#2-理解LSTM层和GRU层" class="headerlink" title="2. 理解LSTM层和GRU层"></a>2. 理解LSTM层和GRU层</h2><p>SimpleRNN并不是Keras中唯一可用的循环层，还有另外两个：LSTM和GRU。在实践中总会用到其中之一，因为SimpleRNN通常过于简化，没有实用价值。SimpleRNN 的最大问题是，在时刻t，理论上来说，它应该能够记住许多时间步之前见过的信息，但实际上它是不可能学到这种长期依赖的。其原因在于梯度消失问题（vanishing gradient problem），这一效应类似于在层数较多的非循环网络（即前馈网络）中观察到的效应：随着层数的增加，网络最终变得无法训练。Hochreiter、Schmidhuber和Bengio在20世纪90年代初研究了这一效应的理论原因a。LSTM层和GRU层都是为了解决这个问题而设计的。</p>
<p>先来看LSTM层。其背后的长短期记忆（LSTM，long short-term memory）算法由Hochreiter和Schmidhuber在1997 年开发b，是二人研究梯度消失问题的重要成果。LSTM层是SimpleRNN层的一种变体，它增加了一种携带信息跨越多个时间步的方法。假设有一条传送带，其运行方向平行于你所处理的序列。序列中的信息可以在任意位置跳上传送带，然后被传送到更晚的时间步，并在需要时原封不动地跳回来。这实际上就是LSTM 的原理：它保存信息以便后面使用，从而防止较早期的信号在处理过程中逐渐消失。</p>
<p>为了详细了解LSTM，我们先从SimpleRNN单元开始讲起。因为有许多个权重矩阵，所以对单元中的$W$和$U$两个矩阵添加下标字母$o$（Wo 和Uo），表示输出。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\lstm.png" width="200" height="200" alt="讨论LSTM层的出发点：SimpleRNN层" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">讨论LSTM层的出发点：SimpleRNN层</div>
</center>

<p>我们向这张图像中添加额外的数据流，其中携带着跨越时间步的信息。它在不同的时间步的值叫作$C_t$，其中$C$表示携带（carry）。这些信息将会对单元产生以下影响：它将与输入连接和循环连接进行运算（通过一个密集变换，即与权重矩阵作点积，然后加上一个偏置，再应用一个激活函数），从而影响传递到下一个时间步的状态（通过一个激活函数和一个乘法运算）。从概念上来看，携带数据流是一种调节下一个输出和下一个状态的方法，到目前为<br>止都很简单。</p>
<p>下面来看这一方法的精妙之处，即携带数据流下一个值的计算方法。它涉及三个不同的变换，这三个变换的形式都和SimpleRNN单元相同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = activation(dot(state_t, U) + dot(input_t, W) + b)</span><br></pre></td></tr></table></figure>
<p>但这三个变换都具有各自的权重矩阵，我们分别用字母$i$、$j$和$k$作为下标。目前的模型架构如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output_t = activation(dot(state_t, Uo) + dot(input_t, Wo) + dot(C_t, Vo) + bo)</span><br><span class="line">i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)</span><br><span class="line">f_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)</span><br><span class="line">k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)</span><br><span class="line">c_t+<span class="number">1</span> = i_t * k_t + c_t * f_t</span><br></pre></td></tr></table></figure>
<p>如果要更哲学一点，你还可以解释每个运算的目的。比如你可以说，将<code>c_t</code>和<code>f_t</code>相乘，是为了故意遗忘携带数据流中的不相关信息。同时，<code>i_t</code>和<code>k_t</code>都提供关于当前的信息，可以用新信息来更新携带轨道。但归根结底，这些解释并没有多大意义，因为这些运算的实际效果是由参数化权重决定的，而权重是以端到端的方式进行学习，每次训练都要从头开始，不可能为某个运算赋予特定的目的。RNN单元的类型（如前所述）决定了你的假设空间，即在训练期间搜索良好模型配置的空间，但它不能决定RNN 单元的作用，那是由单元权重来决定的。同一个单元具有不同的权重，可以实现完全不同的作用。因此，组成RNN 单元的运算组合，最好被解释为对搜索的一组约束，而不是一种工程意义上的设计。</p>
<p>对于研究人员来说，这种约束的选择（即如何实现RNN单元）似乎最好是留给最优化算法来完成（比如遗传算法或强化学习过程），而不是让人类工程师来完成。在未来，那将是我们构建网络的方式。总之，你不需要理解关于LSTM单元具体架构的任何内容。作为人类，理解它不应该是你要做的。你只需要记住LSTM单元的作用：允许过去的信息稍后重新进入，从而解决梯度消失问题。</p>
<h2 id="3-Keras中一个LSTM-的具体例子"><a href="#3-Keras中一个LSTM-的具体例子" class="headerlink" title="3. Keras中一个LSTM 的具体例子"></a>3. Keras中一个LSTM 的具体例子</h2><p>现在我们来看一个更实际的问题：使用LSTM层来创建一个模型，然后在IMDB数据上训练模型。这个网络与前面介绍的SimpleRNN网络类似。你只需指定LSTM层的输出维度，其他所有参数（有很多）都使用Keras默认值。Keras具有很好的默认值，无须手动调参，模型通常也能正常运行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LSTM</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(max_features, <span class="number">32</span>))</span><br><span class="line">model.add(LSTM(<span class="number">32</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'acc'</span>])</span><br><span class="line">history = model.fit(input_train, y_train,</span><br><span class="line">                epochs=<span class="number">10</span>,</span><br><span class="line">                batch_size=<span class="number">128</span>,</span><br><span class="line">                validation_split=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Train on 20000 samples, validate on 5000 samples
Epoch 1/10
20000/20000 [==============================] - 44s 2ms/step - loss: 0.5221 - acc: 0.7523 - val_loss: 0.3656 - val_acc: 0.8520
Epoch 2/10
20000/20000 [==============================] - 43s 2ms/step - loss: 0.3005 - acc: 0.8812 - val_loss: 0.3048 - val_acc: 0.8766
Epoch 3/10
20000/20000 [==============================] - 43s 2ms/step - loss: 0.2410 - acc: 0.9086 - val_loss: 0.3063 - val_acc: 0.8700
Epoch 4/10
20000/20000 [==============================] - 43s 2ms/step - loss: 0.2031 - acc: 0.9252 - val_loss: 0.3210 - val_acc: 0.8862
Epoch 5/10
20000/20000 [==============================] - 43s 2ms/step - loss: 0.1816 - acc: 0.9348 - val_loss: 0.3304 - val_acc: 0.8542
Epoch 6/10
20000/20000 [==============================] - 43s 2ms/step - loss: 0.1661 - acc: 0.9412 - val_loss: 0.3082 - val_acc: 0.8778
Epoch 7/10
20000/20000 [==============================] - 44s 2ms/step - loss: 0.1480 - acc: 0.9482 - val_loss: 0.3066 - val_acc: 0.8738
Epoch 8/10
20000/20000 [==============================] - 45s 2ms/step - loss: 0.1429 - acc: 0.9513 - val_loss: 0.4828 - val_acc: 0.8404
Epoch 9/10
20000/20000 [==============================] - 47s 2ms/step - loss: 0.1278 - acc: 0.9554 - val_loss: 0.3790 - val_acc: 0.8838
Epoch 10/10
20000/20000 [==============================] - 49s 2ms/step - loss: 0.1162 - acc: 0.9593 - val_loss: 0.3512 - val_acc: 0.8512
</code></pre><p>LSTM更适用于评论分析全局的长期性结构（这正是LSTM所擅长的），对情感分析问题帮助不大。对于这样的基本问题，观察每条评论中出现了哪些词及其出现频率就可以很好地解决。这也正是第一个全连接方法的做法。但还有更加困难的自然语言处理问题，特别是问答和机器翻译，这时LSTM的优势就明显了。</p>
<h1 id="三、循环神经网络的高级用法"><a href="#三、循环神经网络的高级用法" class="headerlink" title="三、循环神经网络的高级用法"></a>三、循环神经网络的高级用法</h1><p>本节将介绍提高循环神经网络的性能和泛化能力的三种高级技巧。学完本节，你将会掌握用Keras实现循环网络的大部分内容。我们将在温度预测问题中介绍这三个概念。在这个问题中，数据点时间序列来自建筑物屋顶安装的传感器，包括温度、气压、湿度等，你将要利用这些数据来预测最后一个数据点24小时之后的温度。这是一个相当有挑战性的问题，其中包含许多处理时间序列时经常遇到的困难。</p>
<p>我们将会介绍以下三种技巧。</p>
<ul>
<li>循环<code>dropout</code>（recurrent dropout）。这是一种特殊的内置方法，在循环层中使用<code>dropout</code>来降低过拟合</li>
<li>堆叠循环层（stacking recurrent layers）。这会提高网络的表示能力（代价是更高的计算负荷）</li>
<li>双向循环层（bidirectional recurrent layer）。将相同的信息以不同的方式呈现给循环网络，可以提高精度并缓解遗忘问题</li>
</ul>
<h2 id="1-温度预测问题"><a href="#1-温度预测问题" class="headerlink" title="1. 温度预测问题"></a>1. 温度预测问题</h2><p>到目前为止，我们遇到的唯一一种序列数据就是文本数据，比如IMDB数据集和路透社数据集。但除了语言处理，其他许多问题中也都用到了序列数据。在本节的所有例子中，我们将使用一个天气时间序列数据集，它由德国耶拿的马克思• 普朗克生物地球化学研究所的气象站记录。</p>
<p>在这个数据集中，每10分钟记录14个不同的量（比如气温、气压、湿度、风向等），其中包含多年的记录。原始数据可追溯到2003年，但本例仅使用2009—2016年的数据。这个数据集非常适合用来学习处理数值型时间序列。我们将会用这个数据集来构建模型，输入最近的一些数据（几天的数据点），可以预测24小时之后的气温。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">data_dir = <span class="string">'data/climate/'</span></span><br><span class="line">fname = os.path.join(data_dir, <span class="string">'jena_climate_2009_2016.csv'</span>)</span><br><span class="line">f = open(fname)</span><br><span class="line">data = f.read()</span><br><span class="line">f.close()</span><br><span class="line">lines = data.split(<span class="string">'\n'</span>)</span><br><span class="line">header = lines[<span class="number">0</span>].split(<span class="string">','</span>)</span><br><span class="line">lines = lines[<span class="number">1</span>:]</span><br><span class="line">print(header)</span><br><span class="line">print(len(lines))</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;&quot;Date Time&quot;&#39;, &#39;&quot;p (mbar)&quot;&#39;, &#39;&quot;T (degC)&quot;&#39;, &#39;&quot;Tpot (K)&quot;&#39;, &#39;&quot;Tdew (degC)&quot;&#39;, &#39;&quot;rh (%)&quot;&#39;, &#39;&quot;VPmax (mbar)&quot;&#39;, &#39;&quot;VPact (mbar)&quot;&#39;, &#39;&quot;VPdef (mbar)&quot;&#39;, &#39;&quot;sh (g/kg)&quot;&#39;, &#39;&quot;H2OC (mmol/mol)&quot;&#39;, &#39;&quot;rho (g/m**3)&quot;&#39;, &#39;&quot;wv (m/s)&quot;&#39;, &#39;&quot;max. wv (m/s)&quot;&#39;, &#39;&quot;wd (deg)&quot;&#39;]
420451
</code></pre><p>从输出可以看出，共有 420 551 行数据（每行是一个时间步，记录了一个日期和 14 个与天气有关的值），接下来，将 420 551 行数据转换成一个Numpy数组。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">float_data = np.zeros((len(lines), len(header) - <span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(lines):</span><br><span class="line">    values = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> line.split(<span class="string">','</span>)[<span class="number">1</span>:]]</span><br><span class="line">    float_data[i, :] = values</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制温度时间序列</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">temp = float_data[:, <span class="number">1</span>] <span class="comment"># 温度（单位：摄氏度）</span></span><br><span class="line">plt.plot(range(len(temp)), temp)</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_56_1.png" width="400" height="400" alt="b" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>



<p>下图给出了前 10 天温度数据的图像，因为每 10 分钟记录一个数据，所以每天有 144 个数据点。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(range(<span class="number">1440</span>), temp[:<span class="number">1440</span>])</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_58_1.png" width="400" height="400" alt="b" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<p>在这张图中，你可以看到每天的周期性变化，尤其是最后4 天特别明显。另外请注意，这 10 天一定是来自于很冷的冬季月份。</p>
<p>如果你想根据过去几个月的数据来预测下个月的平均温度，那么问题很简单，因为数据具有可靠的年度周期性。但从几天的数据来看，温度看起来更混乱一些。以天作为观察尺度，这个时间序列是可以预测的吗？我们来寻找这个问题的答案。</p>
<h2 id="2-准备数据"><a href="#2-准备数据" class="headerlink" title="2. 准备数据"></a>2. 准备数据</h2><p>这个问题的确切表述如下：一个时间步是 10 分钟，每 <code>steps</code> 个时间步采样一次数据，给定过去 <code>lookback</code> 个时间步之内的数据，能否预测 <code>delay</code> 个时间步之后的温度？用到的参数值如下。</p>
<ul>
<li><code>lookback = 720</code>：给定过去 5 天内的观测数据。</li>
<li><code>steps = 6</code>：观测数据的采样频率是每小时一个数据点。</li>
<li><code>delay = 144</code>：目标是未来 24 小时之后的数据。</li>
</ul>
<p>开始之前，你需要完成以下两件事。</p>
<ul>
<li>将数据预处理为神经网络可以处理的格式。这很简单。数据已经是数值型的，所以不需要做向量化。但数据中的每个时间序列位于不同的范围（比如温度通道位于 -20 到+30 之间，但气压大约在1000 毫巴上下）。你需要对每个时间序列分别做标准化，让它们在相似的范围内都取较小的值。</li>
<li>编写一个 Python 生成器，以当前的浮点数数组作为输入，并从最近的数据中生成数据批量，同时生成未来的目标温度。因为数据集中的样本是高度冗余的（对于第 <code>N</code> 个样本和第 <code>N+1</code> 个样本，大部分时间步都是相同的），所以显式地保存每个样本是一种浪费。相反，我们将使用原始数据即时生成样本。</li>
</ul>
<p>预处理数据的方法是，将每个时间序列减去其平均值，然后除以其标准差。我们将使用前 200 000 个时间步作为训练数据，所以只对这部分数据计算平均值和标准差。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mean = float_data[:<span class="number">200000</span>].mean(axis=<span class="number">0</span>)</span><br><span class="line">float_data -= mean</span><br><span class="line">std = float_data[:<span class="number">200000</span>].std(axis=<span class="number">0</span>)</span><br><span class="line">float_data /= std</span><br></pre></td></tr></table></figure>
<p>下面的代码给出了将要用到的生成器。它生成了一个元组<code>(samples, targets)</code>，其中<code>samples</code>是输入数据的一个批量，<code>targets</code>是对应的目标温度数组。生成器的参数如下：</p>
<ul>
<li><code>data</code>：浮点数数据组成的原始数组，我们已将其标准化。</li>
<li><code>lookback</code>：输入数据应该包括过去多少个时间步。</li>
<li><code>delay</code>：目标应该在未来多少个时间步之后。</li>
<li><code>min_index</code> 和 <code>max_index</code>：data 数组中的索引，用于界定需要抽取哪些时间步。这有助于保存一部分数据用于验证、另一部分用于测试。</li>
<li><code>shuffle</code>：是打乱样本，还是按顺序抽取样本。</li>
<li><code>batch_size</code>：每个批量的样本数。</li>
<li><code>step</code>：数据采样的周期（单位：时间步）。我们将其设为6，为的是每小时抽取一个数据点。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成时间序列样本及其目标的生成器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(data, lookback, delay, min_index, max_index, shuffle=False, batch_size=<span class="number">128</span>, step=<span class="number">6</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> max_index <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        max_index = len(data) - delay - <span class="number">1</span></span><br><span class="line">    i = min_index + lookback</span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> shuffle:</span><br><span class="line">            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> i + batch_size &gt;= max_index:</span><br><span class="line">                i = min_index + lookback</span><br><span class="line">            rows = np.arange(i, min(i + batch_size, max_index))</span><br><span class="line">            i += len(rows)</span><br><span class="line">        samples = np.zeros((len(rows), lookback // step, data.shape[<span class="number">-1</span>]))</span><br><span class="line">        targets = np.zeros((len(rows),))</span><br><span class="line">        <span class="keyword">for</span> j, row <span class="keyword">in</span> enumerate(rows):</span><br><span class="line">            indices = range(rows[j] - lookback, rows[j], step)</span><br><span class="line">            samples[j] = data[indices]</span><br><span class="line">            targets[j] = data[rows[j] + delay][<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> samples, targets</span><br></pre></td></tr></table></figure>
<p>下面，我们使用这个抽象的<code>generator</code>函数来实例化三个生成器：一个用于训练，一个用于验证，还有一个用于测试。每个生成器分别读取原始数据的不同时间段：训练生成器读取前 200 000 个时间步，验证生成器读取随后的 100 000 个时间步，测试生成器读取剩下的时间步。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lookback = <span class="number">1440</span></span><br><span class="line">step = <span class="number">6</span></span><br><span class="line">delay = <span class="number">144</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">train_gen = generator(float_data,</span><br><span class="line">                lookback=lookback,</span><br><span class="line">                delay=delay,</span><br><span class="line">                min_index=<span class="number">0</span>,</span><br><span class="line">                max_index=<span class="number">200000</span>,</span><br><span class="line">                shuffle=<span class="literal">True</span>,</span><br><span class="line">                step=step,</span><br><span class="line">                batch_size=batch_size)</span><br><span class="line">val_gen = generator(float_data,</span><br><span class="line">                lookback=lookback,</span><br><span class="line">                delay=delay,</span><br><span class="line">                min_index=<span class="number">200001</span>,</span><br><span class="line">                max_index=<span class="number">300000</span>,</span><br><span class="line">                step=step,</span><br><span class="line">                batch_size=batch_size)</span><br><span class="line">test_gen = generator(float_data,</span><br><span class="line">                lookback=lookback,</span><br><span class="line">                delay=delay,</span><br><span class="line">                min_index=<span class="number">300001</span>,</span><br><span class="line">                max_index=<span class="literal">None</span>,</span><br><span class="line">                step=step,</span><br><span class="line">                batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">val_steps = (<span class="number">300000</span> - <span class="number">200001</span> - lookback) //batch_size</span><br><span class="line">test_steps = (len(float_data) - <span class="number">300001</span> - lookback) //batch_size</span><br></pre></td></tr></table></figure>
<h2 id="3-一种基于常识的、非机器学习的基准方法"><a href="#3-一种基于常识的、非机器学习的基准方法" class="headerlink" title="3. 一种基于常识的、非机器学习的基准方法"></a>3. 一种基于常识的、非机器学习的基准方法</h2><p>开始使用黑盒深度学习模型解决温度预测问题之前，我们先尝试一种基于常识的简单方法。它可以作为合理性检查，还可以建立一个基准，更高级的机器学习模型需要打败这个基准才能表现出其有效性。面对一个尚没有已知解决方案的新问题时，这种基于常识的基准方法很有用。</p>
<p>一个经典的例子就是不平衡的分类任务，其中某些类别比其他类别更常见。如果数据集中包含 90% 的类别 A 实例和 10% 的类别B 实例，那么分类任务的一种基于常识的方法就是对新样本始终预测类别“A”。这种分类器的总体精度为90%，因此任何基于学习的方法在精度高于90%时才能证明其有效性。有时候，这样基本的基准方法可能很难打败。</p>
<p>本例中，我们可以放心地假设，温度时间序列是连续的（明天的温度很可能接近今天的温度），并且具有每天的周期性变化。因此，一种基于常识的方法就是始终预测 24 小时后的温度等于现在的温度。我们使用平均绝对误差（MAE）指标来评估这种方法。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.mean(np.abs(preds - targets))</span><br></pre></td></tr></table></figure><br>下面是评估的循环代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_naive_method</span><span class="params">()</span>:</span></span><br><span class="line">    batch_maes = []</span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(val_steps):</span><br><span class="line">        samples, targets = next(val_gen)</span><br><span class="line">        preds = samples[:, <span class="number">-1</span>, <span class="number">1</span>]</span><br><span class="line">        mae = np.mean(np.abs(preds - targets))</span><br><span class="line">        batch_maes.append(mae)</span><br><span class="line">    print(np.mean(batch_maes))</span><br><span class="line">evaluate_naive_method()</span><br><span class="line">    <span class="comment"># 0.2897359729905486</span></span><br></pre></td></tr></table></figure>
<p>得到的 MAE 为 0.29。因为温度数据被标准化成均值为0、标准差为1，所以无法直接对这个值进行解释。它转化成温度的平均绝对误差为<code>0.29×temperature_std</code>摄氏度，即2.57℃。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将 MAE 转换成摄氏温度误差</span></span><br><span class="line">celsius_mae = <span class="number">0.29</span> * std[<span class="number">1</span>]</span><br><span class="line">celsius_mae</span><br><span class="line">    <span class="comment"># 2.5672247338393395</span></span><br></pre></td></tr></table></figure>
<p>这个平均绝对误差还是相当大的。接下来的任务是利用深度学习知识来改进结果。</p>
<h2 id="4-一种基本的机器学习方法"><a href="#4-一种基本的机器学习方法" class="headerlink" title="4. 一种基本的机器学习方法"></a>4. 一种基本的机器学习方法</h2><p>在尝试机器学习方法之前，建立一个基于常识的基准方法是很有用的；同样，在开始研究复杂且计算代价很高的模型（比如RNN）之前，尝试使用简单且计算代价低的机器学习模型也是很有用的，比如小型的密集连接网络。这可以保证进一步增加问题的复杂度是合理的，并且会带来真正的好处。</p>
<p>下面代码给出了一个密集连接模型，首先将数据展平，然后通过两个<code>Dense</code>层并运行。注意，最后一个<code>Dense</code>层没有使用激活函数，这对于回归问题是很常见的。我们使用<code>MAE</code>作为损失。评估数据和评估指标都与常识方法完全相同，所以可以直接比较两种方法的结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练并评估一个密集连接模型</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[<span class="number">-1</span>])))</span><br><span class="line">model.add(layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">model.compile(optimizer=RMSprop(), loss=<span class="string">'mae'</span>)</span><br><span class="line">history = model.fit_generator(train_gen,</span><br><span class="line">                        steps_per_epoch=<span class="number">500</span>,</span><br><span class="line">                        epochs=<span class="number">20</span>,</span><br><span class="line">                        validation_data=val_gen,</span><br><span class="line">                        validation_steps=val_steps)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制曲线</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_70_1.png" width="400" height="400" alt="b" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<p>部分验证损失接近不包含学习的基准方法，但这个结果并不可靠。这也展示了首先建立这个基准方法的优点，事实证明，超越这个基准并不容易。我们的常识中包含了大量有价值的信息，而机器学习模型并不知道这些信息。</p>
<p>你可能会问，如果从数据到目标之间存在一个简单且表现良好的模型（即基于常识的基准方法），那为什么我们训练的模型没有找到这个模型并进一步改进呢？原因在于，这个简单的解决方案并不是训练过程所要寻找的目标。我们在模型空间（即假设空间）中搜索解决方案，这个模型空间是具有我们所定义的架构的所有两层网络组成的空间。这些网络已经相当复杂了。如果你在一个复杂模型的空间中寻找解决方案，那么可能无法学到简单且性能良好的基准方法，虽然技术上来说它属于假设空间的一部分。</p>
<p>通常来说，这对机器学习是一个非常重要的限制：如果学习算法没有被硬编码要求去寻找特定类型的简单模型，那么有时候参数学习是无法找到简单问题的简单解决方案的。</p>
<h2 id="5-第一个循环网络基准"><a href="#5-第一个循环网络基准" class="headerlink" title="5. 第一个循环网络基准"></a>5. 第一个循环网络基准</h2><p>第一个全连接方法的效果并不好，但这并不意味着机器学习不适用于这个问题。前一个方法首先将时间序列展平，这从输入数据中删除了时间的概念。我们来看一下数据本来的样子：它是一个序列，其中因果关系和顺序都很重要。我们将尝试一种循环序列处理模型，它应该特别适合这种序列数据，因为它利用了数据点的时间顺序，这与第一个方法不同。</p>
<p>我们将使用 Chung 等人在2014 年开发的 GRU 层，而不是上一节介绍的 LSTM 层。门控循环单元（GRU，gated recurrent unit）层的工作原理与 LSTM 相同。但它做了一些简化，因此运行的计算代价更低（虽然表示能力可能不如LSTM），机器学习中到处可以见到这种计算代价与表示能力之间的折中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练并评估一个基于 GRU 的模型</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.GRU(<span class="number">32</span>, input_shape=(<span class="literal">None</span>, float_data.shape[<span class="number">-1</span>])))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">model.compile(optimizer=RMSprop(), loss=<span class="string">'mae'</span>)</span><br><span class="line">history = model.fit_generator(train_gen,</span><br><span class="line">                        steps_per_epoch=<span class="number">500</span>,</span><br><span class="line">                        epochs=<span class="number">20</span>,</span><br><span class="line">                        validation_data=val_gen,</span><br><span class="line">                        validation_steps=val_steps)</span><br></pre></td></tr></table></figure>
<p>效果好多了！远优于基于常识的基准方法。这证明了机器学习的价值，也证明了循环网络与序列展平的密集网络相比在这种任务上的优势。</p>
<h2 id="6-使用循环dropout来降低过拟合"><a href="#6-使用循环dropout来降低过拟合" class="headerlink" title="6. 使用循环dropout来降低过拟合"></a>6. 使用循环<code>dropout</code>来降低过拟合</h2><p>从训练和验证曲线中可以明显看出，模型出现过拟合：几轮过后，训练损失和验证损失就开始显著偏离。我们已经学过降低过拟合的一种经典技术——<code>dropout</code>，即将某一层的输入单元随机设为0，其目的是打破该层训练数据中的偶然相关性。但在循环网络中如何正确地使用<code>dropout</code>，这并不是一个简单的问题。人们早就知道，在循环层前面应用<code>dropout</code>，这种正则化会妨碍学习过程，而不是有所帮助。2015 年，在关于贝叶斯深度学习的博士论文中，Yarin Gal确定了在循环网络中使用<code>dropout</code>的正确方法：<strong>对每个时间步应该使用相同的<code>dropout</code>掩码（dropout mask，相同模式的舍弃单元），而不是让<code>dropout</code>掩码随着时间步的增加而随机变化。</strong> 此外，为了对GRU、LSTM等循环层得到的表示做正则化，应该将不随时间变化的<code>dropout</code>掩码应用于层的内部循环激活（叫作循环<code>dropout</code>掩码）。对每个时间步使用相同的<code>dropout</code>掩码，可以让网络沿着时间正确地传播其学习误差，而随时间随机变化的<code>dropout</code>掩码则会破坏这个误差信号，并且不利于学习过程。</p>
<p>Yarin Gal 使用Keras开展这项研究，并帮助将这种机制直接内置到Keras循环层中。Keras的每个循环层都有两个与<code>dropout</code>相关的参数：一个是<code>dropout</code>，它是一个浮点数，指定该层输入单元的<code>dropout</code>比率；另一个是<code>recurrent_dropout</code>，指定循环单元的<code>dropout</code>比率。我们向GRU 层中添加<code>dropout</code>和循环<code>dropout</code>，看一下这么做对过拟合的影响。因为使用<code>dropout</code>正则化的网络总是需要更长的时间才能完全收敛，所以网络训练轮次增加为原来的2倍。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练并评估一个使用dropout正则化的基于GRU的模型</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.GRU(<span class="number">32</span>,</span><br><span class="line">        dropout=<span class="number">0.2</span>,</span><br><span class="line">        recurrent_dropout=<span class="number">0.2</span>,</span><br><span class="line">        input_shape=(<span class="literal">None</span>, float_data.shape[<span class="number">-1</span>])))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">model.compile(optimizer=RMSprop(), loss=<span class="string">'mae'</span>)</span><br><span class="line">history = model.fit_generator(train_gen,</span><br><span class="line">                        steps_per_epoch=<span class="number">500</span>,</span><br><span class="line">                        epochs=<span class="number">1</span>,</span><br><span class="line">                        validation_data=val_gen,</span><br><span class="line">                        validation_steps=val_steps)</span><br></pre></td></tr></table></figure>
<p>成功！前30个轮次不再过拟合。不过，虽然评估分数更加稳定，但最佳分数并没有比之前低很多。</p>
<h2 id="7-循环层堆叠"><a href="#7-循环层堆叠" class="headerlink" title="7. 循环层堆叠"></a>7. 循环层堆叠</h2><p>模型不再过拟合，但似乎遇到了性能瓶颈，所以我们应该考虑增加网络容量。回想一下机器学习的通用工作流程：增加网络容量通常是一个好主意，直到过拟合变成主要的障碍（假设你已经采取基本步骤来降低过拟合，比如使用dropout）。只要过拟合不是太严重，那么很可能是容量不足的问题。</p>
<p>增加网络容量的通常做法是增加每层单元数或增加层数。循环层堆叠（recurrent layer stacking）是构建更加强大的循环网络的经典方法，例如，目前谷歌翻译算法就是7个大型LSTM 层的堆叠——这个架构很大。</p>
<p>在Keras中逐个堆叠循环层，所有中间层都应该返回完整的输出序列（一个3D张量），而不是只返回最后一个时间步的输出。这可以通过指定<code>return_sequences=True</code>来实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练并评估一个使用dropout正则化的堆叠GRU模型</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.GRU(<span class="number">32</span>,</span><br><span class="line">        dropout=<span class="number">0.1</span>,</span><br><span class="line">        recurrent_dropout=<span class="number">0.5</span>,</span><br><span class="line">        return_sequences=<span class="literal">True</span>,</span><br><span class="line">        input_shape=(<span class="literal">None</span>, float_data.shape[<span class="number">-1</span>])))</span><br><span class="line">model.add(layers.GRU(<span class="number">64</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">        dropout=<span class="number">0.1</span>,</span><br><span class="line">        recurrent_dropout=<span class="number">0.5</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">model.compile(optimizer=RMSprop(), loss=<span class="string">'mae'</span>)</span><br><span class="line">history = model.fit_generator(train_gen,</span><br><span class="line">                            steps_per_epoch=<span class="number">500</span>,</span><br><span class="line">                            epochs=<span class="number">1</span>,</span><br><span class="line">                            validation_data=val_gen,</span><br><span class="line">                            validation_steps=val_steps)</span><br></pre></td></tr></table></figure>
<p>可以看到，添加一层的确对结果有所改进，但并不显著。我们可以得出两个结论:</p>
<ul>
<li>因为过拟合仍然不是很严重，所以可以放心地增大每层的大小，以进一步改进验证损失。但这么做的计算成本很高。</li>
<li>添加一层后模型并没有显著改进，所以你可能发现，提高网络能力的回报在逐渐减小。</li>
</ul>
<h2 id="8-使用双向RNN"><a href="#8-使用双向RNN" class="headerlink" title="8. 使用双向RNN"></a>8. 使用双向RNN</h2><p>本节介绍的最后一种方法叫作双向RNN（bidirectional RNN）。双向RNN是一种常见的RNN变体，它在某些任务上的性能比普通RNN更好。它常用于自然语言处理，可谓深度学习对自然语言处理的瑞士军刀。</p>
<p>RNN特别依赖于顺序或时间，RNN按顺序处理输入序列的时间步，而打乱时间步或反转时间步会完全改变RNN从序列中提取的表示。正是由于这个原因，如果顺序对问题很重要（比如温度预测问题），RNN的表现会很好。双向RNN利用了RNN 的顺序敏感性：它包含两个普通RNN，比如你已经学过的GRU层和LSTM层，每个RNN分别沿一个方向对输入序列进行处理（时间正序和时间逆序），然后将它们的表示合并在一起。通过沿这两个方向处理序列，双向RNN能够捕捉到可能被单向RNN忽略的模式。</p>
<p>值得注意的是，本节的RNN 层都是按时间正序处理序列（更早的时间步在前），这可能是一个随意的决定。至少，至今我们还没有尝试质疑这个决定。如果RNN按时间逆序处理输入序列（更晚的时间步在前），能否表现得足够好呢？我们在实践中尝试一下这种方法，看一下会发生什么。你只需要编写一个数据生成器的变体，将输入序列沿着时间维度反转（即将最后一行代码替换为<code>yield samples[:, ::-1, :], targets）</code>。本节第一个实验用到了一个单GRU层的网络，我们训练一个与之相同的网络，得到的结果如图所示。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\res1.png" width="400" height="400" alt="对于耶拿温度预测任务，GRU在逆序序列上训练得到的训练损失和验证损失" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">对于耶拿温度预测任务，GRU在逆序序列上训练得到的训练损失和验证损失</div>
</center>


<p>逆序GRU的效果甚至比基于常识的基准方法还要差很多，这说明在本例中，按时间正序处理对成功解决问题很重要。这非常合理：GRU层通常更善于记住最近的数据，而不是久远的数据，与更早的数据点相比，更靠后的天气数据点对问题自然具有更高的预测能力（这也是基于常识的基准方法非常强大的原因）。因此，按时间正序的模型必然会优于时间逆序的模型。重要的是，对许多其他问题（包括自然语言）而言，情况并不是这样：直觉上来看，一个单词对理解句子的重要性通常并不取决于它在句子中的位置。我们尝试对IMDB示例中的LSTM应用相同的技巧。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用逆序序列训练并评估一个LSTM</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line">max_features = <span class="number">10000</span></span><br><span class="line">maxlen = <span class="number">500</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)</span><br><span class="line">x_train = [x[::<span class="number">-1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> x_train]</span><br><span class="line">x_test = [x[::<span class="number">-1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> x_test]</span><br><span class="line">x_train = sequence.pad_sequences(x_train, maxlen=maxlen)</span><br><span class="line">x_test = sequence.pad_sequences(x_test, maxlen=maxlen)</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.Embedding(max_features, <span class="number">128</span>))</span><br><span class="line">model.add(layers.LSTM(<span class="number">32</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'acc'</span>])</span><br><span class="line">history = model.fit(x_train, y_train,</span><br><span class="line">            epochs=<span class="number">10</span>,</span><br><span class="line">            batch_size=<span class="number">128</span>,</span><br><span class="line">            validation_split=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Train on 20000 samples, validate on 5000 samples
Epoch 1/10
20000/20000 [==============================] - 83s 4ms/step - loss: 0.4988 - acc: 0.7621 - val_loss: 0.3412 - val_acc: 0.8684
Epoch 2/10
20000/20000 [==============================] - 85s 4ms/step - loss: 0.3184 - acc: 0.8795 - val_loss: 0.3068 - val_acc: 0.8814
Epoch 3/10
20000/20000 [==============================] - 86s 4ms/step - loss: 0.2544 - acc: 0.9025 - val_loss: 0.3151 - val_acc: 0.8798
Epoch 4/10
20000/20000 [==============================] - 78s 4ms/step - loss: 0.2133 - acc: 0.9225 - val_loss: 0.3849 - val_acc: 0.8544
Epoch 5/10
20000/20000 [==============================] - 83s 4ms/step - loss: 0.1877 - acc: 0.9332 - val_loss: 0.3698 - val_acc: 0.8684
Epoch 6/10
20000/20000 [==============================] - 73s 4ms/step - loss: 0.1675 - acc: 0.9416 - val_loss: 0.3680 - val_acc: 0.8418
Epoch 7/10
20000/20000 [==============================] - 76s 4ms/step - loss: 0.1474 - acc: 0.9488 - val_loss: 0.4329 - val_acc: 0.8504
Epoch 8/10
20000/20000 [==============================] - 70s 4ms/step - loss: 0.1316 - acc: 0.9559 - val_loss: 0.4002 - val_acc: 0.8484
Epoch 9/10
20000/20000 [==============================] - 77s 4ms/step - loss: 0.1163 - acc: 0.9593 - val_loss: 0.3937 - val_acc: 0.8736
Epoch 10/10
20000/20000 [==============================] - 73s 4ms/step - loss: 0.1023 - acc: 0.9685 - val_loss: 0.4931 - val_acc: 0.8652
</code></pre><p>模型性能与正序LSTM几乎相同。值得注意的是，在这样一个文本数据集上，逆序处理的效果与正序处理一样好，这证实了一个假设：虽然单词顺序对理解语言很重要，但使用哪种顺序并不重要。重要的是，在逆序序列上训练的RNN学到的表示不同于在原始序列上学到的表示，正如在现实世界中，如果时间倒流（你的人生是第一天死去、最后一天出生），那么你的心智模型也会完全不同。在机器学习中，如果一种数据表示不同但有用，那么总是值得加以利用，这种表示与其他表示的差异越大越好，它们提供了查看数据的全新角度，抓住了数据中被其他方法忽略的内容，因此可以提高模型在某个任务上的性能。这是集成（ensembling）方法背后的直觉。</p>
<p>双向RNN正是利用这个想法来提高正序RNN的性能。它从两个方向查看数据，从而得到更加丰富的表示，并捕捉到仅使用正序RNN时可能忽略的一些模式。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\double_rnn.png" width="300" height="300" alt="双向RNN层的工作原理" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">双向RNN层的工作原理</div>
</center>

<p>在Keras中将一个双向RNN实例化，我们需要使用<code>Bidirectional</code>层，它的第一个参数是一个循环层实例。<code>Bidirectional</code>对这个循环层创建了第二个单独实例，然后使用一个实例按正序处理输入序列，另一个实例按逆序处理输入序列。我们在IMDB情感分析任务上来试一下这种方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练并评估一个双向LSTM</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.Embedding(max_features, <span class="number">32</span>))</span><br><span class="line">model.add(layers.Bidirectional(layers.LSTM(<span class="number">32</span>)))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'acc'</span>])</span><br><span class="line">history = model.fit(x_train, y_train,</span><br><span class="line">                epochs=<span class="number">10</span>,</span><br><span class="line">                batch_size=<span class="number">128</span>,</span><br><span class="line">                validation_split=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Train on 20000 samples, validate on 5000 samples
Epoch 1/10
20000/20000 [==============================] - 133s 7ms/step - loss: 0.5509 - acc: 0.7215 - val_loss: 0.3725 - val_acc: 0.8560
Epoch 2/10
20000/20000 [==============================] - 146s 7ms/step - loss: 0.3318 - acc: 0.8729 - val_loss: 0.4149 - val_acc: 0.8526
Epoch 3/10
20000/20000 [==============================] - 134s 7ms/step - loss: 0.2647 - acc: 0.9024 - val_loss: 0.8188 - val_acc: 0.7226
Epoch 4/10
20000/20000 [==============================] - 136s 7ms/step - loss: 0.2384 - acc: 0.9178 - val_loss: 0.3252 - val_acc: 0.8760
Epoch 5/10
20000/20000 [==============================] - 143s 7ms/step - loss: 0.2025 - acc: 0.9269 - val_loss: 0.4345 - val_acc: 0.8728
Epoch 6/10
20000/20000 [==============================] - 146s 7ms/step - loss: 0.1841 - acc: 0.9362 - val_loss: 0.3524 - val_acc: 0.8728
Epoch 7/10
20000/20000 [==============================] - 136s 7ms/step - loss: 0.1692 - acc: 0.9416 - val_loss: 0.4050 - val_acc: 0.8450
Epoch 8/10
20000/20000 [==============================] - 138s 7ms/step - loss: 0.1523 - acc: 0.9474 - val_loss: 0.4994 - val_acc: 0.8456
Epoch 9/10
20000/20000 [==============================] - 141s 7ms/step - loss: 0.1356 - acc: 0.9538 - val_loss: 0.4161 - val_acc: 0.8792
Epoch 10/10
20000/20000 [==============================] - 141s 7ms/step - loss: 0.1338 - acc: 0.9547 - val_loss: 0.3581 - val_acc: 0.8648
</code></pre><p>这个模型的表现比上一节的普通LSTM略好，这个模型似乎也很快就开始过拟合，这并不令人惊讶，因为双向层的参数个数是正序LSTM 的2倍。添加一些正则化，双向方法在这个任务上可能会有很好的表现。</p>
<p>接下来，我们尝试将相同的方法应用于温度预测任务。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练一个双向GRU</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.Bidirectional(layers.GRU(<span class="number">32</span>), input_shape=(<span class="literal">None</span>, float_data.shape[<span class="number">-1</span>])))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">model.compile(optimizer=RMSprop(), loss=<span class="string">'mae'</span>)</span><br><span class="line">history = model.fit_generator(train_gen,</span><br><span class="line">                            steps_per_epoch=<span class="number">500</span>,</span><br><span class="line">                            epochs=<span class="number">10</span>,</span><br><span class="line">                            validation_data=val_gen,</span><br><span class="line">                            validation_steps=val_steps)</span><br></pre></td></tr></table></figure>
<p>这个模型的表现与普通GRU层差不多一样好。其原因很容易理解：所有的预测能力肯定都来自于正序的那一半网络，因为我们已经知道，逆序的那一半在这个任务上的表现非常糟糕（本例同样是因为最近的数据比久远的数据更加重要）。</p>
<h2 id="9-更多尝试"><a href="#9-更多尝试" class="headerlink" title="9. 更多尝试"></a>9. 更多尝试</h2><p>为了提高温度预测问题的性能，你还可以尝试下面这些方法。</p>
<ul>
<li>在堆叠循环层中调节每层的单元个数，当前取值在很大程度上是任意选择的，因此可能不是最优的</li>
<li>调节RMSprop优化器的学习率</li>
<li>尝试使用LSTM层代替GRU层</li>
<li>在循环层上面尝试使用更大的密集连接回归器，即更大的Dense层或Dense层的堆叠</li>
<li>不要忘记最后在测试集上运行性能最佳的模型（即验证MAE最小的模型），否则，你开发的网络架构将会对验证集过拟合</li>
</ul>
<h1 id="四、用卷积神经网络处理序列"><a href="#四、用卷积神经网络处理序列" class="headerlink" title="四、用卷积神经网络处理序列"></a>四、用卷积神经网络处理序列</h1><p>前面我们学习了卷积神经网络（convnet），并知道它在计算机视觉问题上表现出色，原因在于它能够进行卷积运算，从局部输入图块中提取特征，并能够将表示模块化，同时可以高效地利用数据。这些性质让卷积神经网络在计算机视觉领域表现优异，同样也让它对序列处理特别有效。时间可以被看作一个空间维度，就像二维图像的高度或宽度。</p>
<p>对于某些序列处理问题，这种一维卷积神经网络的效果可以媲美RNN，而且计算代价通常要小很多。最近，一维卷积神经网络［通常与空洞卷积核（dilated kernel）一起使用］已经在音频生成和机器翻译领域取得了巨大成功。除了这些具体的成就，人们还早已知道，对于文本分类和时间序列预测等简单任务，小型的一维卷积神经网络可以替代RNN，而且速度更快。</p>
<h2 id="1-理解序列数据的一维卷积"><a href="#1-理解序列数据的一维卷积" class="headerlink" title="1. 理解序列数据的一维卷积"></a>1. 理解序列数据的一维卷积</h2><p>前面介绍的卷积层都是二维卷积，从图像张量中提取二维图块并对每个图块应用相同的变换。按照同样的方法，你也可以使用一维卷积，从序列中提取局部一维序列段（即子序列）</p>
<p>这种一维卷积层可以识别序列中的局部模式。因为对每个序列段执行相同的输入变换，所以在句子中某个位置学到的模式稍后可以在其他位置被识别，这使得一维卷积神经网络具有平移不变性（对于时间平移而言）。举个例子，使用大小为5的卷积窗口处理字符序列的一维卷积神经网络，应该能够学习长度不大于5的单词或单词片段，并且应该能够在输入句子中的任何位置识别这些单词或单词段。因此，字符级的一维卷积神经网络能够学会单词构词法。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\ODRNN.png" width="300" height="300" alt="一维卷积神经网络的工作原理：每个输出时间步都是利用输入序列
在时间维度上的一小段得到的" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">一维卷积神经网络的工作原理：每个输出时间步都是利用输入序列在时间维度上的一小段得到的</div>
</center>

<h2 id="2-序列数据的一维池化"><a href="#2-序列数据的一维池化" class="headerlink" title="2. 序列数据的一维池化"></a>2. 序列数据的一维池化</h2><p>你已经学过二维池化运算，比如二维平均池化和二维最大池化，在卷积神经网络中用于对图像张量进行空间下采样。一维也可以做相同的池化运算：从输入中提取一维序列段（即子序列），然后输出其最大值（最大池化）或平均值（平均池化）。与二维卷积神经网络一样，该运算也是用于降低一维输入的长度（子采样）。</p>
<h2 id="3-实现一维卷积神经网络"><a href="#3-实现一维卷积神经网络" class="headerlink" title="3. 实现一维卷积神经网络"></a>3. 实现一维卷积神经网络</h2><p>Keras中的一维卷积神经网络是<code>Conv1D</code>层，其接口类似于<code>Conv2D</code>。它接收的输入是形状为<code>(samples, time, features)</code>的三维张量，并返回类似形状的三维张量。卷积窗口是时间轴上的一维窗口（时间轴是输入张量的第二个轴）。</p>
<p>我们来构建一个简单的两层一维卷积神经网络，并将其应用于我们熟悉的IMDB情感分类任务。提醒一下，获取数据并预处理的代码如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line">max_features = <span class="number">10000</span></span><br><span class="line">max_len = <span class="number">500</span></span><br><span class="line">print(<span class="string">'Loading data...'</span>)</span><br><span class="line">(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)</span><br><span class="line">print(len(x_train), <span class="string">'train sequences'</span>)</span><br><span class="line">print(len(x_test), <span class="string">'test sequences'</span>)</span><br><span class="line">print(<span class="string">'Pad sequences (samples x time)'</span>)</span><br><span class="line">x_train = sequence.pad_sequences(x_train, maxlen=max_len)</span><br><span class="line">x_test = sequence.pad_sequences(x_test, maxlen=max_len)</span><br><span class="line">print(<span class="string">'x_train shape:'</span>, x_train.shape)</span><br><span class="line">print(<span class="string">'x_test shape:'</span>, x_test.shape)</span><br></pre></td></tr></table></figure>
<p>一维卷积神经网络的架构与二维卷积神经网络相同，它是<code>Conv1D</code>层和<code>MaxPooling1D</code>层的堆叠，最后是一个全局池化层或<code>Flatten</code>层，将三维输出转换为二维输出，让你可以向模型中添加一个或多个<code>Dense</code>层，用于分类或回归。不过二者有一点不同：一维卷积神经网络可以使用更大的卷积窗口。对于二维卷积层，$3×3$的卷积窗口包含$3×3=9$个特征向量；但对于一位卷积层，大小为3的卷积窗口只包含3个卷积向量。因此，你可以轻松使用大小等于7或9 的一维卷积窗口。</p>
<p>用于IMDB数据集的一维卷积神经网络示例如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在IMDB数据上训练并评估一个简单的一维卷积神经网络</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.Embedding(max_features, <span class="number">128</span>, input_length=max_len))</span><br><span class="line">model.add(layers.Conv1D(<span class="number">32</span>, <span class="number">7</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling1D(<span class="number">5</span>))</span><br><span class="line">model.add(layers.Conv1D(<span class="number">32</span>, <span class="number">7</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.GlobalMaxPooling1D())</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">model.summary()</span><br><span class="line">model.compile(optimizer=RMSprop(lr=<span class="number">1e-4</span>),</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'acc'</span>])</span><br><span class="line">history = model.fit(x_train, y_train,</span><br><span class="line">                epochs=<span class="number">10</span>,</span><br><span class="line">                batch_size=<span class="number">128</span>,</span><br><span class="line">                validation_split=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<p>验证精度略低于LSTM，但在CPU和GPU上的运行速度都要更快（速度提高多少取决于具体配置，会有很大差异）。现在，你可以使用正确的轮数（4 轮）重新训练这个模型，然后在测试集上运行。这个结果可以让我们确信，在单词级的情感分类任务上，一维卷积神经网络可以替代循环网络，并且速度更快、计算代价更低。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\train_1.png" width="300" height="300" alt="简单的一维卷积神经网络在IMDB数据上的训练精度和验证精度" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">简单的一维卷积神经网络在IMDB数据上的训练损失和验证损失</div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\train_2.png" width="300" height="300" alt="简单的一维卷积神经网络在IMDB数据上的训练精度和验证精度" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">简单的一维卷积神经网络在IMDB数据上的训练损失和验证损失</div>
</center>

<h2 id="4-结合CNN和RNN来处理长序列"><a href="#4-结合CNN和RNN来处理长序列" class="headerlink" title="4. 结合CNN和RNN来处理长序列"></a>4. 结合CNN和RNN来处理长序列</h2><p>一维卷积神经网络分别处理每个输入序列段，所以它对时间步的顺序不敏感（这里所说顺序的范围要大于局部尺度，即大于卷积窗口的大小），这一点与RNN不同。当然，为了识别更长期的模式，你可以将许多卷积层和池化层堆叠在一起，这样上面的层能够观察到原始输入中更长的序列段，但这仍然不是一种引入顺序敏感性的好方法。想要证明这种方法的不足，一种方法是在温度预测问题上使用一维卷积神经网络，在这个问题中顺序敏感性对良好的预测结果非常关键。以下示例复用了前面定义的这些变量：float_data、train_gen、val_gen 和val_steps。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.Conv1D(<span class="number">32</span>, <span class="number">5</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">input_shape=(<span class="literal">None</span>, float_data.shape[<span class="number">-1</span>])))</span><br><span class="line">model.add(layers.MaxPooling1D(<span class="number">3</span>))</span><br><span class="line">model.add(layers.Conv1D(<span class="number">32</span>, <span class="number">5</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling1D(<span class="number">3</span>))</span><br><span class="line">model.add(layers.Conv1D(<span class="number">32</span>, <span class="number">5</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.GlobalMaxPooling1D())</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">model.compile(optimizer=RMSprop(), loss=<span class="string">'mae'</span>)</span><br><span class="line">history = model.fit_generator(train_gen,</span><br><span class="line">                        steps_per_epoch=<span class="number">500</span>,</span><br><span class="line">                        epochs=<span class="number">10</span>,</span><br><span class="line">                        validation_data=val_gen,</span><br><span class="line">                        validation_steps=val_steps)</span><br></pre></td></tr></table></figure>
<p>下图给出了训练和验证的MAE</p>
<center>
    <img src="\Pic\DeepLearning_Pic\train_3.png" width="300" height="300" alt="简单的一维卷积神经网络在温度预测任务上的训练损失和验证损失" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">简单的一维卷积神经网络在温度预测任务上的训练损失和验证损失</div>
</center>

<p>验证MAE 停留在<code>0.4~0.5</code>，使用小型卷积神经网络甚至无法击败基于常识的基准方法。同样，这是因为卷积神经网络在输入时间序列的所有位置寻找模式，它并不知道所看到某个模式的时间位置（距开始多长时间，距结束多长时间等）。对于这个具体的预测问题，对最新数据点的解释与对较早数据点的解释应该并不相同，所以卷积神经网络无法得到有意义的结果。卷积神经网络的这种限制对于IMDB 数据来说并不是问题，因为对于与正面情绪或负面情绪相关联的关键词模式，无论出现在输入句子中的什么位置，它所包含的信息量是一样的。</p>
<p>要想结合卷积神经网络的速度和轻量与RNN 的顺序敏感性，一种方法是在RNN前面使用一维卷积神经网络作为预处理步骤。对于那些非常长，以至于RNN 无法处理的序列（比如包含上千个时间步的序列），这种方法尤其有用。卷积神经网络可以将长的输入序列转换为高级特征组成的更短序列（下采样）。然后，提取的特征组成的这些序列成为网络中RNN的输入。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\long_term.png" width="200" height="200" alt="结合一维CNN和RNN来处理长序列" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">结合一维CNN和RNN来处理长序列</div>
</center>

<p>这种方法在研究论文和实际应用中并不多见，可能是因为很多人并不知道。这种方法非常有效，应该被更多人使用。我们尝试将其应用于温度预测数据集。因为这种方法允许操作更长的序列，所以我们可以查看更早的数据（通过增大数据生成器的<code>lookback</code>参数）或查看分辨率更高的时间序列（通过减小生成器的<code>step</code>参数）。这里我们任意地将<code>step</code>减半，得到时间序列的长度变为之前的两倍，温度数据的采样频率变为每30分钟一个数据点。本示例复用了之前定义的<code>generator</code>函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为耶拿数据集准备更高分辨率的数据生成器</span></span><br><span class="line">step = <span class="number">3</span></span><br><span class="line">lookback = <span class="number">720</span></span><br><span class="line">delay = <span class="number">144</span></span><br><span class="line">train_gen = generator(float_data,</span><br><span class="line">                    lookback=lookback,</span><br><span class="line">                    delay=delay,</span><br><span class="line">                    min_index=<span class="number">0</span>,</span><br><span class="line">                    max_index=<span class="number">200000</span>,</span><br><span class="line">                    shuffle=<span class="literal">True</span>,</span><br><span class="line">                    step=step)</span><br><span class="line">val_gen = generator(float_data,</span><br><span class="line">                    lookback=lookback,</span><br><span class="line">                    delay=delay,</span><br><span class="line">                    min_index=<span class="number">200001</span>,</span><br><span class="line">                    max_index=<span class="number">300000</span>,</span><br><span class="line">                    step=step)</span><br><span class="line">test_gen = generator(float_data,</span><br><span class="line">                    lookback=lookback,</span><br><span class="line">                    delay=delay,</span><br><span class="line">                    min_index=<span class="number">300001</span>,</span><br><span class="line">                    max_index=<span class="literal">None</span>,</span><br><span class="line">                    step=step)</span><br><span class="line">val_steps = (<span class="number">300000</span> - <span class="number">200001</span> - lookback) // <span class="number">128</span></span><br><span class="line">test_steps = (len(float_data) - <span class="number">300001</span> - lookback) // <span class="number">128</span></span><br></pre></td></tr></table></figure>
<p>下面是模型，开始是两个<code>Conv1D</code>层，然后是一个<code>GRU</code>层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 结合一维卷积基和GRU层的模型</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.Conv1D(<span class="number">32</span>, <span class="number">5</span>, activation=<span class="string">'relu'</span>,input_shape=(<span class="literal">None</span>, float_data.shape[<span class="number">-1</span>])))</span><br><span class="line">model.add(layers.MaxPooling1D(<span class="number">3</span>))</span><br><span class="line">model.add(layers.Conv1D(<span class="number">32</span>, <span class="number">5</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.GRU(<span class="number">32</span>, dropout=<span class="number">0.1</span>, recurrent_dropout=<span class="number">0.5</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">model.summary()</span><br><span class="line">model.compile(optimizer=RMSprop(), loss=<span class="string">'mae'</span>)</span><br><span class="line">history = model.fit_generator(train_gen,</span><br><span class="line">                            steps_per_epoch=<span class="number">500</span>,</span><br><span class="line">                            epochs=<span class="number">20</span>,</span><br><span class="line">                            validation_data=val_gen,</span><br><span class="line">                            validation_steps=val_steps)</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\train_4.png" width="300" height="300" alt="一维卷积神经网络+GRU在耶拿温度预测任务上的训练损失和验证损失" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">一维卷积神经网络+GRU在耶拿温度预测任务上的训练损失和验证损失</div>
</center>

<p>从验证损失来看，这种架构的效果不如只用正则化GRU，但速度要快很多。它查看了两倍的数据量，在本例中可能不是非常有用，但对于其他数据集可能非常重要。</p>
<h2 id="5-小结"><a href="#5-小结" class="headerlink" title="5. 小结"></a>5. 小结</h2><p>下面是你应该从本节中学到的要点。</p>
<ul>
<li>二维卷积神经网络在二维空间中处理视觉模式时表现很好，与此相同，一维卷积神经网络在处理时间模式时表现也很好。对于某些问题，特别是自然语言处理任务，它可以替代RNN，并且速度更快。</li>
<li>通常情况下，一维卷积神经网络的架构与计算机视觉领域的二维卷积神经网络很相似，它将Conv1D层和MaxPooling1D层堆叠在一起，最后是一个全局池化运算或展平操作。</li>
<li>因为RNN在处理非常长的序列时计算代价很大，但一维卷积神经网络的计算代价很小，所以在RNN之前使用一维卷积神经网络作为预处理步骤是一个好主意，这样可以使序列变短，并提取出有用的表示交给RNN来处理。</li>
</ul>
<h1 id="五、本文总结"><a href="#五、本文总结" class="headerlink" title="五、本文总结"></a>五、本文总结</h1><ul>
<li>你在本文学到了以下技术，它们广泛应用于序列数据（从文本到时间序列）组成的数据集。<ul>
<li>如何对文本分词</li>
<li>什么是词嵌入，如何使用词嵌入。</li>
<li>什么是循环网络，如何使用循环网络。</li>
<li>如何堆叠 RNN层和使用双向RNN，以构建更加强大的序列处理模型。</li>
<li>如何使用一维卷积神经网络来处理序列。</li>
<li>如何结合一维卷积神经网络和 RNN来处理长序列。</li>
</ul>
</li>
<li>你可以用 RNN 进行时间序列回归（“预测未来”）、时间序列分类、时间序列异常检测和序列标记（比如找出句子中的人名或日期）。</li>
<li>同样，你可以将一维卷积神经网络用于机器翻译（序列到序列的卷积模型，比如SliceNet）、文档分类和拼写校正。</li>
<li>如果序列数据的整体顺序很重要，那么最好使用循环网络来处理。时间序列通常都是这样，最近的数据可能比久远的数据包含更多的信息量。</li>
<li>如果整体顺序没有意义，那么一维卷积神经网络可以实现同样好的效果，而且计算代价更小。文本数据通常都是这样，在句首发现关键词和在句尾发现关键词一样都很有意义。</li>
</ul>
]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
  </entry>
  <entry>
    <title>Python深度学习（二）深度学习用于计算机视觉</title>
    <url>/posts/1fc7d624.html</url>
    <content><![CDATA[<p>本节将介绍卷积神经网络，也叫<code>convnet</code>，它是计算机视觉应用几乎都在使用的一种深度学习模型。你将学到将卷积神经网络应用于图像分类问题，特别是那些训练数据集较小的问题。如果你工作的地方并非大型科技公司，这也将是你最常见的使用场景。</p>
<h1 id="一、卷积神经网络简介"><a href="#一、卷积神经网络简介" class="headerlink" title="一、卷积神经网络简介"></a>一、卷积神经网络简介</h1><p>我们将深入讲解卷积神经网络的原理，以及它在计算机视觉任务上为什么如此成功。但在此之前，我们先来看一个简单的卷积神经网络示例，即使用卷积神经网络对MNIST 数字进行分类，这个任务我们在第2 章用密集连接网络做过（当时的测试精度为97.8%）。虽然本例中的卷积神经网络很简单，但其精度肯定会超过前面的密集连接网络。</p>
<p>下列代码将会展示一个简单的卷积神经网络。它是Conv2D 层和MaxPooling2D 层的堆叠，很快你就会知道这些层的作用。</p>
<h2 id="1-实例化一个小型的卷积神经网络"><a href="#1-实例化一个小型的卷积神经网络" class="headerlink" title="1. 实例化一个小型的卷积神经网络"></a>1. 实例化一个小型的卷积神经网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Using TensorFlow backend.
</code></pre><p>重要的是，卷积神经网络接收形状为<code>(image_height, image_width, image_channels)</code>的输入张量（不包括批量维度）。本例中设置卷积神经网络处理大小为(28, 28, 1) 的输入张量，这正是MNIST 图像的格式。我们向第一层传入参数<code>input_shape=(28, 28, 1)</code> 来完成此设置。</p>
<p>我们来看一下目前卷积神经网络的架构。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     
=================================================================
Total params: 55,744
Trainable params: 55,744
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>可以看到，每个Conv2D 层和MaxPooling2D 层的输出都是一个形状为(height, width,channels) 的3D 张量。宽度和高度两个维度的尺寸通常会随着网络加深而变小，通道数量由传<br>入Conv2D 层的第一个参数所控制（32 或64）。</p>
<p>下一步是将最后的输出张量［大小为(3, 3, 64)］输入到一个密集连接分类器网络中，即Dense 层的堆叠，你已经很熟悉了。这些分类器可以处理1D 向量，而当前的输出是3D 张量。首先，我们需要将3D 输出展平为1D，然后在上面添加几个Dense 层。</p>
<h2 id="2-在卷积神经网络上添加分类器"><a href="#2-在卷积神经网络上添加分类器" class="headerlink" title="2. 在卷积神经网络上添加分类器"></a>2. 在卷积神经网络上添加分类器</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br></pre></td></tr></table></figure>
<p>我们将进行10 类别分类，最后一层使用带10 个输出的softmax 激活。现在网络的架构如下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     
_________________________________________________________________
flatten_1 (Flatten)          (None, 576)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                36928     
_________________________________________________________________
dense_2 (Dense)              (None, 10)                650       
=================================================================
Total params: 93,322
Trainable params: 93,322
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>如你所见，在进入两个Dense层之前，形状(3, 3, 64) 的输出被展平为形状(576,) 的向量。</p>
<p>下面我们在MNIST数字图像上训练这个卷积神经网络。我们将复用MNIST示例中的很多代码。</p>
<h2 id="3-在MINST图像上训练卷积神经网络"><a href="#3-在MINST图像上训练卷积神经网络" class="headerlink" title="3. 在MINST图像上训练卷积神经网络"></a>3. 在MINST图像上训练卷积神经网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br><span class="line">train_images = train_images.reshape((<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line">train_images = train_images.astype(<span class="string">'float32'</span>) / <span class="number">255</span></span><br><span class="line">test_images = test_images.reshape((<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line">test_images = test_images.astype(<span class="string">'float32'</span>) / <span class="number">255</span></span><br><span class="line">train_labels = to_categorical(train_labels)</span><br><span class="line">test_labels = to_categorical(test_labels)</span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">            loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(train_images, train_labels, epochs=<span class="number">5</span>, batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/5
60000/60000 [==============================] - 25s 419us/step - loss: 0.1652 - accuracy: 0.9488
Epoch 2/5
60000/60000 [==============================] - 26s 429us/step - loss: 0.0458 - accuracy: 0.9864
Epoch 3/5
60000/60000 [==============================] - 24s 400us/step - loss: 0.0320 - accuracy: 0.9897
Epoch 4/5
60000/60000 [==============================] - 24s 396us/step - loss: 0.0247 - accuracy: 0.9924
Epoch 5/5
60000/60000 [==============================] - 24s 393us/step - loss: 0.0196 - accuracy: 0.9940
</code></pre><p>我们在测试数据上对模型进行评估。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_loss, test_acc = model.evaluate(test_images, test_labels)</span><br><span class="line">test_acc</span><br></pre></td></tr></table></figure>
<pre><code>10000/10000 [==============================] - 1s 112us/step


0.9923999905586243
</code></pre><p>密集连接网络的测试精度为97.8%，但这个简单卷积神经网络的测试精度达到了99.1%，我们将错误率降低了68%（相对比例）。相当不错！与密集连接模型相比，为什么这个简单卷积神经网络的效果这么好？要回答这个问题，我们来深入了解Conv2D 层和MaxPooling2D 层的作用。</p>
<h2 id="4-卷积神经网络"><a href="#4-卷积神经网络" class="headerlink" title="4. 卷积神经网络"></a>4. 卷积神经网络</h2><p>密集连接层和卷积层的根本区别在于，Dense 层从输入特征空间中学到的是全局模式（比如对于MNIST 数字，全局模式就是涉及所有像素的模式），而卷积层学到的是局部模式。对于图像来说，学到的就是在输入图像的二维小窗口中发现的模式。在上面的例子中，这些窗口的大小都是3×3。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\conv_1.png" width="300" height="300" alt="图像可以被分解为局部模式，如边缘、纹理等" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图像可以被分解为局部模式，如边缘、纹理等</div>
</center>

<p>这个重要特性使卷积神经网络具有以下两个有趣的性质。</p>
<ul>
<li>卷积神经网络学到的模式具有平移不变性（translation invariant）。卷积神经网络在图像右下角学到某个模式之后，它可以在任何地方识别这个模式，比如左上角。对于密集连接网络来说，如果模式出现在新的位置，它只能重新学习这个模式。这使得卷积神经网络在处理图像时可以高效利用数据（因为视觉世界从根本上具有平移不变性），它只需要更少的训练样本就可以学到具有泛化能力的数据表示。</li>
<li>卷积神经网络可以学到模式的空间层次结构（spatial hierarchies of patterns）。第一个卷积层将学习较小的局部模式（比如边缘），第二个卷积层将学习由第一层特征组成的更大的模式，以此类推。这使得卷积神经网络可以有效地学习越来越复杂、越来越抽象的视觉概念（因为视觉世界从根本上具有空间层次结构）。</li>
</ul>
<p>对于包含两个空间轴（高度和宽度）和一个深度轴（也叫通道轴）的3D 张量，其卷积也叫特征图（feature map）。对于RGB 图像，深度轴的维度大小等于3，因为图像有3 个颜色通道：红色、绿色和蓝色。对于黑白图像（比如MNIST 数字图像），深度等于1（表示灰度等级）。卷积运算从输入特征图中提取图块，并对所有这些图块应用相同的变换，生成输出特征图（output feature map）。该输出特征图仍是一个3D 张量，具有宽度和高度，其深度可以任意取值，因为输出深度是层的参数，深度轴的不同通道不再像RGB 输入那样代表特定颜色，而是代表过滤器（filter）。过滤器对输入数据的某一方面进行编码，比如，单个过滤器可以从更高层次编码这样一个概念：“输入中包含一张脸。”</p>
<center>
    <img src="\Pic\DeepLearning_Pic\cat.png" width="300" height="300" alt="cat" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">视觉世界形成了视觉模块的空间层次结构：超局部的边缘组合成局部的对象，比如眼睛或耳朵，这些局部对象又组合成高级概念，比如“猫”</div>
</center>

<p>在MNIST示例中，第一个卷积层接收一个大小为<code>(28, 28, 1)</code>的特征图，并输出一个大小为<code>(26, 26, 32)</code>的特征图，即它在输入上计算32个过滤器。对于这32个输出通道，每个通道都包含一个26×26的数值网格，它是过滤器对输入的响应图（response map），表示这个过滤器模式在输入中不同位置的响应。这也是特征图这一术语的含义：深度轴的每个维度都是一个特征（或过滤器），而2D 张量<code>output[:, :, n]</code>是这个过滤器在输入上的响应的二维空间图（map）。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\响应图.png" width="300" height="300" alt="响应图" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">响应图的概念：某个模式在输入中的不同位置是否存在的二维图</div>
</center>

<p>卷积由以下两个关键参数所定义：</p>
<ul>
<li>从输入中提取的图块尺寸：这些图块的大小通常是 3×3 或 5×5。本例中为 3×3，这是很常见的选择。</li>
<li>输出特征图的深度：卷积所计算的过滤器的数量。本例第一层的深度为32，最后一层的深度是64。</li>
</ul>
<p>对于Keras 的Conv2D 层，这些参数都是向层传入的前几个参数：<code>Conv2D(output_depth,(window_height, window_width))</code>。</p>
<p>卷积的工作原理：在3D 输入特征图上滑动（slide）这些3×3 或5×5 的窗口，在每个可能的位置停止并提取周围特征的3D图块［形状为<code>(window_height, window_width, input_depth)</code>］。然后每个3D 图块与学到的同一个权重矩阵［叫作卷积核（convolution kernel）］做张量积，转换成形状为<code>(output_depth,)</code> 的1D 向量。然后对所有这些向量进行空间重组，使其转换为形状为<code>(height, width, output_depth)</code>的3D 输出特征图。输出特征图中的每个空间位置都对应于输入特征图中的相同位置（比如输出的右下角包含了输入右下角的信息）。举个例子，利用3×3的窗口，向量<code>output[i, j, :]</code>来自3D 图块<code>input[i-1:i+1,j-1:j+1, :]</code>。整个过程详见下图：</p>
<center>
    <img src="\Pic\DeepLearning_Pic\卷积的工作原理.png" width="300" height="300" alt="卷积的工作原理" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">卷积的工作原理</div>
</center>

<p>注意，输出的宽度和高度可能与输入的宽度和高度不同，不同的原因可能有两点。</p>
<ul>
<li>边界效应，可以通过对输入特征图进行填充来抵消。</li>
<li>使用了步幅（stride），稍后会给出其定义。</li>
</ul>
<h2 id="5-最大池化运算"><a href="#5-最大池化运算" class="headerlink" title="5. 最大池化运算"></a>5. 最大池化运算</h2><p>在卷积神经网络示例中，你可能注意到，在每个MaxPooling2D层之后，特征图的尺寸都会减半。例如，在第一个MaxPooling2D层之前，特征图的尺寸是26×26，但最大池化运算将其减半为13×13。这就是最大池化的作用：<strong>对特征图进行下采样，与步进卷积类似。最大池化是从输入特征图中提取窗口，并输出每个通道的最大值。</strong>它的概念与卷积类似，但是最大池化使用硬编码的<code>max</code>张量运算对局部图块进行变换，而不是使用学到的线性变换（卷积核）。最大池化与卷积的最大不同之处在于，最大池化通常使用2×2的窗口和步幅2，其目的是将特征图下采样2倍。与此相对的是，卷积通常使用3×3 窗口和步幅1。为什么要用这种方式对特征图下采样？为什么不删除最大池化层，一直保留较大的特征图？我们来这么做试一下。这时模型的卷积基（convolutional base）如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_no_max_pool = models.Sequential()</span><br><span class="line">model_no_max_pool.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)))</span><br><span class="line">model_no_max_pool.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model_no_max_pool.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model_no_max_pool.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_2&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 24, 24, 64)        18496     
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 22, 22, 64)        36928     
=================================================================
Total params: 55,744
Trainable params: 55,744
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>这种架构有什么问题？有如下两点问题：</p>
<ul>
<li>这种架构不利于学习特征的空间层级结构。第三层的 3×3 窗口中只包含初始输入的 7×7 窗口中所包含的信息。卷积神经网络学到的高级模式相对于初始输入来说仍然很小，这可能不足以学会对数字进行分类（你可以试试仅通过7 像素×7 像素的窗口观察图像来识别其中的数字）。我们需要让最后一个卷积层的特征包含输入的整体信息。</li>
<li>最后一层的特征图对每个样本共有 22×22×64=30 976 个元素。这太多了。如果你将其展平并在上面添加一个大小为512 的Dense 层，那一层将会有1580 万个参数。这对于这样一个小模型来说太多了，会导致严重的过拟合。</li>
</ul>
<p>简而言之，使用下采样的原因，一是减少需要处理的特征图的元素个数，二是通过让连续卷积层的观察窗口越来越大（即窗口覆盖原始输入的比例越来越大），从而引入空间过滤器的层级结构。</p>
<p>注意，最大池化不是实现这种下采样的唯一方法。你已经知道，还可以在前一个卷积层中使用步幅来实现。此外，你还可以使用平均池化来代替最大池化，其方法是将每个局部输入图块变换为取该图块各通道的平均值，而不是最大值。但最大池化的效果往往比这些替代方法更好。</p>
<p>简而言之，原因在于特征中往往编码了某种模式或概念在特征图的不同位置是否存在（因此得名特征图），而观察不同特征的最大值而不是平均值能够给出更多的信息。因此，最合理的子采样策略是<strong>首先生成密集的特征图（通过无步进的卷积），然后观察特征每个小图块上的最大激活</strong>，而不是查看输入的稀疏窗口（通过步进卷积）或对输入图块取平均，因为后两种方法可能导致错过或淡化特征是否存在的信息。</p>
<p>现在你应该已经理解了卷积神经网络的基本概念，即特征图、卷积和最大池化，并且也知道如何构建一个小型卷积神经网络来解决简单问题，比如MNIST 数字分类。下面我们将介绍更加实用的应用。</p>
<h1 id="二、在小型数据集上从头开始训练一个卷积神经网络"><a href="#二、在小型数据集上从头开始训练一个卷积神经网络" class="headerlink" title="二、在小型数据集上从头开始训练一个卷积神经网络"></a>二、在小型数据集上从头开始训练一个卷积神经网络</h1><p>使用很少的数据来训练一个图像分类模型，这是很常见的情况，如果你要从事计算机视觉方面的职业，很可能会在实践中遇到这种情况。“很少的”样本可能是几百张图像，也可能是几万张图像。来看一个实例，我们将重点讨论猫狗图像分类，数据集中包含4000 张猫和狗的图像（2000 张猫的图像，2000 张狗的图像）。我们将2000 张图像用于训练，1000 张用于验证，1000张用于测试。</p>
<p>本节将介绍解决这一问题的基本策略，即使用已有的少量数据从头开始训练一个新模型。首先，在2000 个训练样本上训练一个简单的小型卷积神经网络，不做任何正则化，为模型目标设定一个基准。这会得到71% 的分类精度。此时主要的问题在于过拟合。然后，我们会介绍数据增强（data augmentation），它在计算机视觉领域是一种非常强大的降低过拟合的技术。使用数据增强之后，网络精度将提高到82%。随后我们会介绍将深度学习应用于小型数据集的另外两个重要技巧：用预训练的网络做特征提取（得到的精度范围在90%~96%），对预训练的网络进行微调（最终精度为97%）。总而言之，这三种策略——从头开始训练一个小型模型、使用预训练的网络做特征提取、对预训练的网络进行微调——构成了你的工具箱，未来可用于解决小型数据集的图像分类问题。</p>
<h2 id="1-深度学习与小数据问题的相关性"><a href="#1-深度学习与小数据问题的相关性" class="headerlink" title="1. 深度学习与小数据问题的相关性"></a>1. 深度学习与小数据问题的相关性</h2><p>有时你会听人说，仅在有大量数据可用时，深度学习才有效。这种说法部分正确：深度学习的一个基本特性就是能够独立地在训练数据中找到有趣的特征，无须人为的特征工程，而这只在拥有大量训练样本时才能实现。对于输入样本的维度非常高（比如图像）的问题尤其如此。</p>
<p>但对于初学者来说，所谓“大量”样本是相对的，即相对于你所要训练网络的大小和深度而言。只用几十个样本训练卷积神经网络就解决一个复杂问题是不可能的，但如果模型很小，并做了很好的正则化，同时任务非常简单，那么几百个样本可能就足够了。由于卷积神经网络学到的是局部的、平移不变的特征，它对于感知问题可以高效地利用数据。虽然数据相对较少，但在非常小的图像数据集上从头开始训练一个卷积神经网络，仍然可以得到不错的结果，而且无须任何自定义的特征工程。</p>
<p>此外，深度学习模型本质上具有高度的可复用性，比如，已有一个在大规模数据集上训练的图像分类模型或语音转文本模型，你只需做很小的修改就能将其复用于完全不同的问题。特别是在计算机视觉领域，许多预训练的模型（通常都是在ImageNet 数据集上训练得到的）现在都可以公开下载，并可以用于在数据很少的情况下构建强大的视觉模型。我们先来看一下数据。</p>
<h2 id="2-下载数据"><a href="#2-下载数据" class="headerlink" title="2. 下载数据"></a>2. 下载数据</h2><p>本节用到的猫狗分类数据集不包含在Keras 中。它由Kaggle 在2013 年末公开并作为一项计算视觉竞赛的一部分，当时卷积神经网络还不是主流算法。你可以从<a href="https://www.kaggle.com/c/dogs-vs-cats/data" target="_blank" rel="noopener">https://www.kaggle.com/c/dogs-vs-cats/data</a> 下载原始数据集。</p>
<p>这些图像都是中等分辨率的彩色JPEG 图像:</p>
<center>
    <img src="\Pic\DeepLearning_Pic\cat_dog_1.png" width="300" height="300" alt="猫狗分类数据集的一些样本。没有修改尺寸：样本在尺寸、外观等方面是不一样的" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">猫狗分类数据集的一些样本。没有修改尺寸：样本在尺寸、外观等方面是不一样的</div>
</center>

<p>不出所料，2013 年的猫狗分类Kaggle 竞赛的优胜者使用的是卷积神经网络。最佳结果达到了95% 的精度。本例中，虽然你只在不到参赛选手所用的10% 的数据上训练模型，但结果也和这个精度相当接近。</p>
<p>这个数据集包含25 000 张猫狗图像（每个类别都有12 500 张），大小为543MB（压缩后）。下载数据并解压之后，你需要创建一个新数据集，其中包含三个子集：每个类别各1000 个样本的训练集、每个类别各500 个样本的验证集和每个类别各500 个样本的测试集。创建新数据集的代码如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将图像复制到训练、验证和测试的目录</span></span><br><span class="line"><span class="keyword">import</span> os, shutil</span><br><span class="line">original_dataset_dir = <span class="string">'data/cat_dog/kaggle_original_data'</span></span><br><span class="line">base_dir = <span class="string">'data/cat_dog/cats_and_dogs_small'</span></span><br><span class="line">os.mkdir(base_dir)</span><br><span class="line">train_dir = os.path.join(base_dir, <span class="string">'train'</span>)</span><br><span class="line">os.mkdir(train_dir)</span><br><span class="line">validation_dir = os.path.join(base_dir, <span class="string">'validation'</span>)</span><br><span class="line">os.mkdir(validation_dir)</span><br><span class="line">test_dir = os.path.join(base_dir, <span class="string">'test'</span>)</span><br><span class="line">os.mkdir(test_dir)</span><br><span class="line">train_cats_dir = os.path.join(train_dir, <span class="string">'cats'</span>)</span><br><span class="line">os.mkdir(train_cats_dir)</span><br><span class="line">train_dogs_dir = os.path.join(train_dir, <span class="string">'dogs'</span>)</span><br><span class="line">os.mkdir(train_dogs_dir)</span><br><span class="line">validation_cats_dir = os.path.join(validation_dir, <span class="string">'cats'</span>)</span><br><span class="line">os.mkdir(validation_cats_dir)</span><br><span class="line">validation_dogs_dir = os.path.join(validation_dir, <span class="string">'dogs'</span>)</span><br><span class="line">os.mkdir(validation_dogs_dir)</span><br><span class="line">test_cats_dir = os.path.join(test_dir, <span class="string">'cats'</span>)</span><br><span class="line">os.mkdir(test_cats_dir)</span><br><span class="line">test_dogs_dir = os.path.join(test_dir, <span class="string">'dogs'</span>)</span><br><span class="line">os.mkdir(test_dogs_dir)</span><br><span class="line">fnames = [<span class="string">'cat.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">dst = os.path.join(train_cats_dir, fname)</span><br><span class="line">shutil.copyfile(src, dst)</span><br><span class="line">fnames = [<span class="string">'cat.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>, <span class="number">1500</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">dst = os.path.join(validation_cats_dir, fname)</span><br><span class="line">shutil.copyfile(src, dst)</span><br><span class="line">fnames = [<span class="string">'cat.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1500</span>, <span class="number">2000</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">dst = os.path.join(test_cats_dir, fname)</span><br><span class="line">shutil.copyfile(src, dst)</span><br><span class="line">fnames = [<span class="string">'dog.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">dst = os.path.join(train_dogs_dir, fname)</span><br><span class="line">shutil.copyfile(src, dst)</span><br><span class="line">fnames = [<span class="string">'dog.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>, <span class="number">1500</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">dst = os.path.join(validation_dogs_dir, fname)</span><br><span class="line">shutil.copyfile(src, dst)</span><br><span class="line">fnames = [<span class="string">'dog.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1500</span>, <span class="number">2000</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">dst = os.path.join(test_dogs_dir, fname)</span><br><span class="line">shutil.copyfile(src, dst)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os, shutil</span><br><span class="line">base_dir = <span class="string">'data/cat_dog/cats_and_dogs_small'</span></span><br><span class="line">train_dir = os.path.join(base_dir, <span class="string">'train'</span>)</span><br><span class="line">test_dir = os.path.join(base_dir, <span class="string">'test'</span>)</span><br><span class="line">validation_dir = os.path.join(base_dir, <span class="string">'validation'</span>)</span><br><span class="line">train_cats_dir = os.path.join(train_dir, <span class="string">'cats'</span>)</span><br><span class="line">test_cats_dir = os.path.join(test_dir, <span class="string">'cats'</span>)</span><br><span class="line">validation_cats_dir = os.path.join(validation_dir, <span class="string">'cats'</span>)</span><br><span class="line">train_dogs_dir = os.path.join(train_dir, <span class="string">'dogs'</span>)</span><br><span class="line">test_dogs_dir = os.path.join(test_dir, <span class="string">'dogs'</span>)</span><br><span class="line">validation_dogs_dir = os.path.join(validation_dir, <span class="string">'dogs'</span>)</span><br><span class="line">print(<span class="string">'total training cat images:'</span>, len(os.listdir(train_cats_dir)))</span><br><span class="line">print(<span class="string">'total training dog images:'</span>, len(os.listdir(train_dogs_dir)))</span><br><span class="line">print(<span class="string">'total validation cat images:'</span>, len(os.listdir(validation_cats_dir)))</span><br><span class="line">print(<span class="string">'total validation dog images:'</span>, len(os.listdir(validation_dogs_dir)))</span><br><span class="line">print(<span class="string">'total test cat images:'</span>, len(os.listdir(test_cats_dir)))</span><br><span class="line">print(<span class="string">'total test dog images:'</span>, len(os.listdir(test_dogs_dir)))</span><br></pre></td></tr></table></figure>
<pre><code>total training cat images: 1000
total training dog images: 1000
total validation cat images: 500
total validation dog images: 500
total test cat images: 500
total test dog images: 500
</code></pre><p>所以我们的确有2000 张训练图像、1000 张验证图像和1000 张测试图像。每个分组中两个类别的样本数相同，这是一个平衡的二分类问题，分类精度可作为衡量成功的指标。</p>
<h2 id="3-构建网络"><a href="#3-构建网络" class="headerlink" title="3. 构建网络"></a>3. 构建网络</h2><p>在前一个MNIST示例中，我们构建了一个小型卷积神经网络，所以你应该已经熟悉这种网络。我们将复用相同的总体结构，即卷积神经网络由<code>Conv2D</code>层（使用<code>relu</code>激活）和<code>MaxPooling2D</code>层交替堆叠构成。</p>
<p>但由于这里要处理的是更大的图像和更复杂的问题，你需要相应地增大网络，即再增加一个<code>Conv2D+MaxPooling2D</code>的组合。这既可以增大网络容量，也可以进一步减小特征图的尺寸，使其在连接<code>Flatten</code>层时尺寸不会太大。本例中初始输入的尺寸为150×150（有些随意的选择），所以最后在<code>Flatten</code>层之前的特征图大小为7×7。</p>
<p>你面对的是一个二分类问题，所以网络最后一层是使用<code>sigmoid</code>激活的单一单元（大小为 1 的<code>Dense</code>层）。这个单元将对某个类别的概率进行编码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将猫狗分类的小型卷积神经网络实例化</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">"relu"</span>, input_shape=(<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>)))</span><br><span class="line">model.add(layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_4&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_8 (Conv2D)            (None, 148, 148, 32)      896       
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 74, 74, 32)        0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 72, 72, 64)        18496     
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 17, 17, 128)       0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 36992)             0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               18940416  
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 513       
=================================================================
Total params: 19,034,177
Trainable params: 19,034,177
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>在编译这一步，和前面一样，我们将使用RMSprop 优化器。因为网络最后一层是单一sigmoid单元，所以我们将使用二元交叉熵作为损失函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 配置模型用于训练</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,optimizer=optimizers.RMSprop(lr=<span class="number">1e-4</span>),metrics=[<span class="string">'acc'</span>])</span><br></pre></td></tr></table></figure>
<h2 id="4-数据预处理"><a href="#4-数据预处理" class="headerlink" title="4. 数据预处理"></a>4. 数据预处理</h2><p>你现在已经知道，将数据输入神经网络之前，应该将数据格式化为经过预处理的浮点数张量。现在，数据以 JPEG 文件的形式保存在硬盘中，所以数据预处理步骤大致如下：</p>
<ol>
<li>读取图像文件</li>
<li>将JPEG文件解码为RGB像素网格</li>
<li>将这些像素网格转换为浮点数张量</li>
<li>将像素值（0~255 范围内）缩放到 <code>[0, 1]</code> 区间（正如你所知，神经网络喜欢处理较小的输<br>入值）</li>
</ol>
<p>这些步骤可能看起来有点吓人，但幸运的是，Keras 拥有自动完成这些步骤的工具。Keras有一个图像处理辅助工具的模块，位于<code>keras.preprocessing.image</code>。特别地，它包含<code>ImageDataGenerator</code>类，可以快速创建Python生成器，能够将硬盘上的图像文件自动转换为预处理好的张量批量。下面我们将用到这个类。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用ImageDataGenerator 从目录中读取图像</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line">train_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line">test_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">        train_dir,</span><br><span class="line">        target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">        batch_size=<span class="number">20</span>,</span><br><span class="line">        class_mode=<span class="string">'binary'</span>)</span><br><span class="line">validation_generator = test_datagen.flow_from_directory(</span><br><span class="line">        validation_dir,</span><br><span class="line">        target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">        batch_size=<span class="number">20</span>,</span><br><span class="line">        class_mode=<span class="string">'binary'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
</code></pre><p>我们来看一下其中一个生成器的输出：它生成了150×150 的RGB 图像［形状为(20,150, 150, 3)］与二进制标签［形状为(20,)］组成的批量。每个批量中包含20 个样本（批量大小）。注意，生成器会不停地生成这些批量，它会不断循环目标文件夹中的图像。因此，你需要在某个时刻终止（break）迭代循环。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> data_batch, labels_batch <span class="keyword">in</span> train_generator:</span><br><span class="line">    print(<span class="string">'data batch shape:'</span>, data_batch.shape)</span><br><span class="line">    print(<span class="string">'labels batch shape:'</span>, labels_batch.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<pre><code>data batch shape: (20, 150, 150, 3)
labels batch shape: (20,)
</code></pre><p>利用生成器，我们让模型对数据进行拟合。我们将使用<code>fit_generator</code>方法来拟合，它在数据生成器上的效果和fit 相同。它的第一个参数应该是一个Python生成器，可以不停地生成输入和目标组成的批量，比如<code>train_generator</code>。因为数据是不断生成的，所以Keras模型要知道每一轮需要从生成器中抽取多少个样本。这是<code>steps_per_epoch</code>参数的作用：从生成器中抽取<code>steps_per_epoch</code>个批量后（即运行了<code>steps_per_epoch</code>次梯度下降），拟合过程将进入下一个轮次。本例中，每个批量包含20个样本，所以读取完所有2000 个样本需要100个批量。</p>
<p>使用<code>fit_generator</code>时，你可以传入一个<code>validation_data</code>参数，其作用和在<code>fit</code>方法中类似。值得注意的是，这个参数可以是一个数据生成器，但也可以是Numpy数组组成的元组。如果向<code>validation_data</code>传入一个生成器，那么这个生成器应该能够不停地生成验证数据批量，因此你还需要指定<code>validation_steps</code>参数，说明需要从验证生成器中抽取多少个批次用于评估。</p>
<h2 id="5-拟合模型"><a href="#5-拟合模型" class="headerlink" title="5. 拟合模型"></a>5. 拟合模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit_generator(</span><br><span class="line">    train_generator,</span><br><span class="line">    steps_per_epoch=<span class="number">100</span>,</span><br><span class="line">    epochs=<span class="number">30</span>,</span><br><span class="line">    validation_data=validation_generator,</span><br><span class="line">    validation_steps=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p>始终在训练完成后保存模型，这是一种良好实践。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">model.save(<span class="string">'model/ComputerVersion/cats_and_dogs_small_1.h5'</span>)</span><br></pre></td></tr></table></figure>
<p>我们来分别绘制训练过程中模型在训练数据和验证数据上的损失和精度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">acc = history.history[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_acc'</span>]</span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line">epochs = range(<span class="number">1</span>, len(acc) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_29_0.png" width="400" height="400" alt="训练损失和验证损失" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练损失和验证损失</div>
</center>


<center>
    <img src="\Pic\DeepLearning_Pic\output_29_1.png" width="400" height="400" alt="训练精度和验证精度" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练精度和验证精度</div>
</center>



<p>从这些图像中都能看出过拟合的特征。训练精度随着时间线性增加，直到接近100%，而验证精度则停留在70%~72%。验证损失仅在5 轮后就达到最小值，然后保持不变，而训练损失则一直线性下降，直到接近于0。</p>
<p>因为训练样本相对较少（2000 个），所以过拟合是你最关心的问题。前面已经介绍过几种降低过拟合的技巧，比如dropout 和权重衰减（L2 正则化）。现在我们将使用一种针对于计算机视觉领域的新方法，在用深度学习模型处理图像时几乎都会用到这种方法，它就是数据增强（data augmentation）。</p>
<h2 id="6-使用数据增强"><a href="#6-使用数据增强" class="headerlink" title="6. 使用数据增强"></a>6. 使用数据增强</h2><p>过拟合的原因是学习样本太少，导致无法训练出能够泛化到新数据的模型。如果拥有无限的数据，那么模型能够观察到数据分布的所有内容，这样就永远不会过拟合。数据增强是从现有的训练样本中生成更多的训练数据，其方法是利用多种能够生成可信图像的随机变换来增加（augment）样本。其目标是，模型在训练时不会两次查看完全相同的图像。这让模型能够观察到数据的更多内容，从而具有更好的泛化能力。</p>
<p>在Keras 中，这可以通过对<code>ImageDataGenerator</code>实例读取的图像执行多次随机变换来实现。我们先来看一个例子。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 利用ImageDataGenerator 来设置数据增强</span></span><br><span class="line">datagen = ImageDataGenerator(</span><br><span class="line">    rotation_range=<span class="number">40</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    shear_range=<span class="number">0.2</span>,</span><br><span class="line">    zoom_range=<span class="number">0.2</span>,</span><br><span class="line">    horizontal_flip=<span class="literal">True</span>,</span><br><span class="line">    fill_mode=<span class="string">'nearest'</span>)</span><br></pre></td></tr></table></figure>
<p>这里只选择了几个参数（想了解更多参数，请查阅Keras 文档）。我们来快速介绍一下这些<br>参数的含义。</p>
<ul>
<li><code>rotation_range</code>是角度值（在 0~180 范围内），表示图像随机旋转的角度范围。</li>
<li><code>width_shift</code> 和 <code>height_shift</code> 是图像在水平或垂直方向上平移的范围（相对于总宽度或总高度的比例）。</li>
<li><code>shear_range</code>是随机错切变换的角度。</li>
<li><code>zoom_range</code>是图像随机缩放的范围。</li>
<li><code>horizontal_flip</code> 是随机将一半图像水平翻转。如果没有水平不对称的假设（比如真实世界的图像），这种做法是有意义的。</li>
<li><code>fill_mode</code>是用于填充新创建像素的方法，这些新像素可能来自于旋转或宽度/高度平移。</li>
</ul>
<p>我们来看一下增强后的图像</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 显示几个随机增强后的训练图像</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line">fnames = [os.path.join(train_cats_dir, fname) <span class="keyword">for</span> fname <span class="keyword">in</span> os.listdir(train_cats_dir)]</span><br><span class="line">img_path = fnames[<span class="number">3</span>]</span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">150</span>, <span class="number">150</span>))</span><br><span class="line">x = image.img_to_array(img)</span><br><span class="line">x = x.reshape((<span class="number">1</span>,) + x.shape)</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> datagen.flow(x, batch_size=<span class="number">1</span>):</span><br><span class="line">    plt.figure(i)</span><br><span class="line">    imgplot = plt.imshow(image.array_to_img(batch[<span class="number">0</span>]))</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">4</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_33_0.png" width="400" height="400" alt="增强后的图像" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">增强后的图像（1）</div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_33_1.png" width="400" height="400" alt="增强后的图像" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">增强后的图像（2）</div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_33_2.png" width="400" height="400" alt="增强后的图像" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">增强后的图像（3）</div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_33_3.png" width="400" height="400" alt="增强后的图像" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">增强后的图像（4）</div>
</center>




<p>如果你使用这种数据增强来训练一个新网络，那么网络将不会两次看到同样的输入。但网络看到的输入仍然是高度相关的，因为这些输入都来自于少量的原始图像。你无法生成新信息，而只能混合现有信息。因此，这种方法可能不足以完全消除过拟合。为了进一步降低过拟合，你还需要向模型中添加一个<code>Dropout</code>层，添加到密集连接分类器之前。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>,input_shape=(<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>)))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,optimizer=optimizers.RMSprop(lr=<span class="number">1e-4</span>),metrics=[<span class="string">'acc'</span>])</span><br></pre></td></tr></table></figure>
<p>我们来训练这个使用了数据增强和dropout 的网络。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 利用数据增强生成器训练卷积神经网络</span></span><br><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">    rescale=<span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">    rotation_range=<span class="number">40</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    shear_range=<span class="number">0.2</span>,</span><br><span class="line">    zoom_range=<span class="number">0.2</span>,</span><br><span class="line">    horizontal_flip=<span class="literal">True</span>,)</span><br><span class="line"></span><br><span class="line">test_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">    train_dir,</span><br><span class="line">    target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">    batch_size=<span class="number">32</span>,</span><br><span class="line">    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">validation_generator = test_datagen.flow_from_directory(</span><br><span class="line">    validation_dir,</span><br><span class="line">    target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">    batch_size=<span class="number">32</span>,</span><br><span class="line">    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit_generator(</span><br><span class="line">    train_generator,</span><br><span class="line">    steps_per_epoch=<span class="number">100</span>,</span><br><span class="line">    epochs=<span class="number">100</span>,</span><br><span class="line">    validation_data=validation_generator,</span><br><span class="line">    validation_steps=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">model.save(<span class="string">'model/ComputerVersion/cats_and_dogs_small_2.h5'</span>)</span><br></pre></td></tr></table></figure>
<p>我们再次绘制结果，使用了数据增强和dropout 之后，模型不再过拟合：训练曲线紧紧跟随着验证曲线。现在的精度为82%，比未正则化的模型提高了15%（相对比例）。</p>
<p>通过进一步使用正则化方法以及调节网络参数（比如每个卷积层的过滤器个数或网络中的层数），你可以得到更高的精度，可以达到86%或87%。但只靠从头开始训练自己的卷积神经网络，再想提高精度就十分困难，因为可用的数据太少。想要在这个问题上进一步提高精度，下一步需要使用预训练的模型，这是接下来两节的重点。</p>
<h1 id="三、使用预训练的卷积神经网络"><a href="#三、使用预训练的卷积神经网络" class="headerlink" title="三、使用预训练的卷积神经网络"></a>三、使用预训练的卷积神经网络</h1><p>想要将深度学习应用于小型图像数据集，一种常用且非常高效的方法是使用预训练网络。预训练网络（pretrained network）是一个保存好的网络，之前已在大型数据集（通常是大规模图像分类任务）上训练好。如果这个原始数据集足够大且足够通用，那么预训练网络学到的特征的空间层次结构可以有效地作为视觉世界的通用模型，因此这些特征可用于各种不同的计算机视觉问题，即使这些新问题涉及的类别和原始任务完全不同。举个例子，你在ImageNet 上训练了一个网络（其类别主要是动物和日常用品），然后将这个训练好的网络应用于某个不相干的任务，比如在图像中识别家具。这种学到的特征在不同问题之间的可移植性，是深度学习与许多早期浅层学习方法相比的重要优势，它使得深度学习对小数据问题非常有效。</p>
<p>本例中，假设有一个在ImageNet 数据集（140 万张标记图像，1000 个不同的类别）上训练好的大型卷积神经网络。ImageNet 中包含许多动物类别，其中包括不同种类的猫和狗，因此可以认为它在猫狗分类问题上也能有良好的表现。我们将使用VGG16 架构，它由Karen Simonyan 和Andrew Zisserman 在2014 年开发a。对于ImageNet，它是一种简单而又广泛使用的卷积神经网络架构。虽然VGG16 是一个比较旧的模型，性能远比不了当前最先进的模型，而且还比许多新模型更为复杂，但我之所以选择它，是因为它的架构与你已经熟悉的架构很相似，因此无须引入新概念就可以很好地理解。这可能是你第一次遇到这种奇怪的模型名称——VGG、ResNet、Inception、Inception-ResNet、Xception 等。你会习惯这些名称的，因为如果你一直用深度学习做计算机视觉的话，它们会频繁出现。使用预训练网络有两种方法：<strong>特征提取</strong>（feature extraction）和<strong>微调模型</strong>（fine-tuning）。两种方法我们都会介绍。首先来看特征提取。</p>
<h2 id="1-特征提取"><a href="#1-特征提取" class="headerlink" title="1. 特征提取"></a>1. 特征提取</h2><p>特征提取是使用之前网络学到的表示来从新样本中提取出有趣的特征。然后将这些特征输入一个新的分类器，从头开始训练。</p>
<p>如前所述，用于图像分类的卷积神经网络包含两部分：首先是一系列池化层和卷积层，最后是一个密集连接分类器。第一部分叫作模型的卷积基（convolutional base）。对于卷积神经网络而言，特征提取就是取出之前训练好的网络的卷积基，在上面运行新数据，然后在输出上面训练一个新的分类器</p>
<center>
    <img src="\Pic\DeepLearning_Pic\预训练模型1.png" width="300" height="300" alt="保持卷积基不变，改变分类器" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">保持卷积基不变，改变分类器</div>
</center>

<p>为什么仅重复使用卷积基？我们能否也重复使用密集连接分类器？一般来说，应该避免这么做。原因在于卷积基学到的表示可能更加通用，因此更适合重复使用。卷积神经网络的特征图表示通用概念在图像中是否存在，无论面对什么样的计算机视觉问题，这种特征图都可能很有用。但是，分类器学到的表示必然是针对于模型训练的类别，其中仅包含某个类别出现在整张图像中的概率信息。此外，密集连接层的表示不再包含物体在输入图像中的位置信息。密集连接层舍弃了空间的概念，而物体位置信息仍然由卷积特征图所描述。如果物体位置对于问题很重要，那么密集连接层的特征在很大程度上是无用的。</p>
<p>注意，某个卷积层提取的表示的通用性（以及可复用性）取决于该层在模型中的深度。模型中更靠近底部的层提取的是局部的、高度通用的特征图（比如视觉边缘、颜色和纹理），而更靠近顶部的层提取的是更加抽象的概念（比如“猫耳朵”或“狗眼睛”）。因此，如果你的新数据集与原始模型训练的数据集有很大差异，那么最好只使用模型的前几层来做特征提取，而不是使用整个卷积基。</p>
<p>本例中，由于ImageNet的类别中包含多种狗和猫的类别，所以重复使用原始模型密集连接层中所包含的信息可能很有用。但我们选择不这么做，以便涵盖新问题的类别与原始模型的类别不一致的更一般情况。我们来实践一下，使用在ImageNet上训练的VGG16 网络的卷积基从猫狗图像中提取有趣的特征，然后在这些特征上训练一个猫狗分类器。VGG16 等模型内置于Keras 中。你可以从<code>keras.applications</code>模块中导入。下面是<code>keras.applications</code>中的一部分图像分类模型（都是在ImageNet数据集上预训练得到的）：</p>
<ul>
<li>Xception</li>
<li>Inception V3</li>
<li>ResNet50</li>
<li>VGG16</li>
<li>VGG19</li>
<li>MobileNet</li>
</ul>
<p>我们将<code>VGG16</code>模型实例化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将 VGG16 卷积基实例化</span></span><br><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> VGG16</span><br><span class="line">conv_base = VGG1616(weights=<span class="string">'imagenet'</span>, include_top=<span class="literal">False</span>, input_shape=(<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<p>这里向构造函数中传入了三个参数。</p>
<ul>
<li>weights指定模型初始化的权重检查点。</li>
<li>include_top 指定模型最后是否包含密集连接分类器。默认情况下，这个密集连接分类器对应于ImageNet的1000个类别。因为我们打算使用自己的密集连接分类器（只有两个类别：cat和dog），所以不需要包含它。</li>
<li>input_shape是输入到网络中的图像张量的形状。这个参数完全是可选的，如果不传入这个参数，那么网络能够处理任意形状的输入。VGG16卷积基的详细架构如下所示。它和你已经熟悉的简单卷积神经网络很相似。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv_base.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;vgg16&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>最后的特征图形状为(4, 4, 512)。我们将在这个特征上添加一个密集连接分类器。接下来，下一步有两种方法可供选择：</p>
<ul>
<li>在你的数据集上运行卷积基，将输出保存成硬盘中的Numpy数组，然后用这个数据作为输入，输入到独立的密集连接分类器中。这种方法速度快，计算代价低，因为对于每个输入图像只需运行一次卷积基，而卷积基是目前流程中计算代价最高的。但出于同样的原因，这种方法不允许你使用数据增强。</li>
<li>在顶部添加<code>Dense</code>层来扩展已有模型（即<code>conv_base</code>），并在输入数据上端到端地运行整个模型。这样你可以使用数据增强，因为每个输入图像进入模型时都会经过卷积基。但出于同样的原因，这种方法的计算代价比第一种要高很多。</li>
</ul>
<h2 id="2-不使用数据增强的快速特征提取"><a href="#2-不使用数据增强的快速特征提取" class="headerlink" title="2. 不使用数据增强的快速特征提取"></a>2. 不使用数据增强的快速特征提取</h2><p>首先，运行<code>ImageDataGenerator</code>实例，将图像及其标签提取为<code>Numpy</code>数组。我们需要调用<code>conv_base</code>模型的<code>predict</code>方法来从这些图像中提取特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用预训练的卷积基提取特征</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line">base_dir = <span class="string">'data/cat_dog/cats_and_dogs_small'</span></span><br><span class="line">train_dir = os.path.join(base_dir, <span class="string">'train'</span>)</span><br><span class="line">validation_dir = os.path.join(base_dir, <span class="string">'validation'</span>)</span><br><span class="line">test_dir = os.path.join(base_dir, <span class="string">'test'</span>)</span><br><span class="line">datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_features</span><span class="params">(directory, sample_count)</span>:</span></span><br><span class="line">    features = np.zeros(shape=(sample_count, <span class="number">4</span>, <span class="number">4</span>, <span class="number">512</span>))</span><br><span class="line">    labels = np.zeros(shape=(sample_count))</span><br><span class="line">    generator = datagen.flow_from_directory(</span><br><span class="line">        directory,</span><br><span class="line">        target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        class_mode=<span class="string">'binary'</span>)</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> inputs_batch, labels_batch <span class="keyword">in</span> generator:</span><br><span class="line">        features_batch = conv_base.predict(inputs_batch)</span><br><span class="line">        features[i * batch_size : (i + <span class="number">1</span>) * batch_size] = features_batch</span><br><span class="line">        labels[i * batch_size : (i + <span class="number">1</span>) * batch_size] = labels_batch</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i * batch_size &gt;= sample_count:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line"></span><br><span class="line">train_features, train_labels = extract_features(train_dir, <span class="number">2000</span>)</span><br><span class="line">validation_features, validation_labels = extract_features(validation_dir, <span class="number">1000</span>)</span><br><span class="line">test_features, test_labels = extract_features(test_dir, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
</code></pre><p>目前，提取的特征形状为<code>(samples, 4, 4, 512)</code>。我们要将其输入到密集连接分类器中，所以首先必须将其形状展平为<code>(samples, 8192)</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_features = np.reshape(train_features, (<span class="number">2000</span>, <span class="number">4</span> * <span class="number">4</span> * <span class="number">512</span>))</span><br><span class="line">validation_features = np.reshape(validation_features, (<span class="number">1000</span>, <span class="number">4</span> * <span class="number">4</span> * <span class="number">512</span>))</span><br><span class="line">test_features = np.reshape(test_features, (<span class="number">1000</span>, <span class="number">4</span> * <span class="number">4</span> * <span class="number">512</span>))</span><br></pre></td></tr></table></figure>
<p>现在你可以定义你的密集连接分类器（注意要使用<code>dropout</code>正则化），并在刚刚保存的数据和标签上训练这个分类器。</p>
<h3 id="定义并训练密集连接分类器"><a href="#定义并训练密集连接分类器" class="headerlink" title="定义并训练密集连接分类器"></a>定义并训练密集连接分类器</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>, input_dim=<span class="number">4</span> * <span class="number">4</span> * <span class="number">512</span>))</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.compile(optimizer=optimizers.RMSprop(lr=<span class="number">2e-5</span>),</span><br><span class="line">    loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">    metrics=[<span class="string">'acc'</span>])</span><br><span class="line">history = model.fit(train_features, train_labels,</span><br><span class="line">    epochs=<span class="number">30</span>,</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">    validation_data=(validation_features, validation_labels))</span><br></pre></td></tr></table></figure>
<p>训练速度非常快，因为你只需处理两个<code>Dense</code>层。我们来看一下训练期间的损失曲线和精度曲线：</p>
<h3 id="绘制结果"><a href="#绘制结果" class="headerlink" title="绘制结果"></a>绘制结果</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">acc = history.history[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_acc'</span>]</span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">epochs = range(<span class="number">1</span>, len(acc) + <span class="number">1</span>)</span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_51_0.png" width="400" height="400" alt="训练损失和验证损失" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练损失和验证损失</div>
</center>


<center>
    <img src="\Pic\DeepLearning_Pic\output_51_1.png" width="400" height="400" alt="训练精度和验证精度" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练精度和验证精度</div>
</center>


<p>我们的验证精度达到了约90%，比上一节从头开始训练的小型模型效果要好得多。但从图中也可以看出，虽然<code>dropout</code>比率相当大，但模型几乎从一开始就过拟合。这是因为本方法没有使用数据增强，而数据增强对防止小型图像数据集的过拟合非常重要。</p>
<h2 id="3-使用数据增强的特征提取"><a href="#3-使用数据增强的特征提取" class="headerlink" title="3. 使用数据增强的特征提取"></a>3. 使用数据增强的特征提取</h2><p>下面我们来看一下特征提取的第二种方法，它的速度更慢，计算代价更高，但在训练期间可以使用数据增强。这种方法就是：扩展<code>conv_base</code>模型，然后在输入数据上端到端地运行模型。</p>
<blockquote>
<p>注意 本方法计算代价很高，只在有GPU的情况下才能尝试运行。它在CPU上是绝对难以运行的。如果你无法在GPU上运行代码，那么就采用第一种方法。</p>
</blockquote>
<p>模型的行为和层类似，所以你可以向<code>Sequential</code>模型中添加一个模型（比如<code>conv_base</code>），就像添加一个层一样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在卷积基上添加一个密集连接分类器</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(conv_base)</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>
<p>现在模型的架构如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_7&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
vgg16 (Model)                (None, 4, 4, 512)         14714688  
_________________________________________________________________
flatten_4 (Flatten)          (None, 8192)              0         
_________________________________________________________________
dense_9 (Dense)              (None, 256)               2097408   
_________________________________________________________________
dense_10 (Dense)             (None, 1)                 257       
=================================================================
Total params: 16,812,353
Trainable params: 16,812,353
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>如你所见，<code>VGG16</code>的卷积基有14 714 688个参数，非常多。在其上添加的分类器有200万个参数。</p>
<p>在编译和训练模型之前，一定要“冻结”卷积基。冻结（<code>freeze</code>）一个或多个层是指在训练过程中保持其权重不变。如果不这么做，那么卷积基之前学到的表示将会在训练过程中被修改。因为其上添加的<code>Dense</code>层是随机初始化的，所以非常大的权重更新将会在网络中传播，对之前学到的表示造成很大破坏。</p>
<p>在<code>Keras</code>中，冻结网络的方法是将其<code>trainable</code>属性设为<code>False</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'This is the number of trainable weights before freezing the conv base:'</span>, len(model.trainable_weights))</span><br><span class="line">conv_base.trainable = <span class="literal">False</span></span><br><span class="line">print(<span class="string">'This is the number of trainable weights after freezing the conv base:'</span>, len(model.trainable_weights))</span><br></pre></td></tr></table></figure>
<pre><code>This is the number of trainable weights before freezing the conv base: 30
This is the number of trainable weights after freezing the conv base: 4
</code></pre><p>如此设置之后，只有添加的两个Dense 层的权重才会被训练。总共有4 个权重张量，每层 2 个（主权重矩阵和偏置向量）。注意，为了让这些修改生效，你必须先编译模型。如果在编译之后修改了权重的<code>trainable</code>属性，那么应该重新编译模型，否则这些修改将被忽略。</p>
<p>现在你可以开始训练模型了，使用和前一个例子相同的数据增强设置。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 利用冻结的卷积基端到端地训练模型</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line"></span><br><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">    rescale=<span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">    rotation_range=<span class="number">40</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    shear_range=<span class="number">0.2</span>,</span><br><span class="line">    zoom_range=<span class="number">0.2</span>,</span><br><span class="line">    horizontal_flip=<span class="literal">True</span>,</span><br><span class="line">    fill_mode=<span class="string">'nearest'</span>)</span><br><span class="line"></span><br><span class="line">test_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">    train_dir,</span><br><span class="line">    target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">validation_generator = test_datagen.flow_from_directory(</span><br><span class="line">    validation_dir,</span><br><span class="line">    target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">2e-5</span>),</span><br><span class="line">    metrics=[<span class="string">'acc'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit_generator(</span><br><span class="line">    train_generator,</span><br><span class="line">    steps_per_epoch=<span class="number">100</span>,</span><br><span class="line">    epochs=<span class="number">30</span>,</span><br><span class="line">    validation_data=validation_generator,</span><br><span class="line">    validation_steps=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(<span class="string">'model/ComputerVersion/cats_and_dogs_small_dataEnforcementFeatureExtraction.h5'</span>)</span><br></pre></td></tr></table></figure>
<p>如你所见，这比从头开始训练的小型卷积神经网络要好得多。</p>
<h2 id="4-微调模型"><a href="#4-微调模型" class="headerlink" title="4. 微调模型"></a>4. 微调模型</h2><p>另一种广泛使用的模型复用方法是模型微调（fine-tuning），与特征提取互为补充。对于用于特征提取的冻结的模型基，微调是指将其顶部的几层“解冻”，并将这解冻的几层和新增加的部分（本例中是全连接分类器）联合训练。之所以叫作微调，是因为它只是略微调整了所复用模型中更加抽象的表示，以便让这些表示与手头的问题更加相关。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\fine_tuning.png" width="200" height="200" alt="微调VGG16网络的最后一个卷积块" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">微调VGG16网络的最后一个卷积块</div>
</center>

<p>前面说过，冻结VGG16的卷积基是为了能够在上面训练一个随机初始化的分类器。同理，只有上面的分类器已经训练好了，才能微调卷积基的顶部几层。如果分类器没有训练好，那么训练期间通过网络传播的误差信号会特别大，微调的几层之前学到的表示都会被破坏。因此，微调网络的步骤如下。</p>
<ol>
<li>在已经训练好的基网络（base network）上添加自定义网络。</li>
<li>冻结基网络。</li>
<li>训练所添加的部分。</li>
<li>解冻基网络的一些层。</li>
<li>联合训练解冻的这些层和添加的部分。</li>
</ol>
<p>你在做特征提取时已经完成了前三个步骤。我们继续进行第四步：先解冻<code>conv_base</code>，然后冻结其中的部分层。提醒一下，卷积基的架构如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv_base.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;vgg16&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         
=================================================================
Total params: 14,714,688
Trainable params: 7,079,424
Non-trainable params: 7,635,264
_________________________________________________________________
</code></pre><p>我们将微调最后三个卷积层，也就是说，直到<code>block4_pool</code>的所有层都应该被冻结，而<code>block5_conv1</code>、<code>block5_conv2</code>和<code>block5_conv3</code>三层应该是可训练的。为什么不微调更多层？为什么不微调整个卷积基？你当然可以这么做，但需要考虑以下几点。</p>
<ul>
<li>卷积基中更靠底部的层编码的是更加通用的可复用特征，而更靠顶部的层编码的是更专业化的特征。微调这些更专业化的特征更加有用，因为它们需要在你的新问题上改变用途。微调更靠底部的层，得到的回报会更少。</li>
<li>训练的参数越多，过拟合的风险越大。卷积基有 1500 万个参数，所以在你的小型数据集上训练这么多参数是有风险的。</li>
</ul>
<p>因此，在这种情况下，一个好策略是仅微调卷积基最后的两三层。我们从上一个例子结束的地方开始，继续实现此方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 冻结直到某一层的所有层</span></span><br><span class="line">conv_base.trainable = <span class="literal">True</span></span><br><span class="line">set_trainable = <span class="literal">False</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> conv_base.layers:</span><br><span class="line">    <span class="keyword">if</span> layer.name == <span class="string">'block5_conv1'</span>:</span><br><span class="line">        set_trainable = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> set_trainable:</span><br><span class="line">        layer.trainable = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        layer.trainable = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>现在你可以开始微调网络。我们将使用学习率非常小的RMSProp优化器来实现。之所以让学习率很小，是因为对于微调的三层表示，我们希望其变化范围不要太大，太大的权重更新可能会破坏这些表示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 微调模型</span></span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">1e-5</span>),</span><br><span class="line">    metrics=[<span class="string">'acc'</span>])</span><br><span class="line">history = model.fit_generator(</span><br><span class="line">    train_generator,</span><br><span class="line">    steps_per_epoch=<span class="number">100</span>,</span><br><span class="line">    epochs=<span class="number">100</span>,</span><br><span class="line">    validation_data=validation_generator,</span><br><span class="line">    validation_steps=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(<span class="string">'model/ComputerVersion/cats_and_dogs_small_dataEnforcementFineTuning.h5'</span>)</span><br></pre></td></tr></table></figure>
<p>我们用和前面一样的绘图代码来绘制结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">acc = history.history[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_acc'</span>]</span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">epochs = range(<span class="number">1</span>, len(acc) + <span class="number">1</span>)</span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_70_0.png" width="400" height="400" alt="训练精度和验证精度" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练精度和验证精度</div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_70_1.png" width="400" height="400" alt="训练损失和验证损失" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练损失和验证损失</div>
</center>




<p>这些曲线看起来包含噪声。为了让图像更具可读性，你可以将每个损失和精度都替换为指数移动平均值，从而让曲线变得平滑。下面用一个简单的实用函数来实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使曲线变得平滑</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smooth_curve</span><span class="params">(points, factor=<span class="number">0.8</span>)</span>:</span></span><br><span class="line">    smoothed_points = []</span><br><span class="line">    <span class="keyword">for</span> point <span class="keyword">in</span> points:</span><br><span class="line">        <span class="keyword">if</span> smoothed_points:</span><br><span class="line">            previous = smoothed_points[<span class="number">-1</span>]</span><br><span class="line">            smoothed_points.append(previous * factor + point * (<span class="number">1</span> - factor))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            smoothed_points.append(point)</span><br><span class="line">    <span class="keyword">return</span> smoothed_points</span><br><span class="line"></span><br><span class="line">plt.plot(epochs,smooth_curve(acc), <span class="string">'bo'</span>, label=<span class="string">'Smoothed training acc'</span>)</span><br><span class="line">plt.plot(epochs,smooth_curve(val_acc), <span class="string">'b'</span>, label=<span class="string">'Smoothed validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(epochs,smooth_curve(loss), <span class="string">'bo'</span>, label=<span class="string">'Smoothed training loss'</span>)</span><br><span class="line">plt.plot(epochs,smooth_curve(val_loss), <span class="string">'b'</span>, label=<span class="string">'Smoothed validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_72_0.png" width="400" height="400" alt="训练精度和验证精度" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">平滑后的训练精度和验证精度</div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_72_1.png" width="400" height="400" alt="训练损失和验证损失" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">平滑后的训练损失和验证损失</div>
</center>


<p>注意，从损失曲线上看不出与之前相比有任何真正的提高（实际上还在变差）。你可能感到奇怪，如果损失没有降低，那么精度怎么能保持稳定或提高呢？答案很简单：图中展示的是逐点（pointwise）损失值的平均值，但影响精度的是损失值的分布，而不是平均值，因为精度是模型预测的类别概率的二进制阈值。即使从平均损失中无法看出，但模型也仍然可能在改进。现在，你可以在测试数据上最终评估这个模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_generator = test_datagen.flow_from_directory(</span><br><span class="line">    test_dir,</span><br><span class="line">    target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">    class_mode=<span class="string">'binary'</span>)</span><br><span class="line">test_loss, test_acc = model.evaluate_generator(test_generator, steps=<span class="number">50</span>)</span><br><span class="line">print(<span class="string">'test acc:'</span>, test_acc)</span><br></pre></td></tr></table></figure>
<pre><code>Found 1000 images belonging to 2 classes.
test acc: 0.9399999976158142
</code></pre><h1 id="四、卷积神经网络的可视化"><a href="#四、卷积神经网络的可视化" class="headerlink" title="四、卷积神经网络的可视化"></a>四、卷积神经网络的可视化</h1><p>人们常说，深度学习模型是“黑盒”，即模型学到的表示很难用人类可以理解的方式来提取和呈现。虽然对于某些类型的深度学习模型来说，这种说法部分正确，但对卷积神经网络来说绝对不是这样。卷积神经网络学到的表示非常适合可视化，很大程度上是因为它们是视觉概念的表示。自2013 年以来，人们开发了多种技术来对这些表示进行可视化和解释。我们不会全部介绍，但会介绍三种最容易理解也最有用的方法。</p>
<ul>
<li>可视化卷积神经网络的中间输出（中间激活）：有助于理解卷积神经网络连续的层如何对输入进行变换，也有助于初步了解卷积神经网络每个过滤器的含义。</li>
<li>可视化卷积神经网络的过滤器：有助于精确理解卷积神经网络中每个过滤器容易接受的视觉模式或视觉概念。</li>
<li>可视化图像中类激活的热力图：有助于理解图像的哪个部分被识别为属于某个类别，从而可以定位图像中的物体。</li>
</ul>
<p>对于第一种方法（即激活的可视化），我们将使用猫狗分类问题上从头开始训练的小型卷积神经网络。对于另外两种可视化方法，我们将使用VGG16模型。</p>
<h2 id="1-可视化中间激活"><a href="#1-可视化中间激活" class="headerlink" title="1. 可视化中间激活"></a>1. 可视化中间激活</h2><p>可视化中间激活，是指对于给定输入，展示网络中各个卷积层和池化层输出的特征图（层的输出通常被称为该层的激活，即激活函数的输出）。这让我们可以看到输入如何被分解为网络学到的不同过滤器。我们希望在三个维度对特征图进行可视化：宽度、高度和深度（通道）。每个通道都对应相对独立的特征，所以将这些特征图可视化的正确方法是将每个通道的内容分别绘制成二维图像。我们首先来加载先前保存的模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line">model = load_model(<span class="string">'model/ComputerVersion/cats_and_dogs_small_2.h5'</span>)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_5&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_11 (Conv2D)           (None, 148, 148, 32)      896       
_________________________________________________________________
max_pooling2d_7 (MaxPooling2 (None, 74, 74, 32)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 72, 72, 64)        18496     
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_9 (MaxPooling2 (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_10 (MaxPooling (None, 7, 7, 128)         0         
_________________________________________________________________
flatten_3 (Flatten)          (None, 6272)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 6272)              0         
_________________________________________________________________
dense_5 (Dense)              (None, 512)               3211776   
_________________________________________________________________
dense_6 (Dense)              (None, 1)                 513       
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>接下来，我们需要一张输入图像，即一张猫的图像，它不属于网络的训练图像。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img_path = <span class="string">"data/cat_dog/cats_and_dogs_small/test/cats/cat.1700.jpg"</span></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">150</span>, <span class="number">150</span>))</span><br><span class="line">img_tensor = image.img_to_array(img)</span><br><span class="line">img_tensor = np.expand_dims(img_tensor, axis=<span class="number">0</span>)</span><br><span class="line">img_tensor /= <span class="number">255.</span></span><br><span class="line">print(img_tensor.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(1, 150, 150, 3)
</code></pre><p>我们来显示这张图像</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.imshow(img_tensor[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_80_0.png" width="400" height="400" alt="a" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<p>为了提取想要查看的特征图，我们需要创建一个<code>Keras</code>模型，以图像批量作为输入，并输出所有卷积层和池化层的激活。为此，我们需要使用<code>Keras</code>的<code>Model</code>类。模型实例化需要两个参数：一个输入张量（或输入张量的列表）和一个输出张量（或输出张量的列表）。得到的类是一个<code>Keras</code>模型，就像你熟悉的<code>Sequential</code>模型一样，将特定输入映射为特定输出。<code>Model</code>类允许模型有多个输出，这一点与<code>Sequential</code>模型不同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用一个输入张量和一个输出张量列表将模型实例化</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line">layer_outputs = [layer.output <span class="keyword">for</span> layer <span class="keyword">in</span> model.layers[:<span class="number">8</span>]]</span><br><span class="line">activation_model = models.Model(inputs=model.input, outputs=layer_outputs)</span><br></pre></td></tr></table></figure>
<p>输入一张图像，这个模型将返回原始模型前8 层的激活值。这是第一次遇到的多输出模型，之前的模型都是只有一个输入和一个输出。一般情况下，模型可以有任意个输入和输出。这个模型有一个输入和8 个输出，即每层激活对应一个输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 以预测模式运行模型</span></span><br><span class="line">activations = activation_model.predict(img_tensor)</span><br><span class="line"><span class="comment"># 例如，对于输入的猫图像，第一个卷积层的激活如下所示。</span></span><br><span class="line">first_layer_activation = activations[<span class="number">0</span>]</span><br><span class="line">print(first_layer_activation.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(1, 148, 148, 32)
</code></pre><p>它是大小为148×148的特征图，有32个通道。我们来绘制原始模型第一层激活的第4个通道</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将第4个通道可视化</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.matshow(first_layer_activation[<span class="number">0</span>, :, :, <span class="number">4</span>], cmap=<span class="string">'viridis'</span>)</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_86_1.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">通道4激活</div>
</center>


<p>这个通道似乎是对角边缘检测器。我们再看一下第7个通道</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将第7个通道可视化</span></span><br><span class="line">plt.matshow(first_layer_activation[<span class="number">0</span>, :, :, <span class="number">7</span>], cmap=<span class="string">'viridis'</span>)</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_88_1.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">通道7激活</div>
</center>



<p>下面我们来绘制网络中所有激活的完整可视化。我们需要在8个特征图中的每一个中提取并绘制每一个通道，然后将结果叠加在一个大的图像张量中，按通道并排。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将每个中间激活的所有通道可视化</span></span><br><span class="line">layer_names = []</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> model.layers[:<span class="number">8</span>]:</span><br><span class="line">    layer_names.append(layer.name)</span><br><span class="line">images_per_row = <span class="number">16</span></span><br><span class="line"><span class="keyword">for</span> layer_name, layer_activation <span class="keyword">in</span> zip(layer_names, activations):</span><br><span class="line">    n_features = layer_activation.shape[<span class="number">-1</span>]</span><br><span class="line">    size = layer_activation.shape[<span class="number">1</span>]</span><br><span class="line">    n_cols = n_features // images_per_row</span><br><span class="line">    display_grid = np.zeros((size * n_cols, images_per_row * size))</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> range(n_cols):</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> range(images_per_row):</span><br><span class="line">            channel_image = layer_activation[<span class="number">0</span>,:, :,col * images_per_row + row]</span><br><span class="line">            channel_image -= channel_image.mean()</span><br><span class="line">            channel_image /= channel_image.std()</span><br><span class="line">            channel_image *= <span class="number">64</span></span><br><span class="line">            channel_image += <span class="number">128</span></span><br><span class="line">            channel_image = np.clip(channel_image, <span class="number">0</span>, <span class="number">255</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">            display_grid[col * size : (col + <span class="number">1</span>) * size,row * size : (row + <span class="number">1</span>) * size] = channel_image</span><br><span class="line">    scale = <span class="number">1.</span> / size</span><br><span class="line">    plt.figure(figsize=(scale * display_grid.shape[<span class="number">1</span>],</span><br><span class="line">    scale * display_grid.shape[<span class="number">0</span>]))</span><br><span class="line">    plt.title(layer_name)</span><br><span class="line">    plt.grid(<span class="literal">False</span>)</span><br><span class="line">    plt.imshow(display_grid, aspect=<span class="string">'auto'</span>, cmap=<span class="string">'viridis'</span>)</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_90_1.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_90_2.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<center>
    <img src="\Pic\DeepLearning_Pic\output_90_3.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<center>
    <img src="\Pic\DeepLearning_Pic\output_90_4.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<center>
    <img src="\Pic\DeepLearning_Pic\output_90_5.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_90_6.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_90_7.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_90_8.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>



<p>这里需要注意以下几点。</p>
<ul>
<li>第一层是各种边缘探测器的集合。在这一阶段，激活几乎保留了原始图像中的所有信息。</li>
<li>随着层数的加深，激活变得越来越抽象，并且越来越难以直观地理解。它们开始表示更高层次的概念，比如“猫耳朵”和“猫眼睛”。层数越深，其表示中关于图像视觉内容的信息就越少，而关于类别的信息就越多。</li>
<li>激活的稀疏度（sparsity）随着层数的加深而增大。在第一层里，所有过滤器都被输入图像激活，但在后面的层里，越来越多的过滤器是空白的。也就是说，输入图像中找不到这些过滤器所编码的模式。</li>
</ul>
<p>我们刚刚揭示了深度神经网络学到的表示的一个重要普遍特征：随着层数的加深，层所提取的特征变得越来越抽象。更高的层激活包含关于特定输入的信息越来越少，而关于目标的信息越来越多（本例中即图像的类别：猫或狗）。深度神经网络可以有效地作为信息蒸馏管道（information distillation pipeline），输入原始数据（本例中是RGB 图像），反复对其进行变换，将无关信息过滤掉（比如图像的具体外观），并放大和细化有用的信息（比如图像的类别）。</p>
<p>这与人类和动物感知世界的方式类似：人类观察一个场景几秒钟后，可以记住其中有哪些抽象物体（比如自行车、树），但记不住这些物体的具体外观。事实上，如果你试着凭记忆画一辆普通自行车，那么很可能完全画不出真实的样子，虽然你一生中见过上千辆自行车。你可以现在就试着画一下，这个说法绝对是真实的。你的大脑已经学会将视觉输入完全抽象化，即将其转换为更高层次的视觉概念，同时过滤掉不相关的视觉细节，这使得大脑很难记住周围事物的外观。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\bike.png" width="200" height="200" alt="（左图）试着凭记忆画一辆自行车；（右图）自行车示意图" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">（左图）试着凭记忆画一辆自行车；（右图）自行车示意图</div>
</center>

<h2 id="2-可视化卷积神经网络的过滤器"><a href="#2-可视化卷积神经网络的过滤器" class="headerlink" title="2. 可视化卷积神经网络的过滤器"></a>2. 可视化卷积神经网络的过滤器</h2><p>想要观察卷积神经网络学到的过滤器，另一种简单的方法是显示每个过滤器所响应的视觉模式。这可以通过在输入空间中进行梯度上升来实现：从空白输入图像开始，将梯度下降应用于卷积神经网络输入图像的值，其目的是让某个过滤器的响应最大化。得到的输入图像是选定过滤器具有最大响应的图像。</p>
<p>这个过程很简单：我们需要构建一个损失函数，其目的是让某个卷积层的某个过滤器的值最大化；然后，我们要使用随机梯度下降来调节输入图像的值，以便让这个激活值最大化。例如，对于在<code>ImageNet</code>上预训练的<code>VGG16</code>网络，其<code>block3_conv1</code>层第0 个过滤器激活的损失如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为过滤器的可视化定义损失张量</span></span><br><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> VGG16</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line">model = VGG16(weights=<span class="string">'imagenet'</span>,include_top=<span class="literal">False</span>)</span><br><span class="line">layer_name = <span class="string">'block3_conv1'</span></span><br><span class="line">filter_index = <span class="number">0</span></span><br><span class="line">layer_output = model.get_layer(layer_name).output</span><br><span class="line">loss = K.mean(layer_output[:, :, :, filter_index])</span><br></pre></td></tr></table></figure>
<p>为了实现梯度下降，我们需要得到损失相对于模型输入的梯度。为此，我们需要使用<code>Keras</code>的<code>backend</code>模块内置的<code>gradients</code>函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取损失相对于输入的梯度</span></span><br><span class="line">grads = K.gradients(loss, model.input)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>为了让梯度下降过程顺利进行，一个非显而易见的技巧是将梯度张量除以其L2范数（张量中所有值的平方的平均值的平方根）来标准化。这就确保了输入图像的更新大小始终位于相同的范围。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 梯度标准化技巧</span></span><br><span class="line">grads /= (K.sqrt(K.mean(K.square(grads))) + <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>
<p>现在你需要一种方法：给定输入图像，它能够计算损失张量和梯度张量的值。你可以定义一个Keras后端函数来实现此方法：<code>iterate</code>是一个函数，它将一个Numpy张量（表示为长度为1的张量列表）转换为两个Numpy张量组成的列表，这两个张量分别是损失值和梯度值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 给定Numpy输入值，得到Numpy输出值</span></span><br><span class="line">iterate = K.function([model.input], [loss, grads])</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">loss_value, grads_value = iterate([np.zeros((<span class="number">1</span>, <span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>))])</span><br></pre></td></tr></table></figure>
<p>现在你可以定义一个Python 循环来进行随机梯度下降。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 通过随机梯度下降让损失最大化</span></span><br><span class="line">input_img_data = np.random.random((<span class="number">1</span>, <span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>)) * <span class="number">20</span> + <span class="number">128.</span></span><br><span class="line">step = <span class="number">1.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">40</span>):</span><br><span class="line">    loss_value, grads_value = iterate([input_img_data])</span><br><span class="line">    input_img_data += grads_value * step</span><br></pre></td></tr></table></figure>
<p>得到的图像张量是形状为<code>(1, 150, 150, 3)</code>的浮点数张量，其取值可能不是<code>[0, 255]</code>区间内的整数。因此，你需要对这个张量进行后处理，将其转换为可显示的图像。下面这个简单的实用函数可以做到这一点。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将张量转换为有效图像的实用函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deprocess_image</span><span class="params">(x)</span>:</span></span><br><span class="line">    x -= x.mean()</span><br><span class="line">    x /= (x.std() + <span class="number">1e-5</span>)</span><br><span class="line">    x *= <span class="number">0.1</span></span><br><span class="line">    x += <span class="number">0.5</span></span><br><span class="line">    x = np.clip(x, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    x *= <span class="number">255</span></span><br><span class="line">    x = np.clip(x, <span class="number">0</span>, <span class="number">255</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>接下来，我们将上述代码片段放到一个Python函数中，输入一个层的名称和一个过滤器索引，它将返回一个有效的图像张量，表示能够将特定过滤器的激活最大化的模式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成过滤器可视化的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_pattern</span><span class="params">(layer_name, filter_index, size=<span class="number">150</span>)</span>:</span></span><br><span class="line">    layer_output = model.get_layer(layer_name).output</span><br><span class="line">    loss = K.mean(layer_output[:, :, :, filter_index])</span><br><span class="line">    grads = K.gradients(loss, model.input)[<span class="number">0</span>]</span><br><span class="line">    grads /= (K.sqrt(K.mean(K.square(grads))) + <span class="number">1e-5</span>)</span><br><span class="line">    iterate = K.function([model.input], [loss, grads])</span><br><span class="line">    input_img_data = np.random.random((<span class="number">1</span>, size, size, <span class="number">3</span>)) * <span class="number">20</span> + <span class="number">128.</span></span><br><span class="line">    step = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">40</span>):</span><br><span class="line">        loss_value, grads_value = iterate([input_img_data])</span><br><span class="line">        input_img_data += grads_value * step</span><br><span class="line">    img = input_img_data[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> deprocess_image(img)</span><br></pre></td></tr></table></figure>
<p>我们来试用一下这个函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.imshow(generate_pattern(<span class="string">'block3_conv1'</span>, <span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_106_1.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>



<p>看起来，<code>block3_conv1</code>层第0个过滤器响应的是波尔卡点（polka-dot）图案。下面来看有趣的部分：我们可以将每一层的每个过滤器都可视化。为了简单起见，我们只查看每一层的前64 个过滤器，并只查看每个卷积块的第一层（即<code>block1_conv1</code>、<code>block2_conv1</code>、<code>block3_conv1</code>、<code>block4_ conv1</code>、<code>block5_conv1</code>）。我们将输出放在一个8×8的网格中，每个网格是一个64像素×64像素的过滤器模式，两个过滤器模式之间留有一些黑边</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成某一层中所有过滤器响应模式组成的网格</span></span><br><span class="line">layer_name = <span class="string">'block1_conv1'</span></span><br><span class="line">size = <span class="number">64</span></span><br><span class="line">margin = <span class="number">5</span></span><br><span class="line">results = np.zeros((<span class="number">8</span> * size + <span class="number">7</span> * margin, <span class="number">8</span> * size + <span class="number">7</span> * margin, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">        filter_img = generate_pattern(layer_name, i + (j * <span class="number">8</span>), size=size)</span><br><span class="line">        horizontal_start = i * size + i * margin</span><br><span class="line">        horizontal_end = horizontal_start + size</span><br><span class="line">        vertical_start = j * size + j * margin</span><br><span class="line">        vertical_end = vertical_start + size</span><br><span class="line">        results[horizontal_start: horizontal_end, vertical_start: vertical_end, :] = filter_img</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">plt.imshow(results)</span><br></pre></td></tr></table></figure>
<p>这些过滤器可视化包含卷积神经网络的层如何观察世界的很多信息：卷积神经网络中每一层都学习一组过滤器，以便将其输入表示为过滤器的组合。这类似于傅里叶变换将信号分解为一组余弦函数的过程。随着层数的加深，卷积神经网络中的过滤器变得越来越复杂，越来越精细。</p>
<ul>
<li>模型第一层（block1_conv1）的过滤器对应简单的方向边缘和颜色（还有一些是彩色边缘）。</li>
<li>block2_conv1层的过滤器对应边缘和颜色组合而成的简单纹理。</li>
<li>更高层的过滤器类似于自然图像中的纹理：羽毛、眼睛、树叶等。</li>
</ul>
<h2 id="3-可视化类激活的热力图"><a href="#3-可视化类激活的热力图" class="headerlink" title="3. 可视化类激活的热力图"></a>3. 可视化类激活的热力图</h2><p>我还要介绍另一种可视化方法，它有助于了解一张图像的哪一部分让卷积神经网络做出了最终的分类决策。这有助于对卷积神经网络的决策过程进行调试，特别是出现分类错误的情况下。这种方法还可以定位图像中的特定目标。</p>
<p>这种通用的技术叫作类激活图（CAM，class activation map）可视化，它是指对输入图像生成类激活的热力图。类激活热力图是与特定输出类别相关的二维分数网格，对任何输入图像的每个位置都要进行计算，它表示每个位置对该类别的重要程度。举例来说，对于输入到猫狗分类卷积神经网络的一张图像，CAM 可视化可以生成类别“猫”的热力图，表示图像的各个部分与“猫”的相似程度，CAM 可视化也会生成类别“狗”的热力图，表示图像的各个部分与“狗”的相似程度。</p>
<p>我们将使用的具体实现方式是“Grad-CAM: visual explanations from deep networks via gradientbasedlocalization”a 这篇论文中描述的方法。这种方法非常简单：给定一张输入图像，对于一个卷积层的输出特征图，用类别相对于通道的梯度对这个特征图中的每个通道进行加权。直观上来看，理解这个技巧的一种方法是，你是用“每个通道对类别的重要程度”对“输入图像对不同通道的激活强度”的空间图进行加权，从而得到了“输入图像对类别的激活强度”的空间图。</p>
<p>我们再次使用预训练的<code>VGG16</code>网络来演示此方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载带有预训练权重的VGG16网络</span></span><br><span class="line"><span class="keyword">from</span> keras.applications.vgg16 <span class="keyword">import</span> VGG16</span><br><span class="line">model = VGG16(weights=<span class="string">'imagenet'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5
553467904/553467096 [==============================] - 441s 1us/step
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为VGG16模型预处理一张输入图像</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> keras.applications.vgg16 <span class="keyword">import</span> preprocess_input, decode_predictions</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img_path = <span class="string">'data/pic_input/elephant1.jpg'</span></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">x = image.img_to_array(img)</span><br><span class="line">x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line">x = preprocess_input(x)</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\elephant1.jpg" width="200" height="200" alt="非洲象" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">非洲象</div>
</center>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">preds = model.predict(x)</span><br><span class="line">print(<span class="string">'Predicted:'</span>, decode_predictions(preds, top=<span class="number">3</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json
40960/35363 [==================================] - 0s 3us/step
Predicted: [(&#39;n02504458&#39;, &#39;African_elephant&#39;, 0.87728226), (&#39;n01871265&#39;, &#39;tusker&#39;, 0.11725453), (&#39;n02504013&#39;, &#39;Indian_elephant&#39;, 0.0054599163)]
</code></pre><p>对这张图像预测的前三个类别分别为：</p>
<ul>
<li>非洲象（African elephant，87.728226% 的概率）</li>
<li>长牙动物（tusker，11.725453% 的概率）</li>
<li>印度象（Indian elephant，0.54599163%的概率）</li>
</ul>
<p>网络识别出图像中包含数量不确定的非洲象。预测向量中被最大激活的元素是对应“非洲象”类别的元素，索引编号为386。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.argmax(preds[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 386</span></span><br></pre></td></tr></table></figure>
<p>为了展示图像中哪些部分最像非洲象，我们来使用<code>Grad-CAM</code>算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">african_elephant_output = model.output[:, <span class="number">386</span>]</span><br><span class="line">last_conv_layer = model.get_layer(<span class="string">'block5_conv3'</span>)</span><br><span class="line">grads = K.gradients(african_elephant_output, last_conv_layer.output)[<span class="number">0</span>]</span><br><span class="line">pooled_grads = K.mean(grads, axis=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">iterate = K.function([model.input],[pooled_grads, last_conv_layer.output[<span class="number">0</span>]])</span><br><span class="line">pooled_grads_value, conv_layer_output_value = iterate([x])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">512</span>):</span><br><span class="line">    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]</span><br><span class="line">heatmap = np.mean(conv_layer_output_value, axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<p>为了便于可视化，我们还需要将热力图标准化到<code>0~1</code>范围内。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">heatmap = np.maximum(heatmap, <span class="number">0</span>)</span><br><span class="line">heatmap /= np.max(heatmap)</span><br><span class="line">plt.matshow(heatmap)</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_120_1.png" width="400" height="400" alt="b" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>



<p>最后，我们可以用OpenCV 来生成一张图像，将原始图像叠加在刚刚得到的热力图上</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">img = cv2.imread(img_path)</span><br><span class="line">heatmap = cv2.resize(heatmap, (img.shape[<span class="number">1</span>], img.shape[<span class="number">0</span>]))</span><br><span class="line">heatmap = np.uint8(<span class="number">255</span> * heatmap)</span><br><span class="line">heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)</span><br><span class="line">superimposed_img = heatmap * <span class="number">0.4</span> + img</span><br><span class="line">cv2.imwrite(<span class="string">'data/pic_output/elephant_cam.jpg'</span>, superimposed_img)</span><br><span class="line">    <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv2.imshow(<span class="string">'合并后的图像'</span>, superimposed_img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>) </span><br><span class="line">    <span class="comment"># -1</span></span><br></pre></td></tr></table></figure>
<h1 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h1><ul>
<li>卷积神经网络是解决视觉分类问题的最佳工具。</li>
<li>卷积神经网络通过学习模块化模式和概念的层次结构来表示视觉世界。</li>
<li>卷积神经网络学到的表示很容易可视化，卷积神经网络不是黑盒。</li>
<li>现在你能够从头开始训练自己的卷积神经网络来解决图像分类问题。</li>
<li>你知道了如何使用视觉数据增强来防止过拟合。</li>
<li>你知道了如何使用预训练的卷积神经网络进行特征提取与模型微调。</li>
<li>你可以将卷积神经网络学到的过滤器可视化，也可以将类激活热力图可视化。</li>
</ul>
]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
  </entry>
  <entry>
    <title>Python深度学习（一）神经网络入门</title>
    <url>/posts/d8ee3b71.html</url>
    <content><![CDATA[<h1 id="一、电影评论分类：二分类问题"><a href="#一、电影评论分类：二分类问题" class="headerlink" title="一、电影评论分类：二分类问题"></a>一、电影评论分类：二分类问题</h1><p>二分类问题可能是应用最广泛的机器学习问题。在这个例子中，你将学习根据电影评论的文字内容将其划分为正面或负面。</p>
<h2 id="1-IMDB-数据集"><a href="#1-IMDB-数据集" class="headerlink" title="1. IMDB 数据集"></a>1. IMDB 数据集</h2><p>本节使用IMDB 数据集，它包含来自互联网电影数据库（IMDB）的50 000条严重两极分化的评论。数据集被分为用于训练的25 000 条评论与用于测试的25 000 条评论，训练集和测试集都包含50% 的正面评论和50% 的负面评论。</p>
<p>为什么要将训练集和测试集分开？因为你不应该将训练机器学习模型的同一批数据再用于测试模型！模型在训练数据上的表现很好，并不意味着它在前所未见的数据上也会表现得很好，而且你真正关心的是模型在新数据上的性能（因为你已经知道了训练数据对应的标签，显然不再需要模型来进行预测）。例如，你的模型最终可能只是记住了训练样本和目标值之间的映射关系，但这对在前所未见的数据上进行预测毫无用处。后面将会更详细地讨论这一点。</p>
<p>与MNIST 数据集一样，IMDB 数据集也内置于Keras 库。它已经过预处理：评论（单词序列）已经被转换为整数序列，其中每个整数代表字典中的某个单词。</p>
<p>下列代码将会加载IMDB 数据集（第一次运行时会下载大约80MB 的数据）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Using TensorFlow backend.
</code></pre><p>参数 <code>num_words=10000</code> 的意思是仅保留训练数据中前10 000个最常出现的单词。低频单词将被舍弃。这样得到的向量数据不会太大，便于处理。</p>
<p><code>train_data</code> 和 <code>test_data</code> 这两个变量都是评论组成的列表，每条评论又是单词索引组成的列表（表示一系列单词）。<code>train_labels</code> 和 <code>test_labels</code> 都是0 和1 组成的列表，其中0代表负面（negative），1 代表正面（positive）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'train data'</span>, train_data[<span class="number">0</span>], <span class="string">'\ntrain label'</span>, train_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>train data [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] 
train label 1
</code></pre><p>由于限定为前10 000 个最常见的单词，单词索引都不会超过10 000。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max([max(sequence) <span class="keyword">for</span> sequence <span class="keyword">in</span> train_data])</span><br></pre></td></tr></table></figure>
<pre><code>9999
</code></pre><p>下面这段代码很有意思，你可以将某条评论迅速解码为英文单词。<br>注：</p>
<ol>
<li>索引减去了3，因为0、1、2是为“padding”（填充）、“start of sequence”（序列开始）、“unknown”（未知词）分别保留的索引</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_index = imdb.get_word_index()</span><br><span class="line">print(<span class="string">'word index of fawn: '</span>, word_index[<span class="string">'fawn'</span>])</span><br><span class="line">reverse_word_index = dict([(value, key) <span class="keyword">for</span> (key, value) <span class="keyword">in</span> word_index.items()])</span><br><span class="line">print(<span class="string">'the word with index 0, 1, 2 are: '</span>, reverse_word_index[<span class="number">1</span>], reverse_word_index[<span class="number">2</span>], reverse_word_index[<span class="number">3</span>])</span><br><span class="line">decoded_review = <span class="string">' '</span>.join(reverse_word_index.get(i<span class="number">-3</span>,<span class="string">'?'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> train_data[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'decoded review of the first train data: '</span>, decoded_review)</span><br></pre></td></tr></table></figure>
<pre><code>word index of fawn:  34701
the word with index 0, 1, 2 are:  the and a
decoded review of the first train data:  ? this film was just brilliant casting location scenery story direction everyone&#39;s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy&#39;s that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&#39;t you think the whole story was so lovely because it was true and was someone&#39;s life after all that was shared with us all
</code></pre><h2 id="2-准备数据"><a href="#2-准备数据" class="headerlink" title="2. 准备数据"></a>2. 准备数据</h2><p>你不能将整数序列直接输入神经网络。你需要将列表转换为张量。转换方法有以下两种：</p>
<ul>
<li>填充列表，使其具有相同的长度，再将列表转换成形状为 (samples, word_indices)的整数张量，然后网络第一层使用能处理这种整数张量的层（即Embedding层）。</li>
<li>对列表进行 one-hot 编码，将其转换为 0 和 1 组成的向量。举个例子，序列[3, 5]将会被转换为10 000 维向量，只有索引为3 和5 的元素是1，其余元素都是0。然后网络第一层可以用Dense 层，它能够处理浮点数向量数据。</li>
</ul>
<p>下面我们采用后一种方法将数据向量化。为了加深理解，我们可以手动实现这一方法，如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将整数序列编码为二进制矩阵</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br></pre></td></tr></table></figure>
<p>样本现在变成了这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x_train[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[0. 1. 1. ... 0. 0. 0.]
</code></pre><p>你还应该将标签向量化，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_train = np.asarray(train_labels).astype(<span class="string">'float32'</span>)</span><br><span class="line">y_test = np.asarray(test_labels).astype(<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure>
<p>现在可以将数据输入到神经网络中。</p>
<h2 id="3-构建网络"><a href="#3-构建网络" class="headerlink" title="3. 构建网络"></a>3. 构建网络</h2><p>输入数据是向量，而标签是标量（1 和0），这是你会遇到的最简单的情况。有一类网络在这种问题上表现很好，就是带有<code>relu</code>激活的全连接层（Dense）的简单堆叠，比如<code>Dense(16, activation=&#39;relu&#39;)</code>。</p>
<p>传入<code>Dense</code>层的参数（16）是该层隐藏单元的个数。一个隐藏单元（hidden unit）是该层表示空间的一个维度。每个带有<code>relu</code>激活的<code>Dense</code>层都实现了下列张量运算：<code>output = relu(dot(W, input) + b)</code></p>
<p>16 个隐藏单元对应的权重矩阵<code>W</code>的形状为(input_dimension, 16)，与<code>W</code>做点积相当于将输入数据投影到16 维表示空间中（然后再加上偏置向量<code>b</code>并应用relu 运算）。你可以将表示空间的维度直观地理解为“网络学习内部表示时所拥有的自由度”。隐藏单元越多（即更高维的表示空间），网络越能够学到更加复杂的表示，但网络的计算代价也变得更大，而且可能会导致学到不好的模式（这种模式会提高训练数据上的性能，但不会提高测试数据上的性能）。</p>
<p>对于这种Dense 层的堆叠，你需要确定以下两个关键架构：</p>
<ul>
<li>网络有多少层；</li>
<li>每层有多少个隐藏单元。</li>
</ul>
<p>现在只需要选择下列架构：</p>
<ul>
<li>两个中间层，每层都有 16 个隐藏单元；</li>
<li>第三层输出一个标量，预测当前评论的情感。<br>中间层使用 relu 作为激活函数，最后一层使用 sigmoid 激活以输出一个0~1 范围内的概率值（表示样本的目标值等于1的可能性，即评论为正面的可能性）。 relu（rectified linear unit，整流线性单元）函数将所有负值归零，而sigmoid函数则将任意值“压缩”到<code>[0,1]</code>区间内，其输出值可以看作概率值。</li>
</ul>
<center>
    <img src="\Pic\DeepLearning_Pic\relu1.png" width="400" height="400" alt="整体线性流函数" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">整体线性流函数</div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\sigmoid1.png" width="400" height="400" alt="sigmoid函数" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">sigmoid函数</div>
</center>

<p>下图显示了网络的结构：</p>
<center>
    <img src="\Pic\DeepLearning_Pic\structure_imdb_1.png" width="200" height="200" alt="structure_imdb_1" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">三层网络结构</div>
</center>

<h2 id="4-模型定义"><a href="#4-模型定义" class="headerlink" title="4. 模型定义"></a>4. 模型定义</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">"relu"</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">"sigmoid"</span>))</span><br></pre></td></tr></table></figure>
<p>最后，你需要选择损失函数和优化器。由于你面对的是一个二分类问题，网络输出是一个概率值（网络最后一层使用<code>sigmoid</code>激活函数，仅包含一个单元），那么最好使用<code>binary_crossentropy</code>（二元交叉熵）损失。这并不是唯一可行的选择，比如你还可以使用<code>mean_squared_error</code>（均方误差）。但对于输出概率值的模型，交叉熵（crossentropy）往往是最好的选择。交叉熵是来自于信息论领域的概念，用于衡量概率分布之间的距离，在这个例子中就<br>是真实分布与预测值之间的距离。</p>
<p>下面的步骤是用<code>rmsprop</code>优化器和<code>binary_crossentropy</code>损失函数来配置模型。注意，我们还在训练过程中监控精度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">"rmsprop"</span>, loss=<span class="string">"binary_crossentropy"</span>, metrics=[<span class="string">"accuracy"</span>])</span><br></pre></td></tr></table></figure>
<p>上述代码将优化器、损失函数和指标作为字符串传入，这是因为<code>rmsprop</code>、<code>binary_crossentropy</code>和<code>accuracy</code>都是Keras 内置的一部分。有时你可能希望配置自定义优化器的参数，或者传入自定义的损失函数或指标函数，前者可通过向<code>optimizer</code>参数传入一个优化器类实例来实现</p>
<h2 id="5-划分数据并训练模型"><a href="#5-划分数据并训练模型" class="headerlink" title="5. 划分数据并训练模型"></a>5. 划分数据并训练模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_val = x_train[:<span class="number">10000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">10000</span>:]</span><br><span class="line">y_val = y_train[:<span class="number">10000</span>]</span><br><span class="line">partial_y_train = y_train[<span class="number">10000</span>:]</span><br></pre></td></tr></table></figure>
<p>现在使用512 个样本组成的小批量，将模型训练20 个轮次（即对<code>x_train</code>和<code>y_train</code>两个张量中的所有样本进行20 次迭代）。与此同时，你还要监控在留出的10 000 个样本上的损失和精度。你可以通过将验证数据传入<code>validation_data</code>参数来完成。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">                    partial_y_train,</span><br><span class="line">                    epochs=<span class="number">20</span>,</span><br><span class="line">                    batch_size=<span class="number">512</span>,</span><br><span class="line">                    validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<pre><code>Train on 15000 samples, validate on 10000 samples
Epoch 1/20
15000/15000 [==============================] - 10s 637us/step - loss: 0.5054 - accuracy: 0.7994 - val_loss: 0.3811 - val_accuracy: 0.8697
Epoch 2/20
15000/15000 [==============================] - 6s 402us/step - loss: 0.3009 - accuracy: 0.9039 - val_loss: 0.3074 - val_accuracy: 0.8841
Epoch 3/20
15000/15000 [==============================] - 2s 151us/step - loss: 0.2207 - accuracy: 0.9309 - val_loss: 0.2773 - val_accuracy: 0.8910
Epoch 4/20
15000/15000 [==============================] - 2s 137us/step - loss: 0.1777 - accuracy: 0.9433 - val_loss: 0.2731 - val_accuracy: 0.8912
Epoch 5/20
15000/15000 [==============================] - 2s 129us/step - loss: 0.1428 - accuracy: 0.9555 - val_loss: 0.2797 - val_accuracy: 0.8888
Epoch 6/20
15000/15000 [==============================] - 2s 137us/step - loss: 0.1201 - accuracy: 0.9629 - val_loss: 0.3101 - val_accuracy: 0.8818
Epoch 7/20
15000/15000 [==============================] - 2s 134us/step - loss: 0.1007 - accuracy: 0.9705 - val_loss: 0.3096 - val_accuracy: 0.8817
Epoch 8/20
15000/15000 [==============================] - 2s 129us/step - loss: 0.0845 - accuracy: 0.9768 - val_loss: 0.3309 - val_accuracy: 0.8826
Epoch 9/20
15000/15000 [==============================] - 2s 126us/step - loss: 0.0715 - accuracy: 0.9809 - val_loss: 0.3477 - val_accuracy: 0.8817
Epoch 10/20
15000/15000 [==============================] - 2s 127us/step - loss: 0.0604 - accuracy: 0.9848 - val_loss: 0.3675 - val_accuracy: 0.8788
Epoch 11/20
15000/15000 [==============================] - 2s 125us/step - loss: 0.0504 - accuracy: 0.9879 - val_loss: 0.4077 - val_accuracy: 0.8692
Epoch 12/20
15000/15000 [==============================] - 2s 125us/step - loss: 0.0410 - accuracy: 0.9908 - val_loss: 0.4214 - val_accuracy: 0.8725
Epoch 13/20
15000/15000 [==============================] - 2s 125us/step - loss: 0.0322 - accuracy: 0.9947 - val_loss: 0.4783 - val_accuracy: 0.8704
Epoch 14/20
15000/15000 [==============================] - 2s 125us/step - loss: 0.0291 - accuracy: 0.9939 - val_loss: 0.4782 - val_accuracy: 0.8741
Epoch 15/20
15000/15000 [==============================] - 2s 125us/step - loss: 0.0222 - accuracy: 0.9962 - val_loss: 0.5157 - val_accuracy: 0.8679
Epoch 16/20
15000/15000 [==============================] - 2s 126us/step - loss: 0.0188 - accuracy: 0.9966 - val_loss: 0.5413 - val_accuracy: 0.8714
Epoch 17/20
15000/15000 [==============================] - 2s 125us/step - loss: 0.0120 - accuracy: 0.9990 - val_loss: 0.5762 - val_accuracy: 0.8659
Epoch 18/20
15000/15000 [==============================] - 2s 129us/step - loss: 0.0129 - accuracy: 0.9979 - val_loss: 0.6005 - val_accuracy: 0.8674
Epoch 19/20
15000/15000 [==============================] - 2s 125us/step - loss: 0.0085 - accuracy: 0.9993 - val_loss: 0.6336 - val_accuracy: 0.8689
Epoch 20/20
15000/15000 [==============================] - 2s 124us/step - loss: 0.0070 - accuracy: 0.9993 - val_loss: 0.6647 - val_accuracy: 0.8675
</code></pre><p>在CPU 上运行，每轮的时间不到2秒，训练过程将在20 秒内结束。每轮结束时会有短暂的停顿，因为模型要计算在验证集的10 000 个样本上的损失和精度。</p>
<p>注意，调用<code>model.fit()</code>返回了一个<code>history</code>对象。这个对象有一个成员<code>history</code>，它是一个字典，包含训练过程中的所有数据，我们来看一下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history_dict = history.history</span><br><span class="line">history_dict.keys()</span><br></pre></td></tr></table></figure>
<pre><code>dict_keys([&#39;val_loss&#39;, &#39;val_accuracy&#39;, &#39;loss&#39;, &#39;accuracy&#39;])
</code></pre><h2 id="6-绘制训练损失和验证损失"><a href="#6-绘制训练损失和验证损失" class="headerlink" title="6. 绘制训练损失和验证损失"></a>6. 绘制训练损失和验证损失</h2><p>字典中包含4 个条目，对应训练过程和验证过程中监控的指标。我们将使用Matplotlib 在同一张图上绘制训练损失和验证损失，以及训练精度和验证精度。请注意，由于网络的随机初始化不同，你得到的结果可能会略有不同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">history_dict = history.history</span><br><span class="line">loss_values = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">val_loss_values = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss_values)+<span class="number">1</span>)</span><br><span class="line">plt.plot(epochs, loss_values, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss_values, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_26_0.png" width="400" height="400" alt="训练损失和验证损失" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练损失和验证损失</div>
</center>


<h2 id="7-绘制训练精度和验证精度"><a href="#7-绘制训练精度和验证精度" class="headerlink" title="7. 绘制训练精度和验证精度"></a>7. 绘制训练精度和验证精度</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.clf()</span><br><span class="line">acc = history_dict[<span class="string">'accuracy'</span>]</span><br><span class="line">val_acc = history_dict[<span class="string">'val_accuracy'</span>]</span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training accuracy'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation accuracy'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_28_0.png" width="400" height="400" alt="训练精度和验证精度" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练精度和验证精度</div>
</center>


<p>如你所见，训练损失每轮都在降低，训练精度每轮都在提升。这就是梯度下降优化的预期结果——你想要最小化的量随着每次迭代越来越小。但验证损失和验证精度并非如此：它们似乎在第四轮达到最佳值。这就是我们之前警告过的一种情况：模型在训练数据上的表现越来越好，但在前所未见的数据上不一定表现得越来越好。准确地说，你看到的是过拟合（overfit）：在第二轮之后，你对训练数据过度优化，最终学到的表示仅针对于训练数据，无法泛化到训练集之外的数据。</p>
<p>在这种情况下，为了防止过拟合，你可以在3 轮之后停止训练。通常来说，你可以使用许多方法来降低过拟合，我们将在后面详细介绍。</p>
<p>我们从头开始训练一个新的网络，训练4 轮，然后在测试数据上评估模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">4</span>, batch_size=<span class="number">512</span>)</span><br><span class="line">results = model.evaluate(x_test, y_test)</span><br><span class="line">print(results)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/4
25000/25000 [==============================] - 3s 100us/step - loss: 0.4556 - accuracy: 0.8245
Epoch 2/4
25000/25000 [==============================] - 2s 78us/step - loss: 0.2628 - accuracy: 0.9074
Epoch 3/4
25000/25000 [==============================] - 2s 81us/step - loss: 0.2002 - accuracy: 0.9290
Epoch 4/4
25000/25000 [==============================] - 2s 88us/step - loss: 0.1687 - accuracy: 0.9394
25000/25000 [==============================] - 7s 270us/step
[0.30153122137069704, 0.880840003490448]
</code></pre><p>这种相当简单的方法得到了88% 的精度。利用最先进的方法，你应该能够得到接近95% 的精度。</p>
<h2 id="8-使用训练好的网络在新数据上生成预测结果"><a href="#8-使用训练好的网络在新数据上生成预测结果" class="headerlink" title="8. 使用训练好的网络在新数据上生成预测结果"></a>8. 使用训练好的网络在新数据上生成预测结果</h2><p>训练好网络之后，你希望将其用于实践。你可以用<code>predict</code>方法来得到评论为正面的可能性大小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.predict(x_test)</span><br></pre></td></tr></table></figure>
<pre><code>array([[0.17173755],
       [0.99980915],
       [0.77910084],
       ...,
       [0.08216667],
       [0.04566944],
       [0.51742095]], dtype=float32)
</code></pre><h1 id="二、新闻分类：多分类问题"><a href="#二、新闻分类：多分类问题" class="headerlink" title="二、新闻分类：多分类问题"></a>二、新闻分类：多分类问题</h1><p>下面你会构建一个网络，将路透社新闻划分为46 个互斥的主题。因为有多个类别，所以这是多分类（multiclass classification）问题的一个例子。因为每个数据点只能划分到一个类别，所以更具体地说，这是单标签、多分类（single-label, multiclass classification）问题的一个例子。如果每个数据点可以划分到多个类别（主题），那它就是一个多标签、多分类（multilabel,multiclass classification）问题。</p>
<h2 id="1-路透社数据集"><a href="#1-路透社数据集" class="headerlink" title="1. 路透社数据集"></a>1. 路透社数据集</h2><p>本节使用路透社数据集，它包含许多短新闻及其对应的主题，由路透社在1986 年发布。它是一个简单的、广泛使用的文本分类数据集。它包括46 个不同的主题：某些主题的样本更多，但训练集中每个主题都有至少10 个样本。与IMDB 和MNIST 类似，路透社数据集也内置为Keras 的一部分。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> reuters</span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=<span class="number">10000</span>)</span><br><span class="line">print(<span class="string">'the length of train data: '</span>, len(train_data), <span class="string">'\nthe length of test data: '</span>, len(test_data))</span><br><span class="line">print(<span class="string">'train data 10: '</span>, train_data[<span class="number">10</span>], <span class="string">'\ntrain label 10: '</span>, train_labels[<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<pre><code>the length of train data:  8982 
the length of test data:  2246
train data 10:  [1, 245, 273, 207, 156, 53, 74, 160, 26, 14, 46, 296, 26, 39, 74, 2979, 3554, 14, 46, 4689, 4329, 86, 61, 3499, 4795, 14, 61, 451, 4329, 17, 12] 
train label 10:  3
</code></pre><p>如果好奇的话，你可以用下列代码将索引解码为单词。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_index = reuters.get_word_index()</span><br><span class="line">reverse_word_index = dict([(value, key) <span class="keyword">for</span> (key, value) <span class="keyword">in</span> word_index.items()])</span><br><span class="line">decoded_newswire = <span class="string">' '</span>.join([reverse_word_index.get(i - <span class="number">3</span>, <span class="string">'?'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> train_data[<span class="number">0</span>]])</span><br><span class="line">print(decoded_newswire)</span><br></pre></td></tr></table></figure>
<pre><code>? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3
</code></pre><h2 id="2-准备数据-1"><a href="#2-准备数据-1" class="headerlink" title="2. 准备数据"></a>2. 准备数据</h2><p>你可以使用与上一个例子相同的代码将数据向量化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br></pre></td></tr></table></figure>
<p>将标签向量化有两种方法：你可以将标签列表转换为整数张量，或者使用one-hot 编码。</p>
<p>one-hot 编码是分类数据广泛使用的一种格式，也叫分类编码（categorical encoding）,在这个例子中，标签的one-hot 编码就是将每个标签表示为全零向量，只有标签索引对应的元素为1。其代码实现如下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_one_hot</span><span class="params">(labels, dimension=<span class="number">46</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(labels), dimension))</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">        results[i, label] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line">one_hot_train_labels = to_one_hot(train_labels)</span><br><span class="line">one_hot_test_labels = to_one_hot(test_labels)</span><br></pre></td></tr></table></figure>
<p>注意，Keras 内置方法可以实现这个操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.utils.np_utils <span class="keyword">import</span> to_categorical</span><br><span class="line">one_hot_train_labels = to_categorical(train_labels)</span><br><span class="line">one_hot_test_labels = to_categorical(test_labels)</span><br></pre></td></tr></table></figure>
<h2 id="3-构建网络-1"><a href="#3-构建网络-1" class="headerlink" title="3. 构建网络"></a>3. 构建网络</h2><p>这个主题分类问题与前面的电影评论分类问题类似，两个例子都是试图对简短的文本片段进行分类。但这个问题有一个新的约束条件：输出类别的数量从2 个变为46 个。输出空间的维度要大得多。</p>
<p>对于前面用过的Dense 层的堆叠，每层只能访问上一层输出的信息。如果某一层丢失了与分类问题相关的一些信息，那么这些信息无法被后面的层找回，也就是说，每一层都可能成为信息瓶颈。上一个例子使用了16 维的中间层，但对这个例子来说16 维空间可能太小了，无法学会区分46 个不同的类别。这种维度较小的层可能成为信息瓶颈，永久地丢失相关信息。</p>
<p>出于这个原因，下面将使用维度更大的层，包含64 个单元。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br></pre></td></tr></table></figure>
<p>关于这个架构还应该注意另外两点：</p>
<ul>
<li>网络的最后一层是大小为 46 的 <code>Dense</code> 层。这意味着，对于每个输入样本，网络都会输出一个46 维向量。这个向量的每个元素（即每个维度）代表不同的输出类别。</li>
<li>最后一层使用了 <code>softmax</code> 激活。你在 MNIST 例子中见过这种用法。网络将输出在 46个不同输出类别上的概率分布——对于每一个输入样本，网络都会输出一个46 维向量，其中<code>output[i]</code>是样本属于第<code>i</code>个类别的概率。46 个概率的总和为1。</li>
</ul>
<p>对于这个例子，最好的损失函数是<code>categorical_crossentropy</code>（分类交叉熵）。它用于衡量两个概率分布之间的距离，这里两个概率分布分别是网络输出的概率分布和标签的真实分布。通过将这两个分布的距离最小化，训练网络可使输出结果尽可能接近真实标签。</p>
<h2 id="4-编译模型"><a href="#4-编译模型" class="headerlink" title="4. 编译模型"></a>4. 编译模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">            loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<h2 id="5-验证你的方法"><a href="#5-验证你的方法" class="headerlink" title="5. 验证你的方法"></a>5. 验证你的方法</h2><p>我们在训练数据中留出1000 个样本作为验证集。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_val = x_train[:<span class="number">1000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">1000</span>:]</span><br><span class="line">y_val = one_hot_train_labels[:<span class="number">1000</span>]</span><br><span class="line">partial_y_train = one_hot_train_labels[<span class="number">1000</span>:]</span><br></pre></td></tr></table></figure>
<p>现在开始训练网络，共20 个轮次。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">                    partial_y_train,</span><br><span class="line">                    epochs=<span class="number">20</span>,</span><br><span class="line">                    batch_size=<span class="number">512</span>,</span><br><span class="line">                    validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<pre><code>Train on 7982 samples, validate on 1000 samples
Epoch 1/20
7982/7982 [==============================] - 2s 197us/step - loss: 2.7220 - accuracy: 0.5438 - val_loss: 1.7716 - val_accuracy: 0.6510
Epoch 2/20
7982/7982 [==============================] - 1s 90us/step - loss: 1.4464 - accuracy: 0.7068 - val_loss: 1.3297 - val_accuracy: 0.7160
Epoch 3/20
7982/7982 [==============================] - 1s 90us/step - loss: 1.0637 - accuracy: 0.7737 - val_loss: 1.1569 - val_accuracy: 0.7520
Epoch 4/20
7982/7982 [==============================] - 1s 92us/step - loss: 0.8311 - accuracy: 0.8247 - val_loss: 1.0649 - val_accuracy: 0.7690
Epoch 5/20
7982/7982 [==============================] - 1s 90us/step - loss: 0.6618 - accuracy: 0.8629 - val_loss: 0.9799 - val_accuracy: 0.7900
Epoch 6/20
7982/7982 [==============================] - 1s 90us/step - loss: 0.5263 - accuracy: 0.8905 - val_loss: 0.9437 - val_accuracy: 0.7910
Epoch 7/20
7982/7982 [==============================] - 1s 91us/step - loss: 0.4242 - accuracy: 0.9118 - val_loss: 0.9042 - val_accuracy: 0.8170
Epoch 8/20
7982/7982 [==============================] - 1s 92us/step - loss: 0.3445 - accuracy: 0.9295 - val_loss: 0.8994 - val_accuracy: 0.8200
Epoch 9/20
7982/7982 [==============================] - 1s 90us/step - loss: 0.2905 - accuracy: 0.9365 - val_loss: 0.9082 - val_accuracy: 0.8180
Epoch 10/20
7982/7982 [==============================] - 1s 89us/step - loss: 0.2414 - accuracy: 0.9437 - val_loss: 0.9217 - val_accuracy: 0.8090
Epoch 11/20
7982/7982 [==============================] - 1s 91us/step - loss: 0.2108 - accuracy: 0.9474 - val_loss: 0.9495 - val_accuracy: 0.8210
Epoch 12/20
7982/7982 [==============================] - 1s 91us/step - loss: 0.1879 - accuracy: 0.9530 - val_loss: 0.9722 - val_accuracy: 0.7960
Epoch 13/20
7982/7982 [==============================] - 1s 89us/step - loss: 0.1656 - accuracy: 0.9526 - val_loss: 1.0056 - val_accuracy: 0.7950
Epoch 14/20
7982/7982 [==============================] - 1s 91us/step - loss: 0.1504 - accuracy: 0.9545 - val_loss: 0.9687 - val_accuracy: 0.8090
Epoch 15/20
7982/7982 [==============================] - 1s 88us/step - loss: 0.1433 - accuracy: 0.9563 - val_loss: 1.0085 - val_accuracy: 0.8090
Epoch 16/20
7982/7982 [==============================] - 1s 90us/step - loss: 0.1312 - accuracy: 0.9551 - val_loss: 1.0123 - val_accuracy: 0.8040
Epoch 17/20
7982/7982 [==============================] - 1s 93us/step - loss: 0.1249 - accuracy: 0.9577 - val_loss: 1.0555 - val_accuracy: 0.8010
Epoch 18/20
7982/7982 [==============================] - 1s 90us/step - loss: 0.1226 - accuracy: 0.9554 - val_loss: 1.0423 - val_accuracy: 0.8000
Epoch 19/20
7982/7982 [==============================] - 1s 91us/step - loss: 0.1163 - accuracy: 0.9572 - val_loss: 1.0554 - val_accuracy: 0.8000
Epoch 20/20
7982/7982 [==============================] - 1s 113us/step - loss: 0.1086 - accuracy: 0.9588 - val_loss: 1.1368 - val_accuracy: 0.7790
</code></pre><h2 id="6-绘制图像"><a href="#6-绘制图像" class="headerlink" title="6. 绘制图像"></a>6. 绘制图像</h2><p>最后，我们来绘制损失曲线和精度曲线</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_53_0.png" width="400" height="400" alt="训练损失和验证损失" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练损失和验证损失</div>
</center>



<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.clf()</span><br><span class="line">acc = history.history[<span class="string">'accuracy'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_accuracy'</span>]</span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training accuracy'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation accuracy'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_54_0.png" width="400" height="400" alt="训练精度和验证精度" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练精度和验证精度</div>
</center>


<p>网络在训练9轮后开始过拟合。我们从头开始训练一个新网络，共9个轮次，然后在测试集上评估模型。</p>
<h2 id="7-从头开始重新训练一个模型"><a href="#7-从头开始重新训练一个模型" class="headerlink" title="7. 从头开始重新训练一个模型"></a>7. 从头开始重新训练一个模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">            loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(partial_x_train,</span><br><span class="line">        partial_y_train,</span><br><span class="line">        epochs=<span class="number">9</span>,</span><br><span class="line">        batch_size=<span class="number">512</span>,</span><br><span class="line">        validation_data=(x_val, y_val))</span><br><span class="line">results = model.evaluate(x_test, one_hot_test_labels)</span><br></pre></td></tr></table></figure>
<pre><code>Train on 7982 samples, validate on 1000 samples
Epoch 1/9
7982/7982 [==============================] - 1s 116us/step - loss: 2.7007 - accuracy: 0.5041 - val_loss: 1.7970 - val_accuracy: 0.6230
Epoch 2/9
7982/7982 [==============================] - 1s 103us/step - loss: 1.4223 - accuracy: 0.7050 - val_loss: 1.3018 - val_accuracy: 0.7210
Epoch 3/9
7982/7982 [==============================] - 1s 106us/step - loss: 1.0310 - accuracy: 0.7791 - val_loss: 1.1309 - val_accuracy: 0.7470
Epoch 4/9
7982/7982 [==============================] - 1s 106us/step - loss: 0.8089 - accuracy: 0.8242 - val_loss: 1.0300 - val_accuracy: 0.7800
Epoch 5/9
7982/7982 [==============================] - 1s 124us/step - loss: 0.6437 - accuracy: 0.8629 - val_loss: 0.9679 - val_accuracy: 0.7970
Epoch 6/9
7982/7982 [==============================] - 1s 116us/step - loss: 0.5177 - accuracy: 0.8931 - val_loss: 0.9502 - val_accuracy: 0.8040
Epoch 7/9
7982/7982 [==============================] - 1s 125us/step - loss: 0.4128 - accuracy: 0.9143 - val_loss: 0.9156 - val_accuracy: 0.8060
Epoch 8/9
7982/7982 [==============================] - 1s 123us/step - loss: 0.3394 - accuracy: 0.9267 - val_loss: 0.9121 - val_accuracy: 0.8110
Epoch 9/9
7982/7982 [==============================] - 1s 136us/step - loss: 0.2785 - accuracy: 0.9386 - val_loss: 0.8915 - val_accuracy: 0.8110
2246/2246 [==============================] - 0s 156us/step
</code></pre><p>这种方法可以得到约80% 的精度。对于平衡的二分类问题，完全随机的分类器能够得到50% 的精度。但在这个例子中，完全随机的精度约为19%，所以上述结果相当不错，至少和随机的基准比起来还不错。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">test_labels_copy = copy.copy(test_labels)</span><br><span class="line">np.random.shuffle(test_labels_copy)</span><br><span class="line">hits_array = np.array(test_labels) == np.array(test_labels_copy)</span><br><span class="line">float(np.sum(hits_array)) / len(test_labels)</span><br></pre></td></tr></table></figure>
<pre><code>0.18655387355298308
</code></pre><h2 id="8-在新数据上生成预测结果"><a href="#8-在新数据上生成预测结果" class="headerlink" title="8. 在新数据上生成预测结果"></a>8. 在新数据上生成预测结果</h2><p>你可以验证，模型实例的<code>predict</code>方法返回了在46 个主题上的概率分布。我们对所有测试数据生成主题预测。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions = model.predict(x_test)</span><br></pre></td></tr></table></figure>
<p>predictions 中的每个元素都是长度为46 的向量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions[<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>
<pre><code>(46,)
</code></pre><p>最大的元素就是预测类别，即概率最大的类别。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.argmax(predictions[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>3
</code></pre><h2 id="9-处理标签和损失的另一种方法"><a href="#9-处理标签和损失的另一种方法" class="headerlink" title="9. 处理标签和损失的另一种方法"></a>9. 处理标签和损失的另一种方法</h2><p>前面提到了另一种编码标签的方法，就是将其转换为整数张量，如下所示。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_train = np.array(train_labels)</span><br><span class="line">y_test = np.array(test_labels)</span><br></pre></td></tr></table></figure><br>对于这种编码方法，唯一需要改变的是损失函数的选择。对于代码清单3-21 使用的损失函数<code>categorical_crossentropy</code>，标签应该遵循分类编码。对于整数标签，你应该使用<code>sparse_categorical_crossentropy</code>。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">metrics=[<span class="string">'acc'</span>])</span><br></pre></td></tr></table></figure><br>这个新的损失函数在数学上与<code>categorical_crossentropy</code>完全相同，二者只是接口不同</p>
<h1 id="三、预测房价：回归问题"><a href="#三、预测房价：回归问题" class="headerlink" title="三、预测房价：回归问题"></a>三、预测房价：回归问题</h1><p>前面两个例子都是分类问题，其目标是预测输入数据点所对应的单一离散的标签。另一种常见的机器学习问题是回归问题，它预测一个连续值而不是离散的标签，例如，根据气象数据预测明天的气温，或者根据软件说明书预测完成软件项目所需要的时间。</p>
<h2 id="1-波士顿房价数据集"><a href="#1-波士顿房价数据集" class="headerlink" title="1. 波士顿房价数据集"></a>1. 波士顿房价数据集</h2><p>下面将要预测20 世纪70 年代中期波士顿郊区房屋价格的中位数，已知当时郊区的一些数据点，比如犯罪率、当地房产税率等。</p>
<p>该数据集包含的数据点相对较少，只有506 个，分为404 个训练样本和102 个测试样本。输入数据的每个特征（比如犯罪率）都有不同的取值范围。例如，有些特性是比例，取值范围为0~1；有的取值范围为1~12；还有的取值范围为0~100，等等。</p>
<h2 id="2-加载波士顿房价数据"><a href="#2-加载波士顿房价数据" class="headerlink" title="2. 加载波士顿房价数据"></a>2. 加载波士顿房价数据</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> boston_housing</span><br><span class="line">(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()</span><br><span class="line">print(train_data.shape, test_data.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(404, 13) (102, 13)
</code></pre><p>如你所见，我们有404 个训练样本和102 个测试样本，每个样本都有13 个数值特征，比如人均犯罪率、每个住宅的平均房间数、高速公路可达性等。</p>
<p>目标函数是房屋价格的中位数，单位是千美元。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_targets[:<span class="number">20</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,
       17.9, 23.1, 19.9, 15.7,  8.8, 50. , 22.5, 24.1, 27.5])
</code></pre><p>房价大都在10 000~50 000 美元。如果你觉得这很便宜，不要忘记当时是20 世纪70 年代中期，而且这些价格没有根据通货膨胀进行调整。</p>
<h2 id="3-准备数据"><a href="#3-准备数据" class="headerlink" title="3. 准备数据"></a>3. 准备数据</h2><p>将取值范围差异很大的数据输入到神经网络中，这是有问题的。网络可能会自动适应这种取值范围不同的数据，但学习肯定变得更加困难。对于这种数据，普遍采用的最佳实践是对每个特征做标准化，即对于输入数据的每个特征（输入数据矩阵中的列），减去特征平均值，再除以标准差，这样得到的特征平均值为0，标准差为1。用Numpy 可以很容易实现标准化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line">mean = train_data.mean(axis=<span class="number">0</span>)</span><br><span class="line">train_data -= mean</span><br><span class="line">std = train_data.std(axis=<span class="number">0</span>)</span><br><span class="line">train_data /= std</span><br><span class="line">test_data -= mean</span><br><span class="line">test_data /= std</span><br></pre></td></tr></table></figure>
<p>注意，用于测试数据标准化的均值和标准差都是在训练数据上计算得到的。在工作流程中，你不能使用在测试数据上计算得到的任何结果，即使是像数据标准化这么简单的事情也不行。</p>
<h2 id="4-构建网络"><a href="#4-构建网络" class="headerlink" title="4. 构建网络"></a>4. 构建网络</h2><p>由于样本数量很少，我们将使用一个非常小的网络，其中包含两个隐藏层，每层有64 个单元。一般来说，训练数据越少，过拟合会越严重，而较小的网络可以降低过拟合。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型定义</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = models.Sequential()</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(train_data.shape[<span class="number">1</span>],)))</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">    model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=<span class="string">'mse'</span>, metrics=[<span class="string">'mae'</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>网络的最后一层只有一个单元，<strong>没有激活</strong>，是一个线性层。这是标量回归（标量回归是预测单一连续值的回归）的典型设置。添加激活函数将会限制输出范围。例如，如果向最后一层添加sigmoid 激活函数，网络只能学会预测0~1 范围内的值。这里最后一层是纯线性的，所以网络可以学会预测任意范围内的值。</p>
<p>注意，编译网络用的是<code>mse</code>损失函数，即均方误差（MSE，mean squared error），预测值与目标值之差的平方。这是回归问题常用的损失函数。</p>
<p>在训练过程中还监控一个新指标：平均绝对误差（MAE，mean absolute error）。它是预测值与目标值之差的绝对值。比如，如果这个问题的MAE 等于0.5，就表示你预测的房价与实际价格平均相差500 美元。</p>
<h2 id="5-利用K折验证来验证你的方法"><a href="#5-利用K折验证来验证你的方法" class="headerlink" title="5. 利用K折验证来验证你的方法"></a>5. 利用K折验证来验证你的方法</h2><p>为了在调节网络参数（比如训练的轮数）的同时对网络进行评估，你可以将数据划分为训练集和验证集，正如前面例子中所做的那样。但由于数据点很少，验证集会非常小（比如大约100 个样本）。因此，验证分数可能会有很大波动，这取决于你所选择的验证集和训练集。也就是说，验证集的划分方式可能会造成验证分数上有很大的方差，这样就无法对模型进行可靠的评估。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\k-fold.png" width="400" height="400" alt="k-fold" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">k折验证</div>
</center>

<p>在这种情况下，最佳做法是使用<code>K</code>折交叉验证。这种方法将可用数据划分为<code>K</code>个分区（<code>K</code>通常取4 或5），实例化<code>K</code>个相同的模型，将每个模型在<code>K-1</code>个分区上训练，并在剩下的一个分区上进行评估。模型的验证分数等于<code>K</code>个验证分数的平均值。这种方法的代码实现很简单：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">num_val_samples = len(train_data) // k</span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line">all_scores = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    print(<span class="string">'processing fold #'</span>, i)</span><br><span class="line">    val_data = train_data[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line">    val_targets = train_targets[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line">    partial_train_data = np.concatenate([train_data[:i * num_val_samples],train_data[(i + <span class="number">1</span>) * num_val_samples:]],axis=<span class="number">0</span>)</span><br><span class="line">    partial_train_targets = np.concatenate([train_targets[:i * num_val_samples],train_targets[(i + <span class="number">1</span>) * num_val_samples:]],axis=<span class="number">0</span>)</span><br><span class="line">    model = build_model()</span><br><span class="line">    model.fit(partial_train_data, partial_train_targets,</span><br><span class="line">    epochs=num_epochs, batch_size=<span class="number">1</span>, verbose=<span class="number">0</span>)</span><br><span class="line">    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=<span class="number">0</span>)</span><br><span class="line">    all_scores.append(val_mae)</span><br></pre></td></tr></table></figure>
<pre><code>processing fold # 0
processing fold # 1
processing fold # 2
processing fold # 3
</code></pre><p>运行结果如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'所有得分：'</span>, all_scores, <span class="string">'\n平均值为'</span>, np.mean(all_scores))</span><br></pre></td></tr></table></figure>
<pre><code>所有得分： [2.0891380310058594, 2.6256139278411865, 2.7675087451934814, 2.588402271270752] 
平均值为 2.51766574382782
</code></pre><p>每次运行模型得到的验证分数有很大差异，从2.6 到3.2 不等。平均分数（3.0）是比单一分数更可靠的指标——这就是K 折交叉验证的关键。在这个例子中，预测的房价与实际价格平均相差3000 美元，考虑到实际价格范围在10 000~50 000 美元，这一差别还是很大的。</p>
<p>我们可以让训练时间更长一点，达到500 个轮次。为了记录模型在每轮的表现，我们需要修改训练循环，以保存每轮的验证分数记录。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs = <span class="number">500</span></span><br><span class="line">all_mae_histories = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    print(<span class="string">'processing fold #'</span>, i)</span><br><span class="line">    val_data = train_data[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line">    val_targets = train_targets[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line">    partial_train_data = np.concatenate([train_data[:i * num_val_samples],train_data[(i + <span class="number">1</span>) * num_val_samples:]],axis=<span class="number">0</span>)</span><br><span class="line">    partial_train_targets = np.concatenate([train_targets[:i * num_val_samples],train_targets[(i + <span class="number">1</span>) * num_val_samples:]],axis=<span class="number">0</span>)</span><br><span class="line">    model = build_model()</span><br><span class="line">    history = model.fit(partial_train_data, partial_train_targets,</span><br><span class="line">    validation_data=(val_data, val_targets),</span><br><span class="line">    epochs=num_epochs, batch_size=<span class="number">1</span>, verbose=<span class="number">0</span>)</span><br><span class="line">    mae_history = history.history[<span class="string">'val_mean_absolute_error'</span>]</span><br><span class="line">    all_mae_histories.append(mae_history)</span><br></pre></td></tr></table></figure><br>然后你可以计算每个轮次中所有折MAE 的平均值。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">average_mae_history = [np.mean([x[i] <span class="keyword">for</span> x <span class="keyword">in</span> all_mae_histories]) <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs)]</span><br></pre></td></tr></table></figure><br>我们画图来看一下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(range(<span class="number">1</span>, len(average_mae_history) + <span class="number">1</span>), average_mae_history)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Validation MAE'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<center>
    <img src="\Pic\DeepLearning_Pic\MAE_1.png" width="400" height="400" alt="MAE_1" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">每轮的验证MAE</div>
</center>

<p>因为纵轴的范围较大，且数据方差相对较大，所以难以看清这张图的规律。我们来重新绘制一张图。</p>
<ul>
<li>删除前 10 个数据点，因为它们的取值范围与曲线上的其他点不同。</li>
<li>将每个数据点替换为前面数据点的指数移动平均值，以得到光滑的曲线。<br>代码如下所示：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smooth_curve</span><span class="params">(points, factor=<span class="number">0.9</span>)</span>:</span></span><br><span class="line">    smoothed_points = []</span><br><span class="line">    <span class="keyword">for</span> point <span class="keyword">in</span> points:</span><br><span class="line">        <span class="keyword">if</span> smoothed_points:</span><br><span class="line">            previous = smoothed_points[<span class="number">-1</span>]</span><br><span class="line">            smoothed_points.append(previous * factor + point * (<span class="number">1</span> - factor))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            smoothed_points.append(point)</span><br><span class="line">    <span class="keyword">return</span> smoothed_points</span><br><span class="line">smooth_mae_history = smooth_curve(average_mae_history[<span class="number">10</span>:])</span><br><span class="line">plt.plot(range(<span class="number">1</span>, len(smooth_mae_history) + <span class="number">1</span>), smooth_mae_history)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Validation MAE'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
从图中可以看出验证MAE 在80 轮后不再显著降低，之后就开始过拟合。</li>
</ul>
<center>
    <img src="\Pic\DeepLearning_Pic\MAE_2.png" width="400" height="400" alt="MAE_2" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">每轮的验证MAE（删除前十个数据点）</div>
</center>

<p>完成模型调参之后（除了轮数，还可以调节隐藏层大小），你可以使用最佳参数在所有训练数据上训练最终的生产模型，然后观察模型在测试集上的性能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = build_model()</span><br><span class="line">model.fit(train_data, train_targets,</span><br><span class="line">epochs=<span class="number">80</span>, batch_size=<span class="number">16</span>, verbose=<span class="number">0</span>)</span><br><span class="line">test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)</span><br><span class="line">print(test_mae_score)</span><br></pre></td></tr></table></figure>
<pre><code>102/102 [==============================] - 0s 147us/step
2.905885696411133
</code></pre><p>你预测的房价还是和实际价格相差约2905 美元。</p>
]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
  </entry>
  <entry>
    <title>XGBoost（二）解决二分类和多分类问题</title>
    <url>/posts/8c5e1ebd.html</url>
    <content><![CDATA[<p>XGBoost由多棵决策树（CART）组成，每棵决策树预测真实值与之前所有决策树预测值之和的残差（残差=真实值-预测值），将所有决策树的预测值累加起来即为最终结果。</p>
<h1 id="一、二分类问题"><a href="#一、二分类问题" class="headerlink" title="一、二分类问题"></a>一、二分类问题</h1><p>XGBoost树模型由多棵回归树组成，并将多棵决策树的预测值累计相加作为最终结果。回归树产生的是连续的回归值，如何用它解决二分类问题呢？通过前面的学习知道，逻辑回归是在线性回归的基础上通过<code>sigmoid</code>函数将预测值映射到0～1的区间来代表二分类结果的概率。和逻辑回归一样，XGBoost也是采用<code>sigmoid</code>函数来解决二分类问题，即先通过回归树对样本进行预测，得到每棵树的预测结果，然后将其进行累加求和，最后通过<code>sigmoid</code>函数将其映射到0～1的区间代表二分类结果的概率。另外，对于二分类问题，XGBoost的目标函数采用的是类似逻辑回归的<code>logloss</code>，而非最小二乘。</p>
<p>XGBoost中关于二分类的常用参数有如下几个：</p>
<ul>
<li><code>Objective</code>: 该参数用来指定目标函数，XGBoost可以根据该参数判断进行何种学习任务，<code>binary:logistic</code>和<code>binary:logitraw</code>都表示学习任务类型为二分类。<code>binary:logistic</code>输出为概率，<code>binary:logitraw</code>输出为逻辑转换前的输出分数。</li>
<li><code>eval_metric</code>: 该参数用来指定模型的评估函数，和二分类相关的评估函数有：error、logloss和auc。error也称错误率，即预测错误的样本数占总样本数的比例，准确来说是预测错误样本的权重和占总样本权重和的比例，也可通过error@k的形式手工指定二分类的阈值。logloss通过惩罚分类来量化模型的准确性，最大限度减少logloss，等同于最大化模型的准确率。另外，AUC也是二分类中最常用的评估指标之一，计算方法可另外查询。</li>
</ul>
<p>下面仍然以该案例数据集进行说明。蘑菇数据集是一个非常著名的二分类数据集。该数据集一共包含23个特征，包括大小、表面、颜色等，每一种蘑菇都会被定义为可食用的或者有毒的，需要通过样本数据分析这些特征与蘑菇毒性的关系。以下是各个特征的详细说明：</p>
<ul>
<li>帽形（cap-shape）：钟形=b，圆锥形=c，凸形=x，平面=f，把手形=k，凹陷=S</li>
<li>帽面（cap-surface）：纤维状=f，凹槽状=g，鳞片状=y，光滑=s</li>
<li>帽颜色（cap-color）：棕色=n，浅黄色=b，肉桂色=c，灰色=g，绿色=r，粉红色=p，紫色=u，红色=e，白色=w，黄色=y</li>
<li>创伤（bruises）：创伤=t，no=f</li>
<li>气味（odor）：杏仁=a，茴香=l，石灰=c，腥味=y，臭味=f，霉味=m，无=n，刺鼻=p，辣=s</li>
<li>菌褶附属物（gill-attachment:）：附着=a，下降=d，自由=f，缺口=n</li>
<li>菌褶间距（gill-spacing）：紧密=c，拥挤=w，远隔=d</li>
<li>菌褶大小（gill-size）：宽=b，窄=n。</li>
<li>菌褶颜色（gill-color）：黑色=k，棕色=n，浅黄色=b，巧克力色=h，灰色=g，绿色=r，橙色=o，粉红色=p，紫色=u，红色=e，白色=w，黄色=y</li>
<li>茎形（stalk-shape）：扩大=e，锥形=t</li>
<li>茎根（stalk-root）：球根=b，棒状=c，杯状=u，均等的=e，根状菌索=z，扎根=r，缺省=？</li>
<li>环上茎面（stalk-surface-above-ring）：纤维状=f，鳞片状=y，丝状=k，光滑=s</li>
<li>环下茎面（stalk-surface-below-ring）：纤维状=f，鳞片状=y，丝状=k，光滑=s</li>
<li>环上茎颜色（stalk-color-above-ring）：棕色=n，浅黄色=b，黄棕色=c，灰色=g，橙色=o，粉红色=p，红色=e，白色=w，黄色=y</li>
<li>环下茎颜色（stalk-color-below-ring）：棕色=n，浅黄色=b，黄棕色=c，灰色=g，橙色=o，粉红色=p，红色=e，白色=w，黄色=y</li>
<li>菌幕类型（veil-type）：部分=p，普遍=u</li>
<li>菌幕颜色（veil-color）：棕色=n，橙色=o，白色=w，黄色=y</li>
<li>环数量（ring-number）：没有=n，一个=o，两个=t</li>
<li>环类型（ring-type）：蛛网状=c，消散状=e，喇叭形=f，大规模的=l，无=n，悬垂的=p，覆盖=s，环带=z</li>
<li>孢子显现颜色（spore-print-color）：黑色=k，棕色=n，蓝色=b，巧克力色=h，绿色=r，橙色=o，紫色=u，白色=w，黄色=y</li>
<li>种群（population）：丰富=a，聚集=c，众多=n，分散=s，个别=v，单独=y</li>
<li>栖息地（habitat）：草地=g，树叶=l，草甸=m，路上=p，城市=u，荒地=w，树林=d</li>
<li>class：label字段，有可食用（edible）和有毒性（poisonous）两个取值</li>
</ul>
<p>该数据集总共有8124个样本，其中类别为可食用的样本有4208个，类别为有毒性的样本有3916个。对于该二分类问题，XGBoost工程文件中提供了示例代码。示例以命令行的方式调用XGBoost，完成模型训练、预测等过程。示例位于<code>demo/CLI/binary_classification</code>文件夹下，其中包括下面几个文件：</p>
<ul>
<li><code>agaricus-lepiota.data</code>——蘑菇数据文件</li>
<li><code>agaricus-lepiota.fmap</code>——字段名称映射文件</li>
<li><code>agaricus-lepiota.names</code>——蘑菇数据集描述文件</li>
<li><code>mapfeat.py</code>——数据集特征值预处理</li>
<li><code>mknfold.py</code>——划分数据集</li>
<li><code>mushroom.conf</code>——模型配置文件</li>
<li><code>runexp.sh</code>——运行脚本</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_dir = <span class="string">"xgboost_source_code/demo/CLI/binary_classification/"</span></span><br></pre></td></tr></table></figure>
<p>读者可自行尝试执行<code>runexp.sh</code>脚本，学习命令行形式的调用过程。本节重点介绍如何通过Python调用XGBoost进行模型训练和预测，并对处理流程中的各个阶段进行详细解析。</p>
<p>首先需要对特征进行预处理。因为原始文件<code>agaricus-lepiota.data</code>中的数据并不能直接作为XGBoost的输入进行加载，需要进行预处理。这里将其中的字符数据转为数值型，并以LibSVM的格式输出。LibSVM是机器学习中经常采用的一种数据格式，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;label&gt; &lt;index1&gt;:&lt;value1&gt;&lt;index2&gt;:&lt;value2&gt;...</span><br></pre></td></tr></table></figure>
<p><code>label</code>为训练数据集的目标值；<code>index</code>为特征索引，是一个以1为起始的整数；<code>value</code>是该特征的取值，如果某一特征的值缺省，则该特征可以空着不填，因此对于一个样本来讲，输出后的数据文件<code>index</code>可能并不连续，上述样本处理后的格式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 3:1 10:1 11:1 21:1 30:1 34:1 36:1 40:1 41:1 53:1 58:1 65:1 69:1 77:1 86:1 88:1 92:1 95:1 102:1 105:1 117:1 124:1</span><br><span class="line">0 3:1 10:1 20:1 21:1 23:1 34:1 36:1 39:1 41:1 53:1 56:1 65:1 69:1 77:1 86:1 88:1 92:1 95:1 102:1 106:1 116:1 120:1</span><br></pre></td></tr></table></figure>
<p>第一个样本中最开始的“1”便是该样本的label，在二分类问题中，一般1代表正样本，0代表负样本。之后的每个特征为一项，冒号前为该特征的索引，如3、10等，冒号后为该特征取值，如3、10两个特征的取值都是1。另外，观察处理后的数据可以发现，特征索引已经远远超过了22，如第一行样本中特征索引最大已经达到了124。</p>
<p>观察该数据集可以发现，其中大部分特征是离散型特征，连续型特征较少。在机器学习算法中，特征之间距离的计算是十分重要的，因此，直接把离散变量的取值转化为数值，并不能很好地代表特征间的距离，如菌幕颜色特征，其总共有棕色、橙色、白色、黄色4种颜色，假如将其映射为1、2、3、4，则棕色和橙色之间的距离是2-1=1，而棕色和白色之间的距离是3-1=2。这显然是不符合实际情况的，因为任意两个颜色之间的距离应该是相等的。因此，需要对特征进行独热编码（one-hot encoding）。</p>
<p>简单来讲，独热编码就是离散特征有多少取值，就用多少维来表示该特征。仍然以菌幕颜色特征为例，经过独热编码后，其将会转为4个特征，分别是菌幕颜色是否为棕色、菌幕颜色是否为橙色、菌幕颜色是否为白色和菌幕颜色是否为黄色，并且这4个特征取值只有0和1。经过独热编码之后，每两个颜色之间的距离都是一样的，比之前的处理更合理。离散特征经过独热编码之后，数据集的总特征数会变多，这就是上述示例中出现较大特征索引的原因。下面来看一下特征处理的代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadmap</span><span class="params">(fname)</span>:</span></span><br><span class="line">    fmap = &#123;&#125;</span><br><span class="line">    nmap = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> open(fname):</span><br><span class="line">        arr = l.split()</span><br><span class="line">        <span class="keyword">if</span> arr[<span class="number">0</span>].find(<span class="string">'.'</span>) != <span class="number">-1</span>:</span><br><span class="line">            idx = int(arr[<span class="number">0</span>].strip(<span class="string">'.'</span>))</span><br><span class="line">            <span class="keyword">assert</span> idx <span class="keyword">not</span> <span class="keyword">in</span> fmap</span><br><span class="line">            fmap[idx] = &#123;&#125;</span><br><span class="line">            ftype = arr[<span class="number">1</span>].strip(<span class="string">":"</span>)</span><br><span class="line">            content = arr[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            content = arr[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> content.split(<span class="string">','</span>):</span><br><span class="line">            <span class="keyword">if</span> it.strip() == <span class="string">''</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            k, v = it.split(<span class="string">'='</span>)</span><br><span class="line">            fmap[idx][v] = len(nmap) + <span class="number">1</span></span><br><span class="line">            nmap[len(nmap)] = ftype + <span class="string">'='</span> + k</span><br><span class="line">    <span class="keyword">return</span> fmap, nmap</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_nmap</span><span class="params">(fo, nmap)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nmap)):</span><br><span class="line">        fo.write(<span class="string">'%d\t%s\ti\n'</span>%(i, nmap[i]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fmap, nmap = loadmap(data_dir+<span class="string">'agaricus-lepiota.fmap'</span>)</span><br><span class="line">fo = open(<span class="string">'output/data/featmap.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line">write_nmap(fo, nmap)</span><br><span class="line">fo.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fo = open(<span class="string">'output/data/agaricus.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> open(data_dir+<span class="string">'agaricus-lepiota.data'</span>):</span><br><span class="line">    arr = l.split(<span class="string">','</span>)</span><br><span class="line">    <span class="keyword">if</span> arr[<span class="number">0</span>] == <span class="string">'p'</span>:</span><br><span class="line">        fo.write(<span class="string">'1'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> arr[<span class="number">0</span>] == <span class="string">'e'</span></span><br><span class="line">        fo.write(<span class="string">'0'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(arr)):</span><br><span class="line">        fo.write(<span class="string">' %d:1'</span> %fmap[i][arr[i].strip()])</span><br><span class="line">    fo.write(<span class="string">'\n'</span>)</span><br><span class="line">fo.close()</span><br></pre></td></tr></table></figure>
<p>首先程序会加载特征描述文件<code>agaricus-lepiota.fmap</code>，为每个特征的每个取值均分配一个唯一的索引标识，并为其重新命名，并将处理后的新特征索引和名称的映射保存为<code>featmap.txt</code>文件（该映射文件会在XGBoost中用到）。然后加载蘑菇数据集，通过新特征索引处理该数据集，生成转化后的新数据文件<code>featmap.txt</code>。特征处理完后即可通过mknfold.py划分数据集。在本示例中，划分数据集是通过代码实现的，当然读者也可以采用第3章介绍的scikit-learn中的<code>train_test_split</code>来划分数据集。下面看一下<code>mknfold.py</code>的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> len(sys.argv) &lt; <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'Usage:&lt;filename&gt; &lt;k&gt; [nfold = 5]'</span>)</span><br><span class="line">    exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">random.seed( <span class="number">10</span> )</span><br><span class="line"></span><br><span class="line">k = int( sys.argv[<span class="number">2</span>] )</span><br><span class="line"><span class="keyword">if</span> len(sys.argv) &gt; <span class="number">3</span>:</span><br><span class="line">    nfold = int( sys.argv[<span class="number">3</span>] )</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    nfold = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">fi = open( sys.argv[<span class="number">1</span>], <span class="string">'r'</span> )</span><br><span class="line">ftr = open( sys.argv[<span class="number">1</span>]+<span class="string">'.train'</span>, <span class="string">'w'</span> )</span><br><span class="line">fte = open( sys.argv[<span class="number">1</span>]+<span class="string">'.test'</span>, <span class="string">'w'</span> )</span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> fi:</span><br><span class="line">    <span class="keyword">if</span> random.randint( <span class="number">1</span> , nfold ) == k:</span><br><span class="line">        fte.write( l )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ftr.write( l )</span><br><span class="line"></span><br><span class="line">fi.close()</span><br><span class="line">ftr.close()</span><br><span class="line">fte.close()</span><br></pre></td></tr></table></figure>
<p>生成训练集和测试集后，便可通过XGBoost加载数据进行训练，下面通过Python实现XGBoost的调用。先加载训练集和测试集：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line">xgb_train = xgb.DMatrix(<span class="string">"xgboost_source_code/demo/data/agaricus.txt.train"</span>)</span><br><span class="line">xgb_test = xgb.DMatrix(<span class="string">"xgboost_source_code/demo/data/agaricus.txt.test"</span>)</span><br></pre></td></tr></table></figure>
<p>设定模型训练参数，开始模型训练：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">params = &#123;</span><br><span class="line">    <span class="string">"objective"</span> : <span class="string">"binary:logistic"</span>,</span><br><span class="line">    <span class="string">"booster"</span> : <span class="string">"gbtree"</span>, </span><br><span class="line">    <span class="string">"eta"</span> : <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"gamma"</span> : <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"min_child_weight"</span> : <span class="number">1</span>,</span><br><span class="line">    <span class="string">"max_depth"</span> : <span class="number">3</span></span><br><span class="line">&#125;</span><br><span class="line">num_round = <span class="number">2</span></span><br><span class="line">watchlist = [(xgb_train, <span class="string">"train"</span>), (xgb_test, <span class="string">"test"</span>)]</span><br><span class="line">model = xgb.train(params, xgb_train, num_round, watchlist)</span><br></pre></td></tr></table></figure>
<pre><code>[0]    train-error:0.01443    test-error:0.01614
[1]    train-error:0.00123    test-error:0.00000
</code></pre><p><code>params</code>中的<code>objective</code>和<code>booster</code>参数已经介绍过了，分别用于指定任务的学习目标和<code>booster</code>类型，其他参数说明如下：</p>
<ul>
<li><code>objective</code>设为<code>binary:logistic</code>，表示任务为二分类问题，最终输出为<code>sigmoid</code>变换后的概率。</li>
<li><code>booster</code>为<code>gbtree</code>表示采用XGBoost中的树模型。参数<code>eta</code>表示学习率，类似于梯度下降中法的$\alpha$，每次迭代完更新权重的步长。</li>
<li><code>gamma</code>表示节点分裂时损失函数减小的最小值，此处为1.0，表示损失函数至少下降1.0该节点才会进行分裂。</li>
<li><code>min_child_weight</code>表示叶子节点最小样本权重和，若节点分裂导致叶子节点的样本权重和小于该值，则节点不进行分裂。</li>
<li><code>max_depth</code>表示决策树分裂的最大深度。</li>
</ul>
<p>另外，该示例中指定了<code>num_round</code>为2，即模型会进行两轮<code>booster</code>训练，最终会生成两棵决策树。通过定义参数<code>watchlist</code>，模型在训练过程中会实时输出训练集和验证集的评估指标。</p>
<p>模型训练完成之后，可通过<code>save_model</code>方法将模型保存成模型文件，以供后续预测使用，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save_model(<span class="string">"output/model/02_agaricus.model"</span>)</span><br></pre></td></tr></table></figure>
<p>预测时，先加载保存的模型文件，然后再对数据集进行预测，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bst = xgb.Booster()</span><br><span class="line">bst.load_model(<span class="string">"output/model/02_agaricus.model"</span>)</span><br><span class="line">pred = bst.predict(xgb_test)</span><br><span class="line">print(pred)</span><br></pre></td></tr></table></figure>
<pre><code>[0.10828121 0.85500014 0.10828121 ... 0.95467216 0.04156424 0.95467216]
</code></pre><p>可以看到，输出结果是一个浮点数组成的数组，其中每个值代表对应样本的预测概率。预测完成后，输出文本格式的模型，这里仍然采用两种方式，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 未作特征名转换</span></span><br><span class="line">dump_model_raw = bst.dump_model(<span class="string">"output/data/dump.raw.txt"</span>)</span><br><span class="line"><span class="comment"># 完成特征名转换</span></span><br><span class="line">dump_model_nice = bst.dump_model(<span class="string">"output/data/dump.nice.txt"</span>, <span class="string">"output/data/featmap.txt"</span>)</span><br></pre></td></tr></table></figure>
<p>下面主要以完成特征名称转换后的模型文件为例进行介绍。先来看一下索引和特征名称映射文件<code>featmap.txt</code>，格式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;featureid&gt; &lt;featurename&gt; &lt;q or i or int&gt;\n</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>featureid</code>为特征索引</li>
<li><code>featurename</code>为特征名称</li>
<li><code>q or i or int</code>为特征的数据类型，其中<code>q</code>代表特征是一个连续值，如距离、价格等；<code>i</code>代表特征是一个二值特征（即特征只有两个取值），一般为0或1；<code>int</code>代表特征是整型值。可以看到，<code>featmap.txt</code>中的很多特征都是二值特征。这个也不难理解，因为该数据集中大部分是离散型的类别特征，因此经过独热编码处理后，新生成的特征基本都是二值特征。</li>
</ul>
<p>了解了特征映射文件后，下面来看一下文本格式的XGBoost树模型文件，以下截取了<code>dump.nice.txt</code>的前几行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">booster[0]:</span><br><span class="line">0:[odor&#x3D;pungent] yes&#x3D;2,no&#x3D;1</span><br><span class="line">1:[stalk-root&#x3D;cup] yes&#x3D;4,no&#x3D;3</span><br><span class="line">3:[stalk-root&#x3D;missing] yes&#x3D;8,no&#x3D;7</span><br><span class="line">			7:leaf&#x3D;1.90174532</span><br><span class="line">			8:leaf&#x3D;-1.95061731</span><br><span class="line">4:[bruises?&#x3D;no] yes&#x3D;10,no&#x3D;9</span><br><span class="line">			9:leaf&#x3D;1.77777779</span><br><span class="line">			10:leaf&#x3D;-1.98104262</span><br><span class="line">2:[spore-print-color&#x3D;orange] yes&#x3D;6,no&#x3D;5</span><br><span class="line">5:[stalk-surface-below-ring&#x3D;silky] yes&#x3D;12,no&#x3D;11</span><br></pre></td></tr></table></figure>
<p>上面的一个booster代表一棵决策树，该模型一共有两棵决策树。在每棵决策树中，每一行代表一个节点，位于行首的数字代表该节点的索引，数字0表示该节点为根节点。若该行节点是非叶子节点，则索引后面是该节点的分裂条件，如第2行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0:[odor&#x3D;pungent] yes&#x3D;2,no&#x3D;1</span><br></pre></td></tr></table></figure>
<p>该节点的索引为0，表示该节点是根节点，其分裂条件是odor=pungent，满足该条件的样本会被划分到节点2，不满足的则被划分到节点1。若该行节点是叶子节点，则索引后面是该叶子节点最终得到的权重。如第5行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">7:leaf&#x3D;1.90174532</span><br></pre></td></tr></table></figure>
<p><code>leaf</code>表示该节点为叶子节点，最终得到的权重为1.90174532。由此，通过文本格式的模型文件，可以使用户了解样本在模型中是如何被划分的，使模型更具有可解释性，并且在实际的机器学习任务中，也有利于用户更好地分析和优化模型。</p>
<h1 id="二、多分类问题"><a href="#二、多分类问题" class="headerlink" title="二、多分类问题"></a>二、多分类问题</h1><p>与处理二分类问题类似，XGBoost在处理多分类问题时也是在树模型的基础上进行转换，不过不再是<code>sigmoid</code>函数，而是<code>softmax</code>函数。相信大家对<code>softmax</code>变换并不陌生，它可以将多分类的预测值映射到0到1之间，代表样本属于该类别的概率。XGBoost中解决多分类问题的主要参数如下：</p>
<ul>
<li><code>num_class</code>：说明在该分类任务的类别数量</li>
<li><code>objective</code>：该参数中的<code>multi:softmax</code>和<code>multi:softprob</code>均是指定学习任务为多分类。<code>multi:softmax</code>通过<code>softmax</code>函数解决多分类问题。<code>multi:softprob</code>和<code>multi:softmax</code>一样，主要区别在于其输出的是一个$ndata*nclass$向量，表示样本属于每个分类的预测概率</li>
<li><code>eval_metric</code>：与多分类相关的评估函数有<code>merror</code>和<code>mlogloss</code>。<code>merror</code>也称多分类错误率，通过判断样本所有分类预测值中预测值最大的分类和样本label是否一致来确定预测是否正确，其计算方式和<code>error</code>相似。<code>mlogloss</code>也是多分类问题中常用的评估指标。有关<code>merror</code>和<code>mlogloss</code>会在后面详细介绍。</li>
</ul>
<p>下面以识别小麦种子的类别作为示例，介绍如何通过XGBoost解决多分类问题。已知小麦种子数据集包含7个特征，分别为面积、周长、紧凑度、籽粒长度、籽粒宽度、不对称系数、籽粒腹沟长度，且均为连续型特征，以及小麦类别字段，共有3个类别，分别用1、2、3表示。加载该数据并进行特征处理，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">"input/seeds_dataset.txt"</span>, header=<span class="literal">None</span>, sep=<span class="string">'\s+'</span>, converters=&#123;<span class="number">7</span>: <span class="keyword">lambda</span> x:int(x)<span class="number">-1</span>&#125;)</span><br><span class="line">data.rename(columns=&#123;<span class="number">7</span>:<span class="string">'label'</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>15.26</td>
      <td>14.84</td>
      <td>0.8710</td>
      <td>5.763</td>
      <td>3.312</td>
      <td>2.221</td>
      <td>5.220</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>14.88</td>
      <td>14.57</td>
      <td>0.8811</td>
      <td>5.554</td>
      <td>3.333</td>
      <td>1.018</td>
      <td>4.956</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>14.29</td>
      <td>14.09</td>
      <td>0.9050</td>
      <td>5.291</td>
      <td>3.337</td>
      <td>2.699</td>
      <td>4.825</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>13.84</td>
      <td>13.94</td>
      <td>0.8955</td>
      <td>5.324</td>
      <td>3.379</td>
      <td>2.259</td>
      <td>4.805</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>16.14</td>
      <td>14.99</td>
      <td>0.9034</td>
      <td>5.658</td>
      <td>3.562</td>
      <td>1.355</td>
      <td>5.175</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>为便于后续处理，将最后一个类别字段作为<code>label</code>字段，因为<code>label</code>的取值需在0到<code>num_class-1</code>范围内，因此需对类别字段进行处理（数据集中的3个类别取值分别为1～3），这里直接减1即可。</p>
<p>可以看到，数据集共包含8列，其中前7列为特征列，最后1列为<code>label</code>列，和数据集描述相符。除<code>label</code>列外，剩余特征没有指定列名，所以pandas自动以数字索引作为列名。下面对数据集进行划分（训练集和测试集的划分比例为4:1），并指定<code>label</code>字段生成XGBoost中的<code>DMatrix</code>数据结构，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mask = np.random.rand(len(data)) &lt; <span class="number">0.8</span></span><br><span class="line">train = data[mask]</span><br><span class="line">test = data[~mask]</span><br><span class="line">xgb_train = xgb.DMatrix(train.iloc[:,:<span class="number">6</span>], label=train.label)</span><br><span class="line">xgb_test = xgb.DMatrix(test.iloc[:,:<span class="number">6</span>], label=test.label)</span><br></pre></td></tr></table></figure>
<p>设置模型训练参数。设置参数<code>objective</code>为<code>multi:softmax</code>，表示采用<code>softmax</code>进行多分类，学习率参数<code>eta</code>和最大树深度<code>max_depth</code>在之前的示例中已有所介绍，不再赘述。参数<code>num_class</code>指定类别数量为3。相关代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'objective'</span>:<span class="string">'multi:softmax'</span>,</span><br><span class="line">    <span class="string">'eta'</span>:<span class="number">0.1</span>,</span><br><span class="line">    <span class="string">'max_depth'</span>:<span class="number">5</span>,</span><br><span class="line">    <span class="string">'num_class'</span>:<span class="number">3</span></span><br><span class="line">&#125;</span><br><span class="line">watchlist = [(xgb_train, <span class="string">"train"</span>), (xgb_test, <span class="string">"test"</span>)]</span><br><span class="line">num_round = <span class="number">10</span></span><br><span class="line">bst = xgb.train(params, xgb_train, num_round, watchlist)</span><br></pre></td></tr></table></figure>
<pre><code>[0]    train-merror:0.01219    test-merror:0.10870
[1]    train-merror:0.01219    test-merror:0.10870
[2]    train-merror:0.01219    test-merror:0.10870
[3]    train-merror:0.01219    test-merror:0.10870
[4]    train-merror:0.01219    test-merror:0.13043
[5]    train-merror:0.00610    test-merror:0.13043
[6]    train-merror:0.00610    test-merror:0.13043
[7]    train-merror:0.00610    test-merror:0.13043
[8]    train-merror:0.00610    test-merror:0.15217
[9]    train-merror:0.00610    test-merror:0.15217
</code></pre><p>在未指定评估函数的情况下，XGBoost默认采用<code>merror</code>作为多分类问题的评估指标。下面通过训练好的模型对测试集进行预测，并计算错误率，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pred = bst.predict(xgb_test)</span><br><span class="line">error_rate = np.sum(pred != test.label) / test.shape[<span class="number">0</span>]</span><br><span class="line">print(error_rate)</span><br></pre></td></tr></table></figure>
<pre><code>0.15217391304347827
</code></pre><p>为了方便对比学习，下面采用<code>multi:softprob</code>方法重新训练模型，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">params[<span class="string">"objective"</span>] = <span class="string">"multi:softprob"</span></span><br><span class="line">bst = xgb.train(params, xgb_train, num_round, watchlist)</span><br></pre></td></tr></table></figure>
<pre><code>[0]    train-merror:0.01219    test-merror:0.10870
[1]    train-merror:0.01219    test-merror:0.10870
[2]    train-merror:0.01219    test-merror:0.10870
[3]    train-merror:0.01219    test-merror:0.10870
[4]    train-merror:0.01219    test-merror:0.13043
[5]    train-merror:0.00610    test-merror:0.13043
[6]    train-merror:0.00610    test-merror:0.13043
[7]    train-merror:0.00610    test-merror:0.13043
[8]    train-merror:0.00610    test-merror:0.15217
[9]    train-merror:0.00610    test-merror:0.15217
</code></pre><p>对比两种函数变换方法的训练输出结果可以看出，不论采用<code>multi:softmax</code>还是<code>multi:softprob</code>作为<code>objective</code>训练模型，并不会影响到模型精度。</p>
<p>下面对测试集进行预测并计算错误率，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pred_prop = bst.predict(xgb_test)</span><br><span class="line">pred_label = np.argmax(pred_prop, axis=<span class="number">1</span>)</span><br><span class="line">error_rate = np.sum(pred_label != test.label) / test.shape[<span class="number">0</span>]</span><br><span class="line">print(<span class="string">'测试集错误率(softprob):&#123;&#125;'</span>.format(error_rate))</span><br></pre></td></tr></table></figure>
<pre><code>测试集错误率(softprob):0.15217391304347827
</code></pre><p>之后的处理则和采用<code>multi:softmax</code>时一样，统计预测错误的样本数，最终计算出分类错误率。采用<code>multi:softprob</code>得到的错误率和<code>multi:softmax</code>也是一样的</p>
]]></content>
      <categories>
        <category>XGBoost</category>
      </categories>
  </entry>
  <entry>
    <title>XGBoost（一）简单机器学习示例</title>
    <url>/posts/298496a6.html</url>
    <content><![CDATA[<h1 id="一、XGBoost简单应用"><a href="#一、XGBoost简单应用" class="headerlink" title="一、XGBoost简单应用"></a>一、XGBoost简单应用</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xgb_train = xgb.DMatrix(<span class="string">"xgboost_source_code/demo/data/agaricus.txt.train"</span>)</span><br><span class="line">xgb_test = xgb.DMatrix(<span class="string">"xgboost_source_code/demo/data/agaricus.txt.test"</span>)</span><br><span class="line">param = &#123;<span class="string">'max_depth'</span>:<span class="number">2</span>, <span class="string">'eta'</span>:<span class="number">1</span>, <span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>&#125;</span><br><span class="line">num_round = <span class="number">5</span></span><br><span class="line">watch_list = [(xgb_train, <span class="string">"train"</span>), (xgb_test, <span class="string">"test"</span>)]</span><br><span class="line">model = xgb.train(param, xgb_train, num_round, watch_list)</span><br></pre></td></tr></table></figure>
<pre><code>[0]    train-error:0.04652    test-error:0.04283
[1]    train-error:0.02226    test-error:0.02173
[2]    train-error:0.00706    test-error:0.00621
[3]    train-error:0.01520    test-error:0.01800
[4]    train-error:0.00706    test-error:0.00621
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">preds = model.predict(xgb_test)</span><br><span class="line">preds</span><br></pre></td></tr></table></figure>
<pre><code>array([0.08073306, 0.92217326, 0.08073306, ..., 0.98059034, 0.01182149,
       0.98059034], dtype=float32)
</code></pre><h1 id="二、机器学习算法基础"><a href="#二、机器学习算法基础" class="headerlink" title="二、机器学习算法基础"></a>二、机器学习算法基础</h1><p>我们首先介绍几个基础的机器学习算法的实现原理和应用，如KNN、线性回归、逻辑回归等，使读者对机器学习算法有一个基本认识的同时，了解如何在模型训练过程中进行优化，以及如何对模型结果进行评估。然后，对决策树模型做了详细介绍。决策树是XGBoost模型的重要组成部分，学习和掌握决策树的生成、剪枝等内容将会对后续的学习提供巨大帮助。排序问题是机器学习中的常见问题，神经网络和支持向量机也是经常采用的机器学习算法，最后将分别介绍两者的实现原理，结合详细的公式推导过程，使读者能够深入理解算法背后的数学原理。</p>
<h2 id="1-KNN做鸢尾花数据预测"><a href="#1-KNN做鸢尾花数据预测" class="headerlink" title="1. KNN做鸢尾花数据预测"></a>1. KNN做鸢尾花数据预测</h2><p>KNN的主要算法思想为：特征空间中的一个样本，如果与其最相似的k个样本中的大部分属于某个类别，则该样本也属于该类别。KNN既可以用于解决分类问题，也可以用于回归问题。</p>
<p>对于分类问题，离样本最近的k个邻居中占多数的类别作为该样本的类别，如果k=1，则选取最近邻居的类别作为该样本的类别。对于回归问题，样本的预测值是最近的k个邻居的平均值。</p>
<p>KNN的计算步骤如下。</p>
<ol>
<li>计算测试样本与训练集中所有（或大部分）样本的距离，该距离可以是欧氏距离、余弦距离等，较常用的是欧氏距离。</li>
<li>找到步骤1中距离最短的k个样本，作为预测样本的邻居。</li>
<li>对于分类问题，通过投票机制选出k个邻居中最多的类别作为预测样本的预测值。对于回归问题，则采用k个邻居的平均值。</li>
</ol>
<p>Iris也称鸢尾花卉数据集，是一类多重变量分析的数据集。数据集包含150个数据集，分为3类，每类50个数据，每个数据包含4个属性。可通过花萼长度，花萼宽度，花瓣长度，花瓣宽度4个属性预测鸢尾花卉属于（Setosa，Versicolour，Virginica）三个种类中的哪一类。</p>
<p><strong>数据描述</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量名</th>
<th>sepal_length</th>
<th>sepal_width</th>
<th>petal_length</th>
<th>petal_width</th>
<th>species</th>
</tr>
</thead>
<tbody>
<tr>
<td>变量解释</td>
<td>花萼长度（单位cm）</td>
<td>花萼宽度（单位cm）</td>
<td>花瓣长度（单位cm）</td>
<td>花瓣宽度（单位cm）</td>
<td>种类</td>
</tr>
<tr>
<td>数据类型</td>
<td>numeric</td>
<td>numeric</td>
<td>numeric</td>
<td>numeric</td>
<td>categorical</td>
</tr>
</tbody>
</table>
</div>
<h3 id="1-1-导入数据集并观察分布"><a href="#1-1-导入数据集并观察分布" class="headerlink" title="1.1 导入数据集并观察分布"></a>1.1 导入数据集并观察分布</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">iris = load_iris()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.DataFrame(iris.data, columns=iris.feature_names).head()</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">setosa_sepal_len = iris.data[:<span class="number">50</span>, <span class="number">0</span>]</span><br><span class="line">setosa_sepal_width = iris.data[:<span class="number">50</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">versi_sepal_len = iris.data[<span class="number">50</span>:<span class="number">100</span>, <span class="number">0</span>]</span><br><span class="line">versi_sepal_width = iris.data[<span class="number">50</span>:<span class="number">100</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">vergi_sepal_len = iris.data[<span class="number">100</span>:, <span class="number">0</span>]</span><br><span class="line">vergi_sepal_width = iris.data[<span class="number">100</span>:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">pyplot.scatter(setosa_sepal_len, setosa_sepal_width, marker = <span class="string">'o'</span>, c = <span class="string">'b'</span>,  s = <span class="number">30</span>, label = <span class="string">'Setosa'</span>)</span><br><span class="line">pyplot.scatter(versi_sepal_len, versi_sepal_width, marker = <span class="string">'o'</span>, c = <span class="string">'r'</span>,  s = <span class="number">50</span>, label = <span class="string">'Versicolour'</span>)</span><br><span class="line">pyplot.scatter(vergi_sepal_len, vergi_sepal_width, marker = <span class="string">'o'</span>, c = <span class="string">'y'</span>,  s = <span class="number">35</span>, label = <span class="string">'Virginica'</span>)</span><br><span class="line">pyplot.xlabel(<span class="string">"sepal length"</span>)</span><br><span class="line">pyplot.ylabel(<span class="string">"sepal width"</span>)</span><br><span class="line">pyplot.title(<span class="string">"sepal length and width scatter"</span>)</span><br><span class="line">pyplot.legend(loc = <span class="string">"upper right"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="\Pic\XGBoost_Pic\output_7_1.png" alt="png"></p>
<h3 id="2-绘制各个品种各个特征平均值的直方图"><a href="#2-绘制各个品种各个特征平均值的直方图" class="headerlink" title="2. 绘制各个品种各个特征平均值的直方图"></a>2. 绘制各个品种各个特征平均值的直方图</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)</span><br><span class="line">iris_data[<span class="string">"class"</span>] = iris.target</span><br><span class="line">iris_data.head()</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">grouped_data = iris_data.groupby(<span class="string">"class"</span>)</span><br><span class="line">group_mean = grouped_data.mean()</span><br><span class="line">group_mean.plot(kind=<span class="string">'bar'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"center right"</span>, bbox_to_anchor=(<span class="number">1.4</span>, <span class="number">0.3</span>), ncol=<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="\Pic\XGBoost_Pic\output_10_0.png" alt="png"></p>
<h3 id="3-划分数据集"><a href="#3-划分数据集" class="headerlink" title="3. 划分数据集"></a>3. 划分数据集</h3><p>因为我们至少需要一个训练集来训练模型（KNN则用于最终预测计算），一个测试集来检验模型对新样本的预测能力，而目前只有一个数据集，因此需要对数据集进行划分。划分数据集有很多方法，比如留出法（hold-out）、交叉验证法等，本示例采用较常用的留出法。留出法的实现原理是，按照一定比例将数据集划分为互不相交的两部分，分别作为训练集和测试集。此处选用的训练集、测试集的比例为4:1，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">msk = np.random.rand(len(iris_data)) &lt; <span class="number">0.8</span></span><br><span class="line">train_data_origin = iris_data[msk]</span><br><span class="line">test_data_origin = iris_data[~msk]</span><br><span class="line"></span><br><span class="line">train_data = train_data_origin.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">test_data = test_data_origin.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_label = train_data[<span class="string">"class"</span>]</span><br><span class="line">test_label = test_data[<span class="string">"class"</span>]</span><br><span class="line"></span><br><span class="line">train_fea = train_data.drop(<span class="string">"class"</span>, <span class="number">1</span>)</span><br><span class="line">test_fea = test_data.drop(<span class="string">"class"</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据归一化</span></span><br><span class="line">train_norm = (train_fea - train_fea.min()) / (train_fea.max() - train_fea.min())</span><br><span class="line">test_norm = (test_fea - test_fea.min()) / (test_fea.max() - test_fea.min())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">knn = neighbors.KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line">knn.fit(train_norm, train_label)</span><br><span class="line">predict = knn.predict(test_norm)</span><br><span class="line">accuracy = accuracy_score(test_label, predict)</span><br><span class="line">accuracy</span><br></pre></td></tr></table></figure>
<pre><code>0.9166666666666666
</code></pre><p>KNN算法是机器学习中最简单、有效的算法。上面通过鸢尾花品种分类的示例详细介绍了KNN算法的实现原理和应用。KNN算法属于懒惰学习算法，当数据集的样本容量比较大时，计算量也会比较大，并且需要较大的存储空间。此外，它无法给出数据的任何基础结构信息，后面介绍的算法将会解决这个问题。</p>
<h2 id="2-线性回归预测波士顿房价"><a href="#2-线性回归预测波士顿房价" class="headerlink" title="2. 线性回归预测波士顿房价"></a>2. 线性回归预测波士顿房价</h2><p>下面通过一个示例来说明如何应用线性回归。以波士顿房屋价格数据集作为示例数据集，该数据集包含了波士顿房屋以及周边环境的一些详细信息，包括城镇人均犯罪率、一氧化碳浓度、住宅平均房屋数等。该数据集包含506个样本、13个特征字段、1个label字段</p>
<p><strong>数据描述</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>No</th>
<th>属性</th>
<th>数据类型</th>
<th>字段描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>CRIM</td>
<td>Float</td>
<td>城镇人均犯罪率</td>
</tr>
<tr>
<td>2</td>
<td>ZN</td>
<td>Float</td>
<td>占地面积超过2.5万平方英尺的住宅用地比例</td>
</tr>
<tr>
<td>3</td>
<td>INDUS</td>
<td>Float</td>
<td>城镇非零售业务地区的比例</td>
</tr>
<tr>
<td>4</td>
<td>CHAS</td>
<td>Integer</td>
<td>查尔斯河虚拟变量 (= 1 如果土地在河边；否则是0)</td>
</tr>
<tr>
<td>5</td>
<td>NOX</td>
<td>Float</td>
<td>一氧化氮浓度（每1000万份）</td>
</tr>
<tr>
<td>6</td>
<td>RM</td>
<td>Float</td>
<td>平均每居民房数</td>
</tr>
<tr>
<td>7</td>
<td>AGE</td>
<td>Float</td>
<td>在1940年之前建成的所有者占用单位的比例</td>
</tr>
<tr>
<td>8</td>
<td>DIS</td>
<td>Float</td>
<td>与五个波士顿就业中心的加权距离</td>
</tr>
<tr>
<td>9</td>
<td>RAD</td>
<td>Integer</td>
<td>辐射状公路的可达性指数</td>
</tr>
<tr>
<td>10</td>
<td>TAX</td>
<td>Float</td>
<td>每10,000美元的全额物业税率</td>
</tr>
<tr>
<td>11</td>
<td>PTRATIO</td>
<td>Float</td>
<td>城镇师生比例</td>
</tr>
<tr>
<td>12</td>
<td>B</td>
<td>Float</td>
<td>1000（Bk - 0.63）^ 2其中Bk是城镇黑人的比例</td>
</tr>
<tr>
<td>13</td>
<td>LSTAT</td>
<td>Float</td>
<td>人口中地位较低人群的百分数</td>
</tr>
<tr>
<td>14</td>
<td>MEDV</td>
<td>Float</td>
<td>（目标变量/类别属性）以1000美元计算的自有住房的中位数</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line">boston = load_boston()</span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line">lr = LinearRegression()</span><br><span class="line">lr.fit(X_train, y_train)</span><br><span class="line">y_pred = lr.predict(X_test)</span><br><span class="line">mean_squared_error(y_test, y_pred)</span><br></pre></td></tr></table></figure>
<pre><code>23.380836480270315
</code></pre><p>另外XGBoost也提供了线性回归的API，其数据加载步骤与上述<code>scikit-learn</code>的方法相同，不再赘述。使用XGBoost，首先要把数据转化为其自定义的<code>DMatrix</code>格式，该格式为XGBoost特定的输入格式。然后定义模型参数，此处定义较为简单，只选用了2个参数，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">params = &#123;<span class="string">"objective"</span>:<span class="string">"reg:linear"</span>, <span class="string">"booster"</span>:<span class="string">"gblinear"</span>&#125;</span><br></pre></td></tr></table></figure>
<p>其中，<code>objective</code>用于确定模型的目标函数，这里以<code>reg:squarederror</code>作为目标函数。参数<code>booster</code>用于确定采用什么样的模型，此处选择的是线性模型（gblinear），读者也可根据应用场景选择其他模型（gbtree、dart），因本节主要介绍线性回归，因此选用线性模型。定义好参数后即可训练模型，最后用该模型对测试集进行预测。代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_xgb = xgb.DMatrix(X_train, y_train)</span><br><span class="line">params = &#123;<span class="string">"objective"</span>:<span class="string">"reg:squarederror"</span>, <span class="string">"booster"</span>:<span class="string">"gblinear"</span>&#125;</span><br><span class="line">model = xgb.train(dtrain=train_xgb, params=params)</span><br><span class="line">y_pred = model.predict(xgb.DMatrix(X_test))</span><br></pre></td></tr></table></figure>
<p>综上，线性回归是一种解决回归问题的常见方法。在线性回归中，求解最优参数的方法是最小化其损失函数。最小化损失函数有两种方法：<strong>正规方程和梯度下降法</strong>。</p>
<p>正规方程通过矩阵运算求得最优参数，但其必须满足$X^TX$可逆，当样本数比特征数还少时，$X^TX$的逆是不能直接计算的。</p>
<p>梯度下降法是沿负梯度的方向一步步最小化损失函数，求解最优参数。梯度下降法需要指定步长并进行多次迭代，但相比于正规方程，梯度下降法可以应用于特征数较大的情况。最后，通过波士顿房价的示例展示了通过scikit-learn和XGBoost如何应用线性回归。</p>
<h2 id="3-逻辑回归预测良性-恶性乳腺肿瘤"><a href="#3-逻辑回归预测良性-恶性乳腺肿瘤" class="headerlink" title="3. 逻辑回归预测良性/恶性乳腺肿瘤"></a>3. 逻辑回归预测良性/恶性乳腺肿瘤</h2><p>下面将使用逻辑回归预测乳腺肿瘤是良性的还是恶性的。示例采用的数据集为威斯康星诊断乳腺癌数据集，它通过细胞核的相关特征来预测乳腺肿瘤为良性/恶性，这是一个非常著名的二分类数据集。该数据集包含569个样本，其中有212个恶性肿瘤样本，357个良性肿瘤样本。共有32个字段，字段1为ID，字段2为label，其他30个字段为细胞核的相关特征，例如：</p>
<ul>
<li>半径（从中心到周边点的平均距离）</li>
<li>纹理（灰度值的标准偏差）</li>
<li>周长</li>
<li>面积</li>
<li>光滑度（半径长度的局部变化）</li>
<li>紧凑性（周长的二次方/面积的负一次方）</li>
<li>凹度（轮廓的凹陷程度）</li>
<li>凹点（轮廓中凹部的数量）</li>
<li>对称</li>
<li>分形维数</li>
</ul>
<p>对于每张图像，分别计算以上10个特征的平均值、标准误差和最差/最大（最大的3个值的平均）值，由此生成30个特征。例如，字段3表示平均半径，字段13表示半径的标准误差，字段23表示最差半径。所有特征都保留4位有效数字。</p>
<p>scikit-learn已经集成了该数据集，并进行了相应的处理（如去掉了ID字段），使用时直接加载即可，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X = cancer.data</span><br><span class="line">y = cancer.target</span><br></pre></td></tr></table></figure>
<p>其中，X为特征数据，包含上面介绍的30个特征，y为标签数据，标记乳腺肿瘤类型，1代表良性，0代表恶性。下面按4:1的比例将数据集划分为训练集和测试集：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr.fit(X_train, y_train)</span><br><span class="line">y_pred = lr.predict(X_test)</span><br><span class="line">print(classification_report(y_test, y_pred, target_names=[<span class="string">"Benign"</span>, <span class="string">"Malignant"</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>              precision    recall  f1-score   support

      Benign       0.95      0.90      0.93        42
   Malignant       0.95      0.97      0.96        72

    accuracy                           0.95       114
   macro avg       0.95      0.94      0.94       114
weighted avg       0.95      0.95      0.95       114
</code></pre><p>其中，列表的左边一列为分类的标签名，<code>avg/total</code>为各列的均值。<code>support</code>表示该类别样本出现的次数。</p>
<p>XGBoost提供了逻辑回归的API，读者可以通过XGBoost中的逻辑回归对数据集进行预测，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_xgb = xgb.DMatrix(X_train, y_train)</span><br><span class="line">params = &#123;<span class="string">"objective"</span>:<span class="string">"reg:logistic"</span>, <span class="string">"booster"</span>:<span class="string">"gblinear"</span>&#125;</span><br><span class="line">model = xgb.train(dtrain=train_xgb, params=params)</span><br><span class="line">y_pred = model.predict(xgb.DMatrix(X_test))</span><br></pre></td></tr></table></figure>
<p>XGBoost逻辑回归API的调用方式和线性回归类似，唯一不同的是目标函数<code>objective</code>改为<code>reg:logistic</code>，<code>booster</code>仍然选择线性模型。</p>
<p>注意，XGBoost在预测结果上和scikit-learn有些差别，XGBoost的预测结果是概率，而scikit-learn的预测结果是0或1的分类（scikit-learn也可通过<code>predict_proba</code>输出概率）。在XGBoost中，如果需要输出0或1的分类，需要用户自己对其进行转化，例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ypred_bst = np.array(y_pred)</span><br><span class="line">y_pred_bst = ypred_bst &gt; <span class="number">0.5</span></span><br><span class="line">y_pred_bst = y_pred_bst.astype(int)</span><br><span class="line">print(classification_report(y_test, y_pred_bst, target_names=[<span class="string">"Benign"</span>, <span class="string">"Malignant"</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>              precision    recall  f1-score   support

      Benign       0.90      0.67      0.77        42
   Malignant       0.83      0.96      0.89        72

    accuracy                           0.85       114
   macro avg       0.87      0.81      0.83       114
weighted avg       0.86      0.85      0.84       114
</code></pre><h2 id="4-决策树解决肿瘤分类问题"><a href="#4-决策树解决肿瘤分类问题" class="headerlink" title="4. 决策树解决肿瘤分类问题"></a>4. 决策树解决肿瘤分类问题</h2><p>scikit-learn实现了决策树算法，它采用的是一种优化的CART版本，既可以解决分类问题，也可以解决回归问题。分类问题使用DecisionTreeClassifier类，回归问题使用DecisionTreeRegressor类。两个类的参数相似，只有部分有所区别，以下是对主要参数的说明。</p>
<ol>
<li><code>criterion</code>：特征选择采用的标准。DecisionTreeClassifier分类树默认采用<code>gini</code>（基尼系数）进行特征选择，也可以使用<code>entropy</code>（信息增益）。DecisionTreeRegressor默认采用MSE（均方误差），也可以使用MAE（平均绝对误差）。</li>
<li><code>splitter</code>：节点划分的策略。支持<code>best</code>和<code>random</code>两种方式，默认为<code>best</code>，即选取所有特征中最优的切分点作为节点的分裂点，<code>random</code>则随机选取部分切分点，从中选取局部最优的切分点作为节点的分裂点。</li>
<li><code>max_depth</code>：树的最大深度，默认为None，表示没有最大深度限制。节点停止分裂的条件是：样本均属于相同类别或所有叶子节点包含的样本数量小于$min_samples_split$。若将该参数设置为None以外的其他值，则决策树生成过程中达到该阈值深度时，节点停止分裂。</li>
<li><code>min_samples_split</code>：节点划分的最小样本数，默认为2。若节点包含的样本数小于该值，则该节点不再分裂。若该字段设置为浮点数，则表示最小样本百分比，划分的最小样本数为$ceil（min_samples_split*n_samples）$。</li>
<li><code>min_samples_leaf</code>：叶子节点包含的最小样本数，默认为1。此字段和<code>min_samples_split</code>类似，取值可以是整型，也可以是浮点型。整型表示一个叶子节点包含的最小样本数，浮点型则表示百分比。叶子节点包含的最小样本数为$ceil（min_samples_leaf*n_samples）$。</li>
<li><code>max_features</code> ：划分节点时备选的最大特征数，默认为None，表示选用所有特征。若该字段为整数，表示选用的最大特征数；若为浮点数，则表示选用特征的最大百分比。最大特征数为$int（max_features*n_features）$。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X = cancer.data</span><br><span class="line">y = cancer.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">clf = tree.DecisionTreeClassifier(max_depth=<span class="number">4</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line">print(classification_report(y_test, y_pred, target_names=[<span class="string">"Benign"</span>, <span class="string">"Malignant"</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>              precision    recall  f1-score   support

      Benign       0.95      0.87      0.91        46
   Malignant       0.92      0.97      0.94        68

    accuracy                           0.93       114
   macro avg       0.93      0.92      0.93       114
weighted avg       0.93      0.93      0.93       114
</code></pre><p>为便于读者直观地理解树模型，可以使用Graphviz工具包将模型可视化。Graphviz是一个开源的图形可视化软件，可以将结构数据转化为形象的图形或网络，在软件工程、数据库、机器学习等领域的可视化界面中有应用。函数<code>export_graphviz</code>可以将<code>scikit-learn</code>中的决策树导出为Graphviz的格式，导出完成后即可对Graphviz格式的决策树进行图形渲染，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line">dot_data = tree.export_graphviz(clf, out_file=<span class="literal">None</span>,</span><br><span class="line">                               feature_names=cancer.feature_names,</span><br><span class="line">                               class_names=cancer.target_names,</span><br><span class="line">                               filled=<span class="literal">True</span>, rounded=<span class="literal">True</span>,</span><br><span class="line">                               special_characters=<span class="literal">True</span>)</span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line"><span class="comment"># graph</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将dot_data写入到txt文件中</span></span><br><span class="line">f = open(<span class="string">'output/img/dot_data.txt'</span>, <span class="string">'w'</span>) </span><br><span class="line">f.write(dot_data) </span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解决中文乱码问题</span></span><br><span class="line"><span class="comment"># import re </span></span><br><span class="line"><span class="comment"># f_old = open('dot_data.txt', 'r') </span></span><br><span class="line"><span class="comment"># f_new = open('dot_data_new.txt', 'w', encoding='utf-8') </span></span><br><span class="line"><span class="comment"># for line in f_old: </span></span><br><span class="line"><span class="comment">#     if 'fontname' in line:</span></span><br><span class="line"><span class="comment">#         font_re = 'fontname=(.*?)]'</span></span><br><span class="line"><span class="comment">#     old_font = re.findall(font_re, line)[0]</span></span><br><span class="line"><span class="comment">#     line = line.replace(old_font, 'SimHei')</span></span><br><span class="line"><span class="comment">#     f_new.write(line)</span></span><br><span class="line"><span class="comment">#     f_old.close()</span></span><br><span class="line"><span class="comment">#     f_new.close()</span></span><br></pre></td></tr></table></figure>
<h2 id="5-神经网络识别手写体数字"><a href="#5-神经网络识别手写体数字" class="headerlink" title="5. 神经网络识别手写体数字"></a>5. 神经网络识别手写体数字</h2><p>手写体数字数据集（MNIST）是一个经典的多分类数据集，由不同的手写体数字图片以及0～9的数字标签样本构成。scikit-learn中的手写体数字数据集共有1797个样本，每个样本包含一个8×8像素的图像和0～9的数字标签。scikit-learn通过<code>MLPClassifier</code>类实现的多层感知器完成分类任务，通过<code>MLPRegressor</code>类完成回归任务。对于手写体数字数据集这样的多分类问题，显然要采用<code>MLPClassifier</code>。<code>MLPClassifier</code>的常用参数如下:</p>
<ul>
<li><code>hidden_layer_sizes</code>：用来指定隐藏层包含的节点数量，其类型为tuple，长度是<code>n_layers-2</code>，其中n_layers为网络总层数；</li>
<li><code>activation</code>：指定隐藏层的激活函数，默认为relu；</li>
<li><code>solver</code>：指定权重的更新方法，默认为sgd，即随机梯度下降法；</li>
<li><code>alpha</code>：指定L2正则的惩罚系数；</li>
<li><code>learning_rate</code>：指定训练过程中学习率更新方法，有constant、invscaling和adaptive这3种方法。其中，constant表示学习率在训练过程中为固定值；invscaling表示随着训练的进行，学习率指数降低；adaptive表示动态调整，当训练误差不断减少时（减少量超过一定阈值），学习率保持不变，若连续两次迭代训练损失未达到上述条件，则学习率缩小为原值的1/5。</li>
<li><code>max_iter</code>表示迭代的最大轮数，对于solver为sgd和adam的情况，<code>max_iter</code>相当于epoch的数量。</li>
</ul>
<p>了解了<code>MLPClassifier</code>类的常用参数后，下面介绍如何使用<code>MLPClassifier</code>来解决识别手写体数字的问题。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line">digits = load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">mlp = MLPClassifier(hidden_layer_sizes=(<span class="number">128</span>, <span class="number">64</span>), max_iter=<span class="number">50</span>, alpha=<span class="number">1e-4</span>, solver=<span class="string">'sgd'</span>)</span><br><span class="line">mlp.fit(X_train, y_train)</span><br><span class="line">y_pred = mlp.predict(X_test)</span><br><span class="line">print(<span class="string">"Accuracy: "</span>, accuracy_score(y_test, y_pred))</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy:  0.9555555555555556
</code></pre><h2 id="6-支持向量机识别手写体数字"><a href="#6-支持向量机识别手写体数字" class="headerlink" title="6. 支持向量机识别手写体数字"></a>6. 支持向量机识别手写体数字</h2><p>下面仍以手写体数字数据集（MNIST）为例，介绍如何使用SVM解决分类问题。SVM既可以解决二分类问题，也能解决多分类问题。SVM解决多分类问题的方法主要有两种：one-vs-one和one-vs-the-rest。</p>
<ul>
<li><code>one-vs-one</code>为每两类样本建立一个二分类器，则$k$个类别的样本需要建立$\frac{k(k-1)}{2}$个二分类器。</li>
<li><code>one-vs-the-rest</code>是为每个类别和其他剩余类别建立一个二分类器，从中选择预测概率最大的分类作为最终分类，k个类别的样本需建立k个二分类器。</li>
</ul>
<p>scikit-learn通过SVC类来解决分类问题，通过SVR类来解决回归问题（SVM也可以解决回归问题），下面采用SVC类解决手写体数字识别的多分类问题。</p>
<p>SVC可以通过参数kernel指定采用的核函数，支持的核函数有：<code>linear</code>（线性核函数）、<code>poly</code>（多项式）、<code>rbf</code>（高斯）、<code>sigmoid</code>、<code>precomputed</code>以及自定义形式<code>callable</code>。若不指定kernel，其默认采用<code>rbf</code>。SVC还有几个比较常用的参数：</p>
<ul>
<li>惩罚参数$C$，即前面松弛变量中介绍的不满足约束条件样本的惩罚系数；</li>
<li>参数<code>degree</code>是多项式核函数（kernel设置为<code>poly</code>）的阶数；</li>
<li>参数gamma表示高斯核和sigmoid核中的内核系数，在高斯核中对应的是高斯核函数公式中的$\frac{1}{2\sigma^2}$。</li>
</ul>
<p>数据集的加载和划分同神经网络中的示例，不再赘述。此处主要介绍模型拟合与评估。</p>
<p>先定义一个SVC模型，这里采用高斯核函数，惩罚系数C为1.0，gamma为0.001，当然也可以通过参数调优来确定参数。定义模型之后即可训练模型，然后对测试集进行预测，最后以准确率为指标评估预测结果。具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">svc = svm.SVC(C=<span class="number">1.0</span>, kernel=<span class="string">"rbf"</span>, gamma=<span class="number">0.001</span>)</span><br><span class="line">svc.fit(X_train, y_train)</span><br><span class="line">y_pred = svc.predict(X_test)</span><br><span class="line">print(<span class="string">"Accuracy"</span>, accuracy_score(y_test, y_pred))</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy 0.9861111111111112
</code></pre><p>也可以采用其他核函数，如多项式核函数，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">svc = svm.SVC(C=<span class="number">1.0</span>, kernel=<span class="string">"poly"</span>, degree=<span class="number">3</span>)</span><br><span class="line">svc.fit(X_train, y_train)</span><br><span class="line">y_pred = svc.predict(X_test)</span><br><span class="line">print(<span class="string">"Accuracy"</span>, accuracy_score(y_test, y_pred))</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy 0.9861111111111112
</code></pre><p>可以看到，在本例中采用多项式核函数和高斯核函数的预测准确率是相同的。读者也可自行尝试其他参数，观察不同参数对模型预测的影响。</p>
]]></content>
      <categories>
        <category>XGBoost</category>
      </categories>
  </entry>
  <entry>
    <title>MySQL入门（九）DML语言</title>
    <url>/posts/dda6bdb1.html</url>
    <content><![CDATA[<p>本文使用的数据集下载链接为：<a href="https://pan.baidu.com/s/18_iwB072qpwUEvhxnF83qA" target="_blank" rel="noopener">https://pan.baidu.com/s/18_iwB072qpwUEvhxnF83qA</a> ，提取码：by2k </p>
<p>DML语言指的是数据操作语言，包含数据的插入（insert）、修改（update）和删除（delete）</p>
<h1 id="一、插入语句"><a href="#一、插入语句" class="headerlink" title="一、插入语句"></a>一、插入语句</h1><p>MySQL 表中使用 <strong>INSERT INTO</strong> SQL语句来插入数据。</p>
<h2 id="1-语法"><a href="#1-语法" class="headerlink" title="1. 语法"></a>1. 语法</h2><h3 id="方式一"><a href="#方式一" class="headerlink" title="方式一"></a>方式一</h3><p>以下为向MySQL数据表插入数据通用的 <strong>INSERT INTO</strong> SQL语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INSERT INTO table_name ( field1, field2,...fieldN )</span><br><span class="line">                       VALUES</span><br><span class="line">                       ( value1, value2,...valueN );</span><br></pre></td></tr></table></figure>
<p>如果数据是字符型，必须使用单引号或者双引号，如：”value”。</p>
<p><strong>注意</strong></p>
<ol>
<li>插入的值的类型要与列的类型一致或兼容，个数也需要一致</li>
<li>创建表时默认数据类型为<code>Nullable</code>，意思为选填，可以为空</li>
<li><code>Nullable</code>列不想设置值可以赋<code>NULL</code>或什么都不写</li>
<li>列的顺序可以调换，但需要一一对应</li>
<li>可以省略列名，默认所有列，而且列的顺序与表中一致，此时为空的值需用NULL填充</li>
</ol>
<h3 id="方式二"><a href="#方式二" class="headerlink" title="方式二"></a>方式二</h3><p>语法为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INSERT INTO 表名</span><br><span class="line">SET 列名&#x3D;值, 列名&#x3D;值, ...</span><br></pre></td></tr></table></figure>
<h2 id="2-示例"><a href="#2-示例" class="headerlink" title="2. 示例"></a>2. 示例</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 方式一：</span><br><span class="line">INSERT INTO beauty(id, NAME, sex, borndate, phone, photo, boyfriend_id)</span><br><span class="line">VALUES(13, &quot;Melody Marks&quot;, &quot;女&quot;, &#39;2000-2-29&#39;, &#39;12345678&#39;, NULL, 2);</span><br><span class="line"></span><br><span class="line"># 方式二：</span><br><span class="line">INSERT INTO beauty</span><br><span class="line">SET id&#x3D;19, NAME&#x3D;&quot;刘涛&quot;, phone&#x3D;&#39;99999&#39;;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title>MySQL入门（八）子查询</title>
    <url>/posts/9a307df7.html</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title>MySQL入门（七）连接查询</title>
    <url>/posts/2517aedb.html</url>
    <content><![CDATA[<h1 id="一、连接查询基础"><a href="#一、连接查询基础" class="headerlink" title="一、连接查询基础"></a>一、连接查询基础</h1><p>我们已经学会了如何在一张表中读取数据，这是相对简单的，但是在真正的应用中经常需要从多个数据表中读取数据。现在将向大家介绍如何使用 <code>MySQL</code> 的<code>JOIN</code> 在两个或多个表中查询数据。</p>
<p>你可以在 <code>SELECT</code>, <code>UPDATE</code> 和 <code>DELETE</code> 语句中使用 Mysql 的 <code>JOIN</code> 来联合多表查询。<code>JOIN</code> 按照功能大致分为如下三类：</p>
<ul>
<li><strong>INNER JOIN（内连接,或等值连接）</strong>：获取两个表中字段匹配关系的记录。</li>
<li><strong>LEFT JOIN（左连接）：</strong>获取左表所有记录，即使右表没有对应匹配的记录。</li>
<li><strong>RIGHT JOIN（右连接）：</strong> 与 LEFT JOIN 相反，用于获取右表所有记录，即使左表没有对应匹配的记录。</li>
</ul>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title>MySQL入门（六）分组查询</title>
    <url>/posts/ceaf7866.html</url>
    <content><![CDATA[<p>本文数据集下载链接：<a href="https://pan.baidu.com/s/1sVcSXfVZimc6ruyMr4Gr3g" target="_blank" rel="noopener">https://pan.baidu.com/s/1sVcSXfVZimc6ruyMr4Gr3g</a> ，提取码：7gfa </p>
<h1 id="一、分组函数简介"><a href="#一、分组函数简介" class="headerlink" title="一、分组函数简介"></a>一、分组函数简介</h1><p><code>GROUP BY</code> 语句根据一个或多个列对结果集进行分组，在分组的列上我们可以使用 <code>COUNT, SUM, AVG</code>等函数。</p>
<p><strong>功能</strong>：用于统计，又称为聚合函数或统计函数或组函数</p>
<p><strong>分类</strong>：<code>sum</code>求和，<code>avg</code>平均值，<code>max</code>最大值，<code>min</code>最小值，<code>count</code>计算个数</p>
<p><strong>特点</strong>：</p>
<ol>
<li><code>SUM, AVG</code>一般处理数值型， <code>MAX, MIN, COUNT</code>可以处理任何类型</li>
<li>是否忽略<code>NULL</code>值：所有分组函数都忽略<code>NULL</code>值</li>
<li>可以和<code>DISTINCT</code>搭配实现去重运算</li>
<li><code>COUNT</code>函数的专门介绍</li>
</ol>
<h2 id="1-简单使用"><a href="#1-简单使用" class="headerlink" title="1. 简单使用"></a>1. 简单使用</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT SUM(salary) FROM employees;</span><br><span class="line">SELECT AVG(salary) FROM employees;</span><br><span class="line">SELECT MIN(salary) FROM employees;</span><br><span class="line">SELECT MAX(salary) FROM employees;</span><br><span class="line">SELECT COUNT(salary) FROM employees; </span><br><span class="line">SELECT SUM(salary) 和, ROUND(AVG(salary), 2) 平均, MAX(salary) 最高, MIN(salary) 最低, COUNT(salary) FROM employees;</span><br></pre></td></tr></table></figure>
<h2 id="2-参数类型"><a href="#2-参数类型" class="headerlink" title="2. 参数类型"></a>2. 参数类型</h2><p>以下为无意义但不报错的使用方式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT SUM(last_name), AVG(last_name) </span><br><span class="line">FROM employees;</span><br></pre></td></tr></table></figure>
<p>同理还有对日期求和等，也是无意义的。以下使用方式是合理的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT MAX(last_name), MIN(last_name) </span><br><span class="line">FROM employees;</span><br></pre></td></tr></table></figure>
<p>这是因为可以按照首字母排序，于是也有最大值和最小值，同理对日期求最大最小值也是可以的</p>
<p>也可以使用<code>COUNT</code>语句对其他类型求和：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(commission_pct), COUNT(last_name) </span><br><span class="line">FROM employees;</span><br><span class="line"># 返回35和107</span><br></pre></td></tr></table></figure>
<p>两者不同是因为<code>COUNT</code>返回 不为NULL的个数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT SUM(DISTINCT salary), sum(salary) </span><br><span class="line">FROM employees;</span><br></pre></td></tr></table></figure>
<h2 id="3-COUNT函数详解"><a href="#3-COUNT函数详解" class="headerlink" title="3. COUNT函数详解"></a>3. <code>COUNT</code>函数详解</h2><p><strong>统计总行数</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(*) </span><br><span class="line">FROM employees;</span><br></pre></td></tr></table></figure>
<p>每一行中只要有不为<code>NULL</code>的计数器就加一，同理也可以使用以下方式统计总行数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(1) </span><br><span class="line">FROM employees;</span><br></pre></td></tr></table></figure>
<p>事实上，<code>COUNT()</code>中的常量值可以取任何值获得同样的效果，但就效率而言</p>
<ul>
<li>在MYISAM存储引擎下，<code>COUNT(*)</code>的效率高</li>
<li>在INNODB存储引擎下，<code>COUNT(*)</code>的效率和<code>COUNT(1)</code>差不多，但比<code>COUNT(字段)</code>效率高</li>
</ul>
<p>因此一般使用<code>COUNT(*)</code>来统计行数</p>
<p><strong>注意：和分组函数一同查询的字段有限制(要求是group by后的字段)</strong></p>
<h1 id="二、分组查询"><a href="#二、分组查询" class="headerlink" title="二、分组查询"></a>二、分组查询</h1><ul>
<li>筛选条件可分为两类：分组前筛选和分组后筛选</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><strong>数据源</strong></th>
<th><strong>位置</strong></th>
<th><strong>关键字</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>分组前筛选</strong></td>
<td>原始表</td>
<td><code>GROUP BY</code>子句的前面</td>
<td><code>WHERE</code></td>
</tr>
<tr>
<td><strong>分组后筛选</strong></td>
<td>分组后的结果集</td>
<td><code>GROUP BY</code>子句的后面</td>
<td><code>HAVING</code></td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p>GROUP BY`子句支持单个字段分组、多个字段分组（多个字段之间用逗号隔开，没有顺序要求）</p>
</li>
<li><p>也可以添加排序（放在整个分组查询最后）</p>
</li>
</ul>
<p>注：</p>
<ol>
<li>分组函数做条件肯定是放在HAVING子句中</li>
<li>能用分组前筛选的优先使用分组前筛选</li>
</ol>
<h2 id="1-GROUP-BY语法"><a href="#1-GROUP-BY语法" class="headerlink" title="1. GROUP BY语法"></a>1. <code>GROUP BY</code>语法</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT column_name, function(column_name)</span><br><span class="line">FROM table_name</span><br><span class="line">WHERE column_name operator value</span><br><span class="line">GROUP BY column_name;</span><br></pre></td></tr></table></figure>
<h2 id="2-具体示例"><a href="#2-具体示例" class="headerlink" title="2. 具体示例"></a>2. 具体示例</h2><p><strong>查询每个部门的平均工资</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT department_id, AVG(salary) </span><br><span class="line">FROM employees </span><br><span class="line">GROUP BY department_id;</span><br></pre></td></tr></table></figure>
<p><strong>查询每个工种有奖金的员工的最高工资</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT MAX(salary), job_id </span><br><span class="line">FROM employees </span><br><span class="line">WHERE commission_pct IS NOT NULL </span><br><span class="line">GROUP BY job_id;</span><br></pre></td></tr></table></figure>
<p><strong>根据上一题查询结果筛选最高工资&gt;12000</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT MAX(salary), job_id </span><br><span class="line">FROM employees </span><br><span class="line">WHERE commission_pct IS NOT NULL </span><br><span class="line">GROUP BY job_id </span><br><span class="line">HAVING MAX(salary)&gt;12000;</span><br></pre></td></tr></table></figure>
<p><strong>查询领导编号&gt;102的每个领导手下的最低工资&gt;5000的领导编号是哪个，以及其最低工资</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT MIN(salary), manager_id </span><br><span class="line">FROM employees </span><br><span class="line">WHERE manager_id&gt;102</span><br><span class="line">GROUP BY manager_id </span><br><span class="line">HAVING MIN(salary)&gt;5000</span><br></pre></td></tr></table></figure>
<p><strong>按表达式（函数）分组：按员工姓名的长度分组，查询每一组的员工个数，筛选员工个数&gt;5的有哪些</strong></p>
<p>① 查询每个长度的员工个数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(*), LENGTH(last_name) len_name </span><br><span class="line">FROM employees </span><br><span class="line">GROUP BY LENGTH(last_name);</span><br></pre></td></tr></table></figure>
<p>② 添加筛选条件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(*), LENGTH(last_name) len_name </span><br><span class="line">FROM employees </span><br><span class="line">GROUP BY LENGTH(last_name) </span><br><span class="line">HAVING COUNT(*) &gt; 5;</span><br></pre></td></tr></table></figure>
<p><strong>按多个字段分组</strong></p>
<p>案例：查询每个部门每个工种的员工的平均工资</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT AVG(salary), department_id, job_id </span><br><span class="line">FROM employees </span><br><span class="line">GROUP BY department_id, job_id;</span><br></pre></td></tr></table></figure>
<p><strong>添加排序</strong></p>
<p>案例：查询部门编号不为NULL的每个工种的员工的平均工资，并按工资高低显示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT AVG(salary), department_id, job_id </span><br><span class="line">FROM employees </span><br><span class="line">WHERE department_id IS NOT NULL </span><br><span class="line">GROUP BY department_id, job_id </span><br><span class="line">ORDER BY AVG(salary) DESC;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title>MySQL入门（五）函数</title>
    <url>/posts/a5e4959e.html</url>
    <content><![CDATA[<p>本文数据集下载链接：<a href="https://pan.baidu.com/s/1sVcSXfVZimc6ruyMr4Gr3g" target="_blank" rel="noopener">https://pan.baidu.com/s/1sVcSXfVZimc6ruyMr4Gr3g</a> ，提取码：7gfa </p>
<h1 id="一、常见函数"><a href="#一、常见函数" class="headerlink" title="一、常见函数"></a>一、常见函数</h1><p><strong>调用方法</strong>：<code>SELECT 函数名(实参列表) [FROM 表]</code></p>
<p><strong>优点</strong>：隐藏实现细节；提高代码的重用性</p>
<p><strong>分类</strong>：</p>
<ol>
<li>单行函数，如<code>concat</code>、<code>length</code>、<code>ifnull</code></li>
<li>分组函数，传递一组值进去，传出一个值（又称统计函数）</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT DATABASE();		# 查看当前DATABASE</span><br><span class="line">SELECT USER();</span><br><span class="line">SELECT VERSION();</span><br></pre></td></tr></table></figure>
<h1 id="二、单行函数"><a href="#二、单行函数" class="headerlink" title="二、单行函数"></a>二、单行函数</h1><h2 id="1-字符函数"><a href="#1-字符函数" class="headerlink" title="1. 字符函数"></a>1. 字符函数</h2><p><strong>函数1：<code>LENGTH</code></strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT LENGTH(&quot;MySQL&quot;);</span><br><span class="line">SELECT LENGTH(&quot;MySQL真简单&quot;);</span><br></pre></td></tr></table></figure>
<p>第一个显示为5，第二个显示为14（一个汉字占3个字节），由此我们知道<code>LENGTH</code>函数是用于获取参数值的字节个数的</p>
<p><strong>函数2：<code>CONCAT</code>拼接函数</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT CONCAT(last_name, &#39;_&#39;, first_name) 姓名 FROM employees;</span><br></pre></td></tr></table></figure>
<p><strong>函数3：<code>UPPER、LOWER</code></strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT UPPER(&#39;MySQL&#39;);</span><br><span class="line">SELECT LOWER(&#39;MySQL&#39;);</span><br></pre></td></tr></table></figure>
<p><strong>套娃1：将姓大写，名小写然后拼接</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT CONCAT(UPPER(last_name),LOWER(first_name)) FROM employees;</span><br></pre></td></tr></table></figure>
<p>由此可见函数可以嵌套调用</p>
<p><strong>函数4：<code>SUBSTR\SUBSTRING</code></strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT SUBSTR(&#39;我喜欢MySQL&#39;, 4) output;</span><br></pre></td></tr></table></figure>
<p>该函数返回<code>MySQL</code>，由此我们可见该函数的作用是返回索引及其之后的内容，同时我们可以发现MySQL中的<strong>索引是从0开始</strong>的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT SUBSTR(&#39;我喜欢你&#39;, 2, 4) output;</span><br></pre></td></tr></table></figure>
<p>该函数返回<code>喜欢你</code>，这里是截取从指定索引处指定字符长度的字符</p>
<p><strong>套娃2：姓名中首字符大写，其他字符小写，并用_拼接显示出来</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT CONCAT(UPPER(SUBSTR(last_name, 1, 1)),&#39;_&#39;,LOWER(SUBSTR(last_name, 2))) output FROM employees;</span><br></pre></td></tr></table></figure>
<p><strong>函数5：<code>instr</code>返回起始索引</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT INSTR(&#39;我喜欢MySQL&#39;, &#39;MySQL&#39;) AS output;</span><br></pre></td></tr></table></figure>
<p>返回结果为4</p>
<p>注意：如果找不到对应索引，返回0</p>
<p><strong>函数6：<code>trim</code>去首尾空格</strong></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">SELECT TRIM(<span class="string">'    MySQL   '</span>) AS output;</span><br></pre></td></tr></table></figure>
<p>实际上也可以去除前后某个指定的元素，比如我们要去除下例中首尾的<code>a</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT TRIM(&#39;a&#39; FROM &#39;aaaaaaaMySQLaaaa&#39;) AS output;</span><br></pre></td></tr></table></figure>
<p><strong>函数7：<code>lpad</code>用指定字符实现左填充指定长度</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT LPAD(&#39;MySQL&#39;, 10, &#39;*&#39;) AS output;</span><br></pre></td></tr></table></figure>
<p>返回结果为<code>*****MySQL</code>，若我们将长度指定为小于字段长的数字，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT LPAD(&#39;MySQL&#39;, 3, &#39;*&#39;) AS output;</span><br></pre></td></tr></table></figure>
<p>返回值为<code>MyS</code></p>
<p><strong>函数8：<code>rpad</code>用指定字符右填充指定长度</strong></p>
<p>用法与左填充一样</p>
<p><strong>函数9：<code>replace</code>替换</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT REPLACE(&#39;我爱MySQL&#39;, &#39;爱&#39;, &#39;讨厌&#39;) AS output;</span><br></pre></td></tr></table></figure>
<p>结果很容易猜到，嘿嘿这里就不说啦。而且注意哦，这里的替换是全部替换，可以自己验证一下</p>
<h2 id="2-数学函数"><a href="#2-数学函数" class="headerlink" title="2. 数学函数"></a>2. 数学函数</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">round: 四舍五入</span><br><span class="line">*&#x2F;</span><br><span class="line">SELECT ROUND(-1.55);		# 输出结果为-2</span><br><span class="line">SELECT ROUND(1.467,2);		# 输出结果为1.47</span><br><span class="line"></span><br><span class="line">&#x2F;*</span><br><span class="line">ceil: 向上取整(返回大于等于该数的最小整数)</span><br><span class="line">floor: 向下取整(返回小于等于该数的最大整数)</span><br><span class="line">*&#x2F;</span><br><span class="line">SELECT CEIL(1.01);			# 输出结果为2</span><br><span class="line">SELECT FLOOR(-9.99);		# 输出结果为-10</span><br><span class="line"></span><br><span class="line">&#x2F;*</span><br><span class="line">truncate: 截断</span><br><span class="line">*&#x2F;</span><br><span class="line">SELECT TRUNCATE(1.65, 1);	# 输出结果为1.6</span><br><span class="line"></span><br><span class="line">&#x2F;*</span><br><span class="line">mod: 取余</span><br><span class="line">*&#x2F;</span><br><span class="line">SELECT MOD(10, 3);			# 输出结果为1</span><br><span class="line"># 被除数如果是正则为正，如果是负则为负，因为运算方式为：MOD(a,b)&#x3D;a-a&#x2F;b*b</span><br></pre></td></tr></table></figure>
<h2 id="3-日期函数"><a href="#3-日期函数" class="headerlink" title="3. 日期函数"></a>3. 日期函数</h2><h3 id="3-1-获取日期的函数"><a href="#3-1-获取日期的函数" class="headerlink" title="3.1 获取日期的函数"></a>3.1 获取日期的函数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># now: 返回当前系统日期+时间</span><br><span class="line">SELECT NOW();</span><br><span class="line"></span><br><span class="line"># curdate: 返回当前系统日期，不包含时间</span><br><span class="line">SELECT CURDATE();</span><br><span class="line"></span><br><span class="line"># curtime: 返回当前系统时间，不包含日期</span><br><span class="line">SELECT CURTIME();</span><br><span class="line"></span><br><span class="line"># 也可以自己定其他截取的时间特征:</span><br><span class="line">SELECT YEAR(NOW()); 		# 返回今年</span><br><span class="line"># 若希望返回英文：</span><br><span class="line">SELECT MONTHNAME(NOW());</span><br></pre></td></tr></table></figure>
<h3 id="3-2-转换日期的函数"><a href="#3-2-转换日期的函数" class="headerlink" title="3.2 转换日期的函数"></a>3.2 转换日期的函数</h3><p><strong><code>STR_TO_DATE</code></strong></p>
<p>该函数的作用是按日期格式的字符转换成指定格式的日期，示例如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT STR_TO_DATE(&#39;1999&#x2F;2&#x2F;18&#39;,&#39;%Y&#x2F;%m&#x2F;%d&#39;);</span><br></pre></td></tr></table></figure>
<p>返回<code>1999-02-18</code>，小伙伴可以自己尝试大小写的区别，并自行查阅其他格式符的含义和功能</p>
<p><strong><code>DATE_FORMAT</code></strong></p>
<p>该函数的作用是将日期转换成字符，恰好与<code>STR_TO_DATE</code>反过来，示例如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT DATE_FORMAT(NOW(),&#39;%Y年%m月%d日&#39;);</span><br></pre></td></tr></table></figure>
<p><strong>案例1：查询入职日期是1992年4月3号的员工信息</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * </span><br><span class="line">FROM employees </span><br><span class="line">WHERE hiredate &#x3D; STR_TO_DATE(&#39;1992年4月3日&#39;,&#39;%Y年%m月%d日&#39;);</span><br></pre></td></tr></table></figure>
<p><strong>案例2：查询有奖金的员工名及入职日期，要求格式为：xx月/xx日/xx年</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name, DATE_FORMAT(hiredate,&#39;%m月&#x2F;%d日 %y年&#39;) 入职日期 </span><br><span class="line">FROM employees </span><br><span class="line">WHERE commission_pct IS NOT NULL;</span><br></pre></td></tr></table></figure>
<h1 id="三、流程控制函数"><a href="#三、流程控制函数" class="headerlink" title="三、流程控制函数"></a>三、流程控制函数</h1><h2 id="1-IF函数"><a href="#1-IF函数" class="headerlink" title="1. IF函数"></a>1. <code>IF</code>函数</h2><p><code>IF</code>函数有三个参数，第一个表达式的结果若为 true，则返回表达式二的值，否则返回表达式三的值，如下将返回21</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT IF(10&gt;5, 21, 10);</span><br></pre></td></tr></table></figure>
<p><strong>示例：若员工有奖金则提示有，否则提示没有</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name, commission_pct, IF(commission_pct IS NULL, &#39;没奖金&#39;, &#39;有奖金&#39;) FROM employees;</span><br></pre></td></tr></table></figure>
<h2 id="2-case函数"><a href="#2-case函数" class="headerlink" title="2. case函数"></a>2. <code>case</code>函数</h2><p><strong>使用语法一</strong>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CASE 要判断的字段或表达式</span><br><span class="line">WHEN 常量1  THEN  要显示的值1或语句1</span><br><span class="line">WHEN 常量2  THEN  要显示的值2或语句2</span><br><span class="line">...</span><br><span class="line">ELSE 要显示的值n或语句n</span><br><span class="line">END</span><br></pre></td></tr></table></figure>
<p><strong>案例1：查询员工的工资，要求部门编号=30，则显示工资为1.1倍，部门编号=20，则显示工资为1.2倍，部门编号=30，则显示工资为1.3倍，其他部门显示的工资为原工资</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT salary 原始工资, department_id, </span><br><span class="line">CASE department_id </span><br><span class="line">WHEN 30 THEN salary*1.1</span><br><span class="line">WHEN 40 THEN salary*1.2</span><br><span class="line">WHEN 50 THEN salary*1.3</span><br><span class="line">ELSE salary;</span><br><span class="line">END AS 新工资</span><br><span class="line">FROM employees;</span><br></pre></td></tr></table></figure>
<p><strong>使用语法二：类似于多重<code>if</code></strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CASE</span><br><span class="line">WHEN 条件1 THEN 要显示的值1或语句1</span><br><span class="line">WHEN 条件2 THEN 要显示的值2或语句2</span><br><span class="line">...</span><br><span class="line">THEN 要显示的值n或语句n</span><br><span class="line">END</span><br></pre></td></tr></table></figure>
<p><strong>案例2:查询员工的工资情况：如果工资&gt;20000，显示级别A，若工资&gt;15000，显示级别B，若工资&gt;10000，显示级别C，否则显示级别D </strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT salary,</span><br><span class="line">CASE</span><br><span class="line">WHEN salary&gt;20000 THEN &#39;A&#39;</span><br><span class="line">WHEN salary&gt;15000 THEN &#39;B&#39;</span><br><span class="line">WHEN salary&gt;10000 THEN &#39;C&#39;</span><br><span class="line">ELSE &#39;D&#39;</span><br><span class="line">END AS &#39;工资级别&#39;</span><br><span class="line">FROM employees;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title>MySQL入门（四）排序查询</title>
    <url>/posts/6b667049.html</url>
    <content><![CDATA[<p>本文数据集下载链接：<a href="https://pan.baidu.com/s/1sVcSXfVZimc6ruyMr4Gr3g" target="_blank" rel="noopener">https://pan.baidu.com/s/1sVcSXfVZimc6ruyMr4Gr3g</a> ，提取码：7gfa </p>
<h1 id="一、排序查询"><a href="#一、排序查询" class="headerlink" title="一、排序查询"></a>一、排序查询</h1><p>我们知道从 MySQL 表中使用 SQL SELECT 语句来读取数据，如果我们需要对读取的数据进行排序，我们就可以使用 MySQL 的 <strong>ORDER BY</strong> 子句来设定你想按哪个字段哪种方式来进行排序，再返回搜索结果。</p>
<h2 id="1-语法"><a href="#1-语法" class="headerlink" title="1. 语法"></a>1. 语法</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT 查询列表</span><br><span class="line">FROM 表</span><br><span class="line">[WHERE 筛选条件]</span><br><span class="line">ORDER BY 排序列表 【ASC】</span><br></pre></td></tr></table></figure>
<p>一般ORDER BY语句放在查询语句的最后【<code>LIMIT</code>子句除外】</p>
<ul>
<li>你可以使用任何字段来作为排序的条件，从而返回排序后的查询结果。</li>
<li>你可以设定多个字段来排序。</li>
<li>你可以使用 <code>ASC</code> 或 <code>DESC</code> 关键字来设置查询结果是按升序或降序排列。 默认情况下，它是按升序排列。</li>
<li>你可以添加 <code>WHERE...LIKE</code> 子句来设置条件。</li>
</ul>
<h2 id="2-案例"><a href="#2-案例" class="headerlink" title="2. 案例"></a>2. 案例</h2><p><strong>案例1</strong>：<strong>查询员工信息，要求工资从高到低排序</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * </span><br><span class="line">FROM employees </span><br><span class="line">ORDER BY salary DESC;</span><br></pre></td></tr></table></figure>
<p><strong>查询员工信息，要求工资从低到高排序</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * </span><br><span class="line">FROM employees </span><br><span class="line">ORDER BY salary ASC;</span><br></pre></td></tr></table></figure>
<p><strong>案例2：查询部门编号&gt;=90的员工信息，按入职时间的先后进行排序</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * </span><br><span class="line">FROM employees </span><br><span class="line">WHERE department_id &gt;&#x3D; 90 </span><br><span class="line">ORDER BY hiredate ASC;</span><br></pre></td></tr></table></figure>
<p><strong>案例3：按年薪高低显示员工的信息和年薪【按表达式排序】</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT *, salary*12*(1+IFNULL(commission_pct,0)) 年薪 </span><br><span class="line">FROM employees </span><br><span class="line">ORDER BY 年薪</span><br></pre></td></tr></table></figure>
<p><strong>案例4：按姓名长度显示员工的姓名和工资【按函数排序】</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT LENGTH(last_name) 字节长度,last_name,salary </span><br><span class="line">FROM employees </span><br><span class="line">ORDER BY 字节长度 DESC;</span><br></pre></td></tr></table></figure>
<p><strong>案例5：查询员工信息，要求先按工资排序，再按员工编号排序【按多个字段排序】</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * </span><br><span class="line">FROM employees </span><br><span class="line">ORDER BY salary ASC, employee_id DESC;</span><br></pre></td></tr></table></figure>
<h1 id="二、MySQL拼音排序"><a href="#二、MySQL拼音排序" class="headerlink" title="二、MySQL拼音排序"></a>二、MySQL拼音排序</h1><p>如果字符集采用的是 gbk(汉字编码字符集)，直接在查询语句后边添加 <code>ORDER BY</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * </span><br><span class="line">FROM runoob_tbl</span><br><span class="line">ORDER BY runoob_title;</span><br></pre></td></tr></table></figure>
<p>如果字符集采用的是 utf8(万国码)，需要先对字段进行转码然后排序：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * </span><br><span class="line">FROM runoob_tbl</span><br><span class="line">ORDER BY CONVERT(runoob_title using gbk);</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title>MySQL入门（三）条件查询</title>
    <url>/posts/ca70db86.html</url>
    <content><![CDATA[<p>本文数据集下载链接：<a href="https://pan.baidu.com/s/1sVcSXfVZimc6ruyMr4Gr3g" target="_blank" rel="noopener">https://pan.baidu.com/s/1sVcSXfVZimc6ruyMr4Gr3g</a> ，提取码：7gfa </p>
<h1 id="一、条件查询基础"><a href="#一、条件查询基础" class="headerlink" title="一、条件查询基础"></a>一、条件查询基础</h1><p>我们知道从 MySQL 表中使用 SQL<code>SELECT</code> 语句来读取数据，如需有条件地从表中选取数据，可将 <code>WHERE</code> 子句添加到 <code>SELECT</code>语句中。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">操作符</th>
<th style="text-align:left">描述</th>
<th style="text-align:left">实例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">=</td>
<td style="text-align:left">等号，检测两个值是否相等，如果相等返回true</td>
<td style="text-align:left">(A = B) 返回false。</td>
</tr>
<tr>
<td style="text-align:left">&lt;&gt;, !=</td>
<td style="text-align:left">不等于，检测两个值是否相等，如果不相等返回true</td>
<td style="text-align:left">(A != B) 返回 true。</td>
</tr>
<tr>
<td style="text-align:left">&gt;</td>
<td style="text-align:left">大于号，检测左边的值是否大于右边的值, 如果左边的值大于右边的值返回true</td>
<td style="text-align:left">(A &gt; B) 返回false。</td>
</tr>
<tr>
<td style="text-align:left">&lt;</td>
<td style="text-align:left">小于号，检测左边的值是否小于右边的值, 如果左边的值小于右边的值返回true</td>
<td style="text-align:left">(A &lt; B) 返回 true。</td>
</tr>
<tr>
<td style="text-align:left">&gt;=</td>
<td style="text-align:left">大于等于号，检测左边的值是否大于或等于右边的值, 如果左边的值大于或等于右边的值返回true</td>
<td style="text-align:left">(A &gt;= B) 返回false。</td>
</tr>
<tr>
<td style="text-align:left">&lt;=</td>
<td style="text-align:left">小于等于号，检测左边的值是否小于或等于右边的值, 如果左边的值小于或等于右边的值返回true</td>
<td style="text-align:left">(A &lt;= B) 返回 true。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>语法</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT </span><br><span class="line">				查询列表 </span><br><span class="line">	FROM </span><br><span class="line">				表名 </span><br><span class="line">	WHERE </span><br><span class="line">				筛选条件</span><br></pre></td></tr></table></figure>
<p><strong>分类</strong></p>
<ol>
<li>按条件表达式筛选：&gt; &lt; = != &lt;&gt; &gt;+ &lt;=</li>
<li>按逻辑表达式筛选：&amp;&amp;  ||  !（and or not）</li>
<li>模糊查询：LIKE,  BETWEEN AND, IN, IS NULL</li>
</ol>
<h1 id="二、三种查询方式介绍"><a href="#二、三种查询方式介绍" class="headerlink" title="二、三种查询方式介绍"></a>二、三种查询方式介绍</h1><h2 id="1-按条件表达式筛选"><a href="#1-按条件表达式筛选" class="headerlink" title="1. 按条件表达式筛选"></a>1. 按条件表达式筛选</h2><p><strong>案例一：查询工资&gt;12000的员工信息</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * </span><br><span class="line">FROM employees </span><br><span class="line">WHERE salary&gt;12000;</span><br></pre></td></tr></table></figure>
<p><strong>案例二：查询部门编号不等于90号的员工名和部门编号</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name, department_id </span><br><span class="line">FROM employees </span><br><span class="line">WHERE department_id&lt;&gt;90;</span><br></pre></td></tr></table></figure>
<h2 id="2-按逻辑表达式筛选"><a href="#2-按逻辑表达式筛选" class="headerlink" title="2. 按逻辑表达式筛选"></a>2. 按逻辑表达式筛选</h2><p><strong>案例一：查询工资在10000到20000之间的员工名、工资及奖金</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name, salary, commission_pct </span><br><span class="line">FROM employees </span><br><span class="line">WHERE salary&gt;&#x3D;10000 AND salary&lt;&#x3D;20000;</span><br></pre></td></tr></table></figure>
<p><strong>案例二：查询部门编号不是在90到110之间的，或者工资高于15000的员工信息</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * </span><br><span class="line">FROM employees </span><br><span class="line">WHERE department_id&lt;90 </span><br><span class="line">OR department_id&gt;110 </span><br><span class="line">OR salary&gt;15000;</span><br></pre></td></tr></table></figure>
<p>更为简洁的写法是（使用逻辑表达式）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * </span><br><span class="line">FROM employees </span><br><span class="line">WHERE NOT(department_id&gt;&#x3D;90 AND department_id&lt;&#x3D;110) OR salary&gt;15000;</span><br></pre></td></tr></table></figure>
<h2 id="3-模糊查询"><a href="#3-模糊查询" class="headerlink" title="3. 模糊查询"></a>3. 模糊查询</h2><h3 id="3-1-LIKE"><a href="#3-1-LIKE" class="headerlink" title="3.1 LIKE"></a>3.1 LIKE</h3><ul>
<li>一般和通配符搭配使用</li>
<li>通配符：<ul>
<li>$\%$ 百分号：任意多个字符，包含0个字符</li>
<li>$_$ 下划线：任意单个字符</li>
</ul>
</li>
</ul>
<p><strong>案例一：查询员工名中包含字符a的员工信息</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * </span><br><span class="line">FROM employees </span><br><span class="line">WHERE last_name LIKE &#39;%a%&#39;;</span><br></pre></td></tr></table></figure>
<p><strong>案例二：查询员工名中第三个字符为n，第五个字符为l的员工名和工资</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name, salary </span><br><span class="line">FROM employees </span><br><span class="line">WHERE last_name LIKE &#39;__n_l%&#39;;</span><br></pre></td></tr></table></figure>
<p><strong>案例三：查询员工名中第二个字符为下划线（_）的员工名，第二种方法为手动指定转义字符</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name </span><br><span class="line">FROM employees </span><br><span class="line">WHERE last_name </span><br><span class="line">LIKE &#39;_\_%&#39;;</span><br></pre></td></tr></table></figure>
<p>第二种方法代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name </span><br><span class="line">FROM employees </span><br><span class="line">WHERE last_name LIKE &#39;_$_%&#39; ESCAPE &#39;$&#39;;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-BETWEEN-AND"><a href="#3-2-BETWEEN-AND" class="headerlink" title="3.2 BETWEEN AND"></a>3.2 BETWEEN AND</h3><ol>
<li><p>使用BETWEEN AND可以提高语句的简洁度</p>
<ol>
<li>包含临界值</li>
<li>两个值不能颠倒顺序</li>
</ol>
</li>
</ol>
<p><strong>案例：查询员工编号在100到120之间的员工信息，第一种做法较繁琐，第二种用BETWEEN AND</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * </span><br><span class="line">FROM employees </span><br><span class="line">WHERE employee_id&gt;&#x3D;100 </span><br><span class="line">AND employee_id&lt;&#x3D;120;</span><br></pre></td></tr></table></figure>
<p>或者更简洁的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * </span><br><span class="line">FROM employees </span><br><span class="line">WHERE employee_id BETWEEN 100 AND 120;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-IN"><a href="#3-3-IN" class="headerlink" title="3.3 IN"></a>3.3 IN</h3><ol>
<li>使用IN提高语句简洁度</li>
<li>IN列表的值类型必须一致或兼容</li>
<li>不支持下划线或通配符</li>
</ol>
<p><strong>案例：查询员工的工种编号是 IT_PROG、AD_VP、AD_PRES中的一个员工名和工种编号</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name, job_id </span><br><span class="line">FROM employees </span><br><span class="line">WHERE job_id&#x3D;&#39;IT_PROG&#39; </span><br><span class="line">OR job_id&#x3D;&#39;AD_VP&#39; OR job_id&#x3D;&#39;AD_PRES&#39;;</span><br></pre></td></tr></table></figure>
<p>或者更简洁的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name, job_id </span><br><span class="line">FROM employees </span><br><span class="line">WHERE job_id IN (&#39;IT_PROG&#39;, &#39;AD_VP&#39;, &#39;AD_PRES&#39;);</span><br></pre></td></tr></table></figure>
<h3 id="3-4-IS-NULL"><a href="#3-4-IS-NULL" class="headerlink" title="3.4 IS NULL"></a>3.4 IS NULL</h3><ul>
<li>=或&lt;&gt;不能用于判断null值</li>
<li>is null或is not null可以判断null值</li>
</ul>
<p><strong>案例：查询没有奖金的员工名和奖金率</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name, commission_pct </span><br><span class="line">FROM employees </span><br><span class="line">WHERE commission_pct IS NULL;</span><br></pre></td></tr></table></figure>
<p><strong>变式：查询有奖金的员工名和奖金率</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name, commission_pct </span><br><span class="line">FROM employees </span><br><span class="line">WHERE commission_pct IS NOT NULL;</span><br></pre></td></tr></table></figure>
<p><strong>注意：以下为错误：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * FROM employees</span><br><span class="line">WHERE salary IS 12000;</span><br></pre></td></tr></table></figure>
<h2 id="4-安全等与-lt-gt"><a href="#4-安全等与-lt-gt" class="headerlink" title="4. 安全等与 &lt;=&gt;"></a>4. 安全等与 &lt;=&gt;</h2><ol>
<li>优点：既可以判断NULL值又可以判断普通数值</li>
<li>缺点：可读性较低</li>
</ol>
<p><strong>案例1：查询没有奖金的员工名和奖金率</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name, commission_pct </span><br><span class="line">FROM employees </span><br><span class="line">WHERE commission_pct &lt;&#x3D;&gt; NULL;</span><br></pre></td></tr></table></figure>
<p><strong>案例2：查询工资为12000的员工信息</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name, commission_pct </span><br><span class="line">FROM employees </span><br><span class="line">WHERE salary &lt;&#x3D;&gt; 12000;</span><br></pre></td></tr></table></figure>
<h1 id="三、易混辨析"><a href="#三、易混辨析" class="headerlink" title="三、易混辨析"></a>三、易混辨析</h1><p><strong>where：</strong>数据库中常用的是where关键字，用于在初始表中筛选查询。它是一个约束声明，用于约束数据，在返回结果集之前起作用。</p>
<p><strong>group by:</strong>对select查询出来的结果集按照某个字段或者表达式进行分组，获得一组组的集合，然后从每组中取出一个指定字段或者表达式的值。</p>
<p><strong>having：</strong>用于对where和group by查询出来的分组经行过滤，查出满足条件的分组结果。它是一个过滤声明，是在查询返回结果集以后对查询结果进行的过滤操作。</p>
<p><strong>执行顺序</strong></p>
<p><code>select –&gt;where –&gt; group by–&gt; having–&gt;order by</code></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title>MySQL入门（二）基础查询</title>
    <url>/posts/8bec0637.html</url>
    <content><![CDATA[<p>本文数据集下载链接：<a href="https://pan.baidu.com/s/1sVcSXfVZimc6ruyMr4Gr3g" target="_blank" rel="noopener">https://pan.baidu.com/s/1sVcSXfVZimc6ruyMr4Gr3g</a> ，提取码：7gfa </p>
<p><strong>DQL指的是Data Query Language，即数据查询语言，是MySQL语言中的一个子集</strong></p>
<h1 id="基础查询"><a href="#基础查询" class="headerlink" title="基础查询"></a>基础查询</h1><p>MySQL 数据库使用SQL SELECT语句来查询数据。你可以通过 mysql&gt; 命令提示窗口中在数据库中查询数据，或者通过PHP脚本来查询数据。</p>
<ul>
<li>查询语句中你可以使用一个或者多个表，表之间使用逗号(,)分割，并使用WHERE语句来设定查询条件。</li>
<li>SELECT 命令可以读取一条或者多条记录。</li>
<li>你可以使用星号（*）来代替其他字段，SELECT语句会返回表的所有字段数据</li>
<li>你可以使用 WHERE 语句来包含任何条件。</li>
<li>你可以使用 LIMIT 属性来设定返回的记录数。</li>
<li>你可以通过OFFSET指定SELECT语句开始查询的数据偏移量。默认情况下偏移量为0。</li>
</ul>
<h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><ol>
<li><code>select</code> 查询列表 <code>from</code> 表名;</li>
<li>可以使用 <code>字段</code>区分关键字</li>
</ol>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><pre><code> 1. 查询列表可以是：表中的字段、常量、表达式、函数
 2. 查询的结果是一个虚拟的表格
</code></pre><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p><strong>查询表中的单个字段</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name FROM employees;</span><br></pre></td></tr></table></figure>
<p><strong>查询表中的多个字段</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name, salary, email FROM employees;</span><br></pre></td></tr></table></figure>
<p><strong>查询表中的所有字段（可以通过全选复制字段名快速复制）</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT employee_id,	first_name,	last_name	email,	phone_number,	job_id,	salary,	commission_pct,	manager_id,	department_id,	hiredate FROM employees;</span><br></pre></td></tr></table></figure>
<p><strong>等同于下面语句（但*使得顺序与原来一样）</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * FROM employees;</span><br></pre></td></tr></table></figure>
<p><strong>查询常量值</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT 100；</span><br><span class="line">SELECT &#39;john&#39;;</span><br></pre></td></tr></table></figure>
<p><strong>查询表达式</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT 100%98;</span><br></pre></td></tr></table></figure>
<p><strong>查询函数</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT VERSION();</span><br></pre></td></tr></table></figure>
<h1 id="为字段取别名"><a href="#为字段取别名" class="headerlink" title="为字段取别名"></a>为字段取别名</h1><pre><code> 1. 便于理解
 2. 如果要查询的字段有重名的情况，使用别名可以区分开
</code></pre><p><strong>方式一：使用As</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT 100%98 AS 结果;</span><br><span class="line"></span><br><span class="line">SELECT last_name AS 姓, first_name As 名 </span><br><span class="line">FROM employees;</span><br></pre></td></tr></table></figure>
<p><strong>方式二：使用空格</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT last_name 姓, first_name 名 </span><br><span class="line">FROM employees;</span><br></pre></td></tr></table></figure>
<p><strong>案例：查询salary，显示结果为out put</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT salary AS &#39;out put&#39; </span><br><span class="line">FROM employees;</span><br></pre></td></tr></table></figure>
<h1 id="去重"><a href="#去重" class="headerlink" title="去重"></a>去重</h1><p>去重的关键字是<code>DISTINCT</code>，注意：不可以同时对多个字段去重</p>
<p><strong>案例：查询员工表中涉及到的所有的部门编号(DISTINCT关键字)</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT DISTINCT department_id </span><br><span class="line">FROM employees;</span><br></pre></td></tr></table></figure>
<h1 id="加号和CONCAT的作用"><a href="#加号和CONCAT的作用" class="headerlink" title="加号和CONCAT的作用"></a>加号和CONCAT的作用</h1><p>JAVA中的加号有两个功能：</p>
<pre><code>1. 运算符，两个操作数都是数值型
2. 连接符，只要有一个操作数为字符串
</code></pre><p>MySQL中的加号只有一个功能：运算符</p>
<pre><code>1. 若两个操作数都为数值型，则做加法运算： select 100 + 90;
2. 若其中一方为字符型，试图将字符型数值转换成数值型：select &#39;123&#39;+90;
3. 若转换失败，则将字符型数值转换成0：select &#39;john&#39;+90;
4. 只要有其中一方为null，则结果必为null
</code></pre><p><strong>案例：查询员工名和姓连接成一个字段，并显示为姓名</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT CONCAT(last_name,&#39; &#39;,first_name) AS 姓名 </span><br><span class="line">FROM employees;</span><br></pre></td></tr></table></figure>
<p><strong>案例：显示出表employees的全部列，各个列之间用逗号连接，列头显示成OUT_PUT</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT </span><br><span class="line">		CONCAT(employee_id, &#39;,&#39;,	first_name, &#39;,&#39;,	last_name, &#39;,&#39;,	email, &#39;,&#39;,	phone_number, &#39;,&#39;,	job_id, &#39;,&#39;,	salary, &#39;,&#39;,	IFNULL(commission_pct,0), &#39;,&#39;,	manager_id, &#39;,&#39;,	department_id, &#39;,&#39;,	hiredate)</span><br><span class="line">AS </span><br><span class="line">		out_put </span><br><span class="line">FROM </span><br><span class="line">		employees;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title>MySQL入门（一）数据库基础概念</title>
    <url>/posts/e89e7d68.html</url>
    <content><![CDATA[<h1 id="一、数据库基本概念"><a href="#一、数据库基本概念" class="headerlink" title="一、数据库基本概念"></a>一、数据库基本概念</h1><p>数据库（Database）是按照数据结构来组织、存储和管理数据的仓库。每个数据库都有一个或多个不同的 API 用于创建，访问，管理，搜索和复制所保存的数据。我们也可以将数据存储在文件中，但是在文件中读写数据速度相对较慢。</p>
<p>所以，现在我们使用关系型数据库管理系统（RDBMS）来存储和管理大数据量。所谓的关系型数据库，是建立在关系模型基础上的数据库，借助于集合代数等数学概念和方法来处理数据库中的数据。<strong>数据库的好处</strong>是可以持久化数据到本地，同时可以实现结构化查询，方便管理<strong>数据库相关概念</strong>如下：</p>
<ol>
<li>DB：数据库，保存一组有组织的数据的容器</li>
<li>DBMS：数据库管理系统，又称为数据库软件（产品），用于管理数据库中的数据</li>
<li>SQL：结构化查询语言，用于和DBMS通信的语言</li>
</ol>
<h2 id="1-RDBMS-术语"><a href="#1-RDBMS-术语" class="headerlink" title="1. RDBMS 术语"></a>1. RDBMS 术语</h2><p>在我们开始学习MySQL 数据库前，让我们先了解下RDBMS的一些术语：</p>
<ul>
<li><strong>数据库:</strong> 数据库是一些关联表的集合。</li>
<li><strong>数据表:</strong> 表是数据的矩阵。在一个数据库中的表看起来像一个简单的电子表格。</li>
<li><strong>列:</strong> 一列(数据元素) 包含了相同类型的数据, 例如邮政编码的数据。</li>
<li><strong>行：</strong>一行（=元组，或记录）是一组相关的数据，例如一条用户订阅的数据。</li>
<li><strong>冗余</strong>：存储两倍数据，冗余降低了性能，但提高了数据的安全性。</li>
<li><strong>主键</strong>：主键是唯一的。一个数据表中只能包含一个主键。你可以使用主键来查询数据。</li>
<li><strong>外键：</strong>外键用于关联两个表。</li>
<li><strong>复合键</strong>：复合键（组合键）将多个列作为一个索引键，一般用于复合索引。</li>
<li><strong>索引：</strong>使用索引可快速访问数据库表中的特定信息。索引是对数据库表中一列或多列的值进行排序的一种结构。类似于书籍的目录。</li>
<li><strong>参照完整性:</strong> 参照的完整性要求关系中不允许引用不存在的实体。与实体完整性是关系模型必须满足的完整性约束条件，目的是保证数据的一致性。</li>
</ul>
<h2 id="2-MySQL数据库基本概况"><a href="#2-MySQL数据库基本概况" class="headerlink" title="2. MySQL数据库基本概况"></a>2. MySQL数据库基本概况</h2><p>MySQL 是一个关系型数据库管理系统，由瑞典 MySQL AB 公司开发，目前属于 Oracle 公司。MySQL 是一种关联数据库管理系统，关联数据库将数据保存在不同的表中，而不是将所有数据放在一个大仓库内，这样就增加了速度并提高了灵活性。</p>
<ul>
<li>MySQL 是开源的，目前隶属于 Oracle 旗下产品。</li>
<li>MySQL 支持大型的数据库。可以处理拥有上千万条记录的大型数据库。</li>
<li>MySQL 使用标准的 SQL 数据语言形式。</li>
<li>MySQL 可以运行于多个系统上，并且支持多种语言。这些编程语言包括 C、C++、Python、Java、Perl、PHP、Eiffel、Ruby 和 Tcl 等。</li>
<li>MySQL 对PHP有很好的支持，PHP 是目前最流行的 Web 开发语言。</li>
<li>MySQL 支持大型数据库，支持 5000 万条记录的数据仓库，32 位系统表文件最大可支持 4GB，64 位系统支持最大的表文件为8TB。</li>
<li>MySQL 是可以定制的，采用了 GPL 协议，你可以修改源码来开发自己的 MySQL 系统。</li>
</ul>
<h2 id="4-数据库存储数据的特点"><a href="#4-数据库存储数据的特点" class="headerlink" title="4. 数据库存储数据的特点"></a>4. 数据库存储数据的特点</h2><ol>
<li>将数据放到表中，表再放到库中</li>
<li>一个数据库中可以有多个表，每个表都有一个名字用来标识自己，表名具有唯一性</li>
<li>表具有一些特性，这些特性定义了数据在表中如何存储，类似java中的“类”的设计</li>
<li>表由列组成，我们也称之为字段。所有表都是由一个或多个列组成的，每一列类似java中的”属性“</li>
<li>表中的数据是按行存储的，每一行类似java中的“对象”</li>
</ol>
<h1 id="二、MySQL基本操作"><a href="#二、MySQL基本操作" class="headerlink" title="二、MySQL基本操作"></a>二、MySQL基本操作</h1><p><strong>MySQL服务的启动和终止</strong></p>
<p>假设已经 安装好了MySQL，我们可以通过Windows中的服务手动 启动或关闭MySQL服务，也可以通过管理员模式打开命令行，使用<code>net stop mysql</code>与<code>net start mysql</code>分别关闭和启动MySQL，</p>
<p><strong>MySQL服务端的登陆和退出</strong></p>
<p>MySQL服务端可以直接通过MySQL Shell进入，但这种方式<strong>只能容许root用户进入</strong>，我们推荐使用命令行进入，在命令行下输入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -h localhost -P 3307 -u root -p</span><br></pre></td></tr></table></figure>
<p>其中3307为本机设定的端口号，每个人可能不一样，-p之后将会要去输入密码，此时输入之前预设的密码即可，也可以直接在后面接，但注意-p若后面直接接密码，<strong>不加空格</strong>。若是直接本机进入，可以省略<code>-h localhost -P 3307</code>。</p>
<p>退出时直接使用<code>exit</code>或<code>Ctrl+C</code>即可</p>
<h1 id="三、MySQL常用命令"><a href="#三、MySQL常用命令" class="headerlink" title="三、MySQL常用命令"></a>三、MySQL常用命令</h1><p>以下命令结尾都需要加分号；</p>
<ul>
<li><p><code>show databases;</code>可以显示所有数据库</p>
</li>
<li><p><code>use+库名</code>：打开某个数据库</p>
</li>
<li><p><code>show tables</code>：显示库中的某个表内容</p>
</li>
<li><p><code>show tables from+库名（比如MySQL）</code>：从某个库显示表</p>
</li>
<li><p><code>select database()</code>：显示所在的库</p>
</li>
<li><p>创建表名为stuinfo：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create table stuinfo(                                                                                   -&gt; id int,                                                                                              -&gt; name varchar(20));</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>select * from stuinfo</code>：由于为空表，故返回Empty Set</p>
</li>
<li><p><code>desc stuinfo</code>：显示stuinfo表</p>
</li>
<li><p><code>insert info stuinfo(id,name) values(1, &#39;rose&#39;)</code>：插入表</p>
<p>此时再显示，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from stuinfo;                                                                                 +------+------+                                                                                         | id   | name |                                                                                         +------+------+                                                                                         |    1 | rose |                                                                                         |    2 | john |                                                                                          +------+------+</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>update stuinfo set name=&#39;lilei&#39; where id=1;</code>将rose的名字修改为lilei</p>
</li>
<li><code>delete from stuinfo where id=1</code>：将lilei删除</li>
<li><code>select version()</code>：显示版本号（也可以退出之后<code>mysql --version</code>查看）</li>
</ul>
<h1 id="四、MySQL的语法规范"><a href="#四、MySQL的语法规范" class="headerlink" title="四、MySQL的语法规范"></a>四、MySQL的语法规范</h1><ol>
<li>不区分大小写，但建议关键字大写，表名、列名小写</li>
<li>每条命令最好分号结尾</li>
<li>每条命令根据需要，可以进行缩进或换行</li>
<li>注释：<ul>
<li>单行注释：#注释文字</li>
<li>单行注释：— 注释文字（需要加空格）</li>
<li>多行注释：/<em> 注释文字 </em>/</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title>区块链入门（五）合约编写实例补充</title>
    <url>/posts/a4c5a08d.html</url>
    <content><![CDATA[<font color="red"> 注：本教程为技术教程，不谈论且不涉及炒作任何数字货币 </font>

<h1 id="合约编写实战实例"><a href="#合约编写实战实例" class="headerlink" title="合约编写实战实例"></a>合约编写实战实例</h1><h2 id="一、简单代币合约"><a href="#一、简单代币合约" class="headerlink" title="一、简单代币合约"></a>一、简单代币合约</h2><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity &gt; <span class="number">0.4</span><span class="number">.22</span>;</span><br><span class="line"></span><br><span class="line">contract Coin&#123;</span><br><span class="line">    <span class="comment">//这里我们定义了一个address 作为key, uint做为value的hashTable balances; 我们还定义了一个address的变量minter;</span></span><br><span class="line">    address public minter;</span><br><span class="line">    mapping(<span class="function"><span class="params">address</span>=&gt;</span>uint) balances;</span><br><span class="line">    event Sent(address <span class="keyword">from</span>, address to, uint amount);</span><br><span class="line">    <span class="keyword">constructor</span>()&#123;</span><br><span class="line">        <span class="comment">//代表创建这个合约的账户地址，被赋值给变量minter.</span></span><br><span class="line">        minter = msg.sender;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//添加一个挖矿合约 </span></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">mint</span>(<span class="params">address receiver, uint amount</span>) <span class="title">public</span></span>&#123;</span><br><span class="line">        <span class="built_in">require</span>(msg.sender == minter);</span><br><span class="line">        balances[receiver] += amount;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">send</span>(<span class="params">address receiver, uint amount</span>) <span class="title">public</span></span>&#123;</span><br><span class="line">        <span class="built_in">require</span>(balances[msg.sender] &gt;= amount);</span><br><span class="line">        balances[msg.sender] -= amount;</span><br><span class="line">        balances[receiver] += amount;</span><br><span class="line">        emit Sent(msg.sender,receiver,amount);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>解析：<br>上面实现一个简单的加密货币，币在这里可以无中生有，但只有创建合约的人才能做到，且任何人都可以给他人转币，无需注册名和密码。</p>
<p><code>address</code>类型是一个160位的值，不允许任何算数操作，这种类型适合存储合约地址或外部人员。</p>
<p><code>mappings</code>可看作是一个哈希表，它会执行虚拟初始化，以使得所有可能存在的键都映射到一个字节表示为全零的值。</p>
<p><code>event Sent(address from, address to, uint amount)</code>;声明了一个所谓的事件，它在send函数最后一行被发出。用户界面可以监听区块链上正在发送的事件，且不会花费太多成本，一旦它被发出，监听该事件的listener都将收到通知，而所有的事件都包含了<code>from</code>,<code>t</code>o和<code>amoun</code>t三个参数，可方便追踪事务。</p>
<p><code>msg.sender</code>始终是当前函数或者外部函数调用的来源地址。</p>
<p>最后真正被用户和其他合约所调用的，用于完成本合约功能的方法是<code>mint</code>和<code>send</code>。若<code>mint</code>被合约创建者外的其他调用则说明都不会发生。</p>
<p><code>send</code>函数可被任何人用于向其他人发送代币，前提是发送者拥有这些代币，若使用合约发送代币给一个地址，当在区块链浏览器上查到该地址时时看不到任何相关信息的，因为，实际上发送币和更改余额的信息仅仅存在特定合约的数据存储器中。通过使用事件，可非常简单地为新币创建一个区块链浏览器来追踪交易和余额。</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-15.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>



<h2 id="二、水龙头合约"><a href="#二、水龙头合约" class="headerlink" title="二、水龙头合约"></a>二、水龙头合约</h2><p>在前面我们通过 Ropsten 测试网络的水龙头（Faucet）获取了一些以太币，并提到可以向水龙头账户发送以太币来捐赠以太币。实际上，水龙头账户是一个合约账户，水龙头就是一份合约，而整个网站就是合约+前端组成的DApp。下面我们通过 Remix 来编写一个简单的水龙头合约，借此了解如何创建、部署合约以及一些 Solidity 的基本语法。</p>
<p>首先打开 Remix，并新建一个名为 faucet.sol 的文件，该文件就是 Solidity 的源文件</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-16.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>

<p>打开 faucet.sol，并写入如下代码</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.7</span><span class="number">.0</span>;</span><br><span class="line"></span><br><span class="line">contract faucet &#123;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">withdraw</span> (<span class="params">uint amount</span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        <span class="built_in">require</span> (amount &lt;= <span class="number">1e18</span>);</span><br><span class="line">        msg.sender.transfer (amount);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    receive () external payable &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过这几行代码我们就实现了一个非常简单的水龙头合约。首行代码 <code>pragma solidity ^0.7.0</code>是一个<strong>杂注</strong>，指定了我们的源文件使用的编译器版本不能低于 0.7.0，也不能高于 0.8.0。</p>
<p><code>contract faucet{...}</code> 声明了一个合约对象，合约对象类似面向对象语言中的类，对象名必须跟文件名相同。</p>
<p>接下来通过  <code>function withdraw (uint amount) public {...}</code> 创建了一个名为  withdraw 的函数，该函数接收一个无符号整数（uint）作为参数，并且被声明为 public 函数，意为可以被其他合约调用。</p>
<p>withdraw 函数体中的 <code>require</code> 是 Solidity 的内置函数，用来检测括号中的条件是否满足。条件满足则继续执行合约，条件不满足则合约停止执行，回撤所有执行过的操作，并抛出异常。在这里我们通过 <code>require (amount &lt;= 1e18)</code> 来检测输入的以太币值是否小于等于1个以太。</p>
<p>接下来的这一行 <code>msg.sender.transfer (amount)</code> 就是实际的提款操作了。<code>msg</code> 是 Solidity 中内置的对象，所有合约都可以访问，它代表触发此合约的交易。也就是说当我们调用 <code>withdraw</code> 函数的时候实际上触发了一笔交易，并用 <code>msg</code> 来表示它。<code>sender</code> 是交易 <code>msg</code> 的属性，表示了交易的发件人地址。函数 <code>transfer</code> 是一个内置函数，它接收一个参数作为以太币的数量，并将该数量的以太币从合约账户发送到调用合约的用户的地址中。</p>
<p>最后一行是一个特殊的函数 <code>receive</code> ，这是所谓的 <code>fallback</code> 或 <code>default</code> 函数。当合约中的其他函数无法处理发送到合约中的交易信息时，就会执行该函数。在这里，我们将该函数声明为 <code>external</code> 和 <code>payable</code> ，<code>external</code> 意味着该函数可以接收来自外部账户的调用，<code>payable</code> 意味着该函数可以接收来自外部账户发送的以太币。</p>
<p>这样，当我们调用合约中的 <code>withdraw</code> 并提供一个参数时，我们可以从这份合约中提出以太币；当我们向合约发送以太币时，就会调用 <code>receive</code> 函数往合约中捐赠以太币。</p>
<p>代码编写完毕后，在 Remix 左侧的功能栏中选择第二项，并点击 <em>Compile faucet.sol</em> 来编译我们的 sol 文件。</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-17.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<p>编译完成后会出现一个 Warning，提示我们添加 SPDX license，可以忽略。</p>
<p>随后选择 Remix 左侧工具栏的第三项，进入合约部署界面</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-18.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<p>首先将 ENVIRONMENT 选择为 Injected Web3，这样才能通过 MetaMask 钱包来发送交易。</p>
<p>随后点击 Deploy 部署合约，MetaMask 会弹出部署合约的交易界面</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-19.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<p>因为该笔交易是合约创建交易，因此我们支付的以太币为0，但仍需支付一定的 Gas 费用，可以自己设定 Gas 的价格。</p>
<p>合约部署成功后会收到 Chrome 的消息提示，并在 Remix 的 Deployed Contracts 中也会有显示</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-20.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<p>这样我们就完成了这个水龙头合约的部署。</p>
<h4 id="水龙头测试"><a href="#水龙头测试" class="headerlink" title="水龙头测试"></a>水龙头测试</h4><p>我们刚刚创建的水龙头中还没有以太坊，因此我们可以通过 MetaMask 向水龙头合约的地址中发送一些以太坊。水龙头合约的地址会显示在 Remix 中的，见上图 FAUCET AT 0X7A4…34219，可以直接复制。</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-21.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<p>交易被确认后，我们的水龙头中就有了0.999726个以太币，现在我们可以通过 Remix 中合约一栏的 withdraw 按钮来提取以太币了。需要注意，这里输入的以太币个数是以 wei 为单位的。</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-22.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<p>点击 withdraw 后，会弹出警告框</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-23.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<p>这是因为目前我们还没有设置这笔交易的 Gas，不用担心，点击 Send Transaction 后，在弹出的 MetaMask 中设置即可。</p>
<p>交易被确认后，我们得到了刚刚提取的0.999726个以太币</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-15.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<p>若大家没有执行成功可以重新做一次、查找其他资料或者<a href="https://www.bilibili.com/video/BV1sJ411D72u?p=465" target="_blank" rel="noopener">观看此视频</a></p>
<h2 id="三、投票合约的实现"><a href="#三、投票合约的实现" class="headerlink" title="三、投票合约的实现"></a>三、投票合约的实现</h2><p><img src="\Pic\Blockchain_Pic\rating.png" style="zoom:67%;"></p>
<p>本次教程将以一个较复杂的投票合约作为结束，我们希望实现的功能是为每个（投票）建议建立一份合约,然后作为合约的创造者-主席，主席将赋予每个成员(地址)投票权，而成员的投票权可以选择委托给其他人也可以自己投票，结束时将返回投票最多的提案。听起来很简单一个功能实现起来却较为复杂，下面我们拆分开进行讲解</p>
<p>注：</p>
<ol>
<li>代码可直接在Remix编辑器的已有solidity文件中找到,在contract/_Ballot.sol文件里</li>
<li>若学习者前面部分掌握较牢固，不妨尝试直接自行阅读代码，无需阅读本节内容</li>
</ol>
<p>首先我们定义成员类型，我们为每个投票者定义权重、是否已投票、</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">struct Voter &#123;</span><br><span class="line">    uint weight; <span class="comment">// weight is accumulated by delegation</span></span><br><span class="line">    bool voted;  <span class="comment">// if true, that person already voted</span></span><br><span class="line">    address delegate; <span class="comment">// person delegated to</span></span><br><span class="line">    uint vote;   <span class="comment">// index of the voted proposal</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后我们定义提案类型，包含提案名和投票总数：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">struct Proposal &#123;</span><br><span class="line">    bytes32 name;   <span class="comment">// short name (up to 32 bytes)</span></span><br><span class="line">    uint voteCount; <span class="comment">// number of accumulated votes</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>定义三个变量，主席是一个公开的地址，建立投票者与地址的映射，然后定义提案动态数组：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">address public chairperson;</span><br><span class="line">mapping(<span class="function"><span class="params">address</span> =&gt;</span> Voter) public voters;</span><br><span class="line">Proposal[] public proposals;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>address public chairperson</code>：投票发起人，类型为 address。</li>
<li><code>mapping(address =&gt; Voter) public voters</code>：所有投票人，类型为 <code>address</code> 到 <code>Voter</code> 的映射。</li>
<li><code>Proposal[] public proposals</code>：所有提案，类型为动态大小的 <code>Proposal</code> 数组。</li>
</ul>
<p>3 个状态变量都使用了 <code>public</code> 关键字，使得变量可以被外部访问（即通过消息调用）。事实上，编译器会自动为 <code>public</code>的变量创建同名的 <code>getter</code> 函数，供外部直接读取。</p>
<p>我们还需要为每个投票赋予初始权值，并将主席的权重设置为1。我们一般使用<code>constructor</code>赋初值，这与C++等语言类似：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">constructor</span>(bytes32[] memory proposalNames) &#123;</span><br><span class="line">    chairperson = msg.sender;</span><br><span class="line">    voters[chairperson].weight = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (uint i = <span class="number">0</span>; i &lt; proposalNames.length; i++) &#123;</span><br><span class="line">        proposals.push(Proposal(&#123;</span><br><span class="line">            name: proposalNames[i],</span><br><span class="line">            voteCount: <span class="number">0</span></span><br><span class="line">        &#125;));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所有提案的名称通过参数 <code>bytes32[] proposalNames</code> 传入，逐个记录到状态变量 <code>proposals</code> 中。同时用 <code>msg.sender</code> 获取当前调用消息的发送者的地址，记录为投票发起人 <code>chairperson</code>，该发起人投票权重设为 1。</p>
<p>接下来我们需要给每个投票者赋予权重：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">giveRightToVote</span>(<span class="params">address voter</span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">    <span class="built_in">require</span>(</span><br><span class="line">        msg.sender == chairperson,</span><br><span class="line">        <span class="string">"Only chairperson can give right to vote."</span></span><br><span class="line">    );</span><br><span class="line">    <span class="built_in">require</span>(</span><br><span class="line">        !voters[voter].voted,</span><br><span class="line">        <span class="string">"The voter already voted."</span></span><br><span class="line">    );</span><br><span class="line">    <span class="built_in">require</span>(voters[voter].weight == <span class="number">0</span>);</span><br><span class="line">    voters[voter].weight = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该函数给 <code>address voter</code> 赋予投票权，即将 <code>voter</code> 的投票权重设为 1，存入 <code>voters</code> 状态变量。</p>
<p>上面这个函数只有投票发起人 <code>chairperson</code> 可以调用。这里用到了 <code>require((msg.sender == chairperson) &amp;&amp; !voters[voter].voted)</code> 函数。如果<code>require</code> 中表达式结果为 <code>false</code>，这次调用会中止，且回滚所有状态和以太币余额的改变到调用前。但已消耗的 <code>Gas</code> 不会返还。</p>
<p>下面一段是整段代码的重点，其作用是委托其他人代理投票，基本思路是：</p>
<ol>
<li>使用<code>require</code>判断委托人是否已投票（若投过票再委托则重复投票），并判断被委托对象是否是自己</li>
<li>当判断被委托人不是0地址（主席）时，被委托人代理委托人的票，【绕口警告】由于被委托人也可能委托了别人，因此这里需要一直循环直到找到最后没有委托别人的被委托人为止！</li>
<li>委托人找到对应的被委托人，委托人已投票（避免重复投票）</li>
<li>判断被委托人是否已投票，若投了票则将被委托人投的提案票数加上委托人的权重，若未投票则令被委托人的权重加上委托人的权重（以后投票自然相当于投两票）</li>
</ol>
<p>注：该函数使用了 <code>while</code> 循环，这里合约编写者需要十分谨慎，防止调用者消耗过多 <code>Gas</code>，甚至出现死循环。</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">delegate</span>(<span class="params">address to</span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">    Voter storage sender = voters[msg.sender];</span><br><span class="line">    <span class="built_in">require</span>(!sender.voted, <span class="string">"You already voted."</span>);</span><br><span class="line">    <span class="built_in">require</span>(to != msg.sender, <span class="string">"Self-delegation is disallowed."</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (voters[to].delegate != address(<span class="number">0</span>)) &#123;</span><br><span class="line">    	to = voters[to].delegate;</span><br><span class="line">    	<span class="built_in">require</span>(to != msg.sender, <span class="string">"Found loop in delegation."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    sender.voted = <span class="literal">true</span>;</span><br><span class="line">    sender.delegate = to;</span><br><span class="line">    Voter storage delegate_ = voters[to];</span><br><span class="line">    <span class="keyword">if</span> (delegate_.voted) &#123;</span><br><span class="line">    	proposals[delegate_.vote].voteCount += sender.weight;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    	delegate_.weight += sender.weight;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>投票部分仅是几个简单的条件判断：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">vote</span>(<span class="params">uint proposal</span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        Voter storage sender = voters[msg.sender];</span><br><span class="line">        <span class="built_in">require</span>(sender.weight != <span class="number">0</span>, <span class="string">"Has no right to vote"</span>);</span><br><span class="line">        <span class="built_in">require</span>(!sender.voted, <span class="string">"Already voted."</span>);</span><br><span class="line">        sender.voted = <span class="literal">true</span>;</span><br><span class="line">        sender.vote = proposal;</span><br><span class="line">        proposals[proposal].voteCount += sender.weight;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>用 <code>voters[msg.sender]</code> 获取投票人，即此次调用的发起人。接下来检查是否是重复投票，如果不是，进行投票后相关状态变量的更新。</p>
<p>接下来是计算获胜提案:</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">winningProposal</span>(<span class="params"></span>) <span class="title">public</span> <span class="title">view</span></span></span><br><span class="line"><span class="function">        <span class="title">returns</span> (<span class="params">uint winningProposal_</span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    uint winningVoteCount = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (uint p = <span class="number">0</span>; p &lt; proposals.length; p++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (proposals[p].voteCount &gt; winningVoteCount) &#123;</span><br><span class="line">            winningVoteCount = proposals[p].voteCount;</span><br><span class="line">            winningProposal_ = p;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>returns (uint winningProposal)</code> 指定了函数的返回值类型，<code>constant</code> 表示该函数不会改变合约状态变量的值。</p>
<p>最后是查询获胜者名称：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">winnerName</span>(<span class="params"></span>) <span class="title">public</span> <span class="title">view</span></span></span><br><span class="line"><span class="function">        <span class="title">returns</span> (<span class="params">bytes32 winnerName_</span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    winnerName_ = proposals[winningProposal()].name;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里采用内部调用 <code>winningProposal()</code> 函数的方式获得获胜提案。如果需要采用外部调用，则需要写为 <code>this.winningProposal()</code>。</p>
<p><strong>参考自：</strong></p>
<p><a href="https://www.bilibili.com/video/BV1sJ411D72u" target="_blank" rel="noopener">尚硅谷区块链全套Go语言→GoWeb→以太坊→项目实战</a><br><a href="http://cw.hubwiz.com/card/c/web3.js-1.0/" target="_blank" rel="noopener">web3.js 1.0中文手册</a></p>
]]></content>
      <categories>
        <category>Blockchain</category>
      </categories>
  </entry>
  <entry>
    <title>区块链入门（四）web3js</title>
    <url>/posts/5521455b.html</url>
    <content><![CDATA[<font color="red">注：本教程为技术教程，不谈论且不涉及炒作任何数字货币 </font>

<h2 id="一、以太坊客户端"><a href="#一、以太坊客户端" class="headerlink" title="一、以太坊客户端"></a>一、以太坊客户端</h2><h3 id="1-1、什么是以太坊客户端"><a href="#1-1、什么是以太坊客户端" class="headerlink" title="1.1、什么是以太坊客户端"></a>1.1、什么是以太坊客户端</h3><ul>
<li>以太坊客户端是一个软件应用程序，它实现以太坊规范并通过p2p网络与其他以太坊客户端进行通信。如果不同的以太坊客户端符合参考规范和标准化通信协议，则可以进行相互操作。</li>
<li>以太坊是一个开源项目，由“黄皮书”正式规范定义。除了各种以太坊改进提案之外，此正式规范还定义了以太坊客户端的标准行为。</li>
<li>因为以太坊有明确的正式规范，以太网客户端有了许多独立开发的软件实现，它们之间又可以彼此交互。</li>
</ul>
<h3 id="1-2、基于以太坊规范的网络"><a href="#1-2、基于以太坊规范的网络" class="headerlink" title="1.2、基于以太坊规范的网络"></a>1.2、基于以太坊规范的网络</h3><ul>
<li>存在各种基于以太坊规范的网络，这些网络基本符合以太坊“黄皮书”中定义的形式规范，但它们之间可能相互也可能不相互操作。</li>
<li>这些基于以太坊的网络中有：以太坊，以太坊经典，Ella，Expanse，Ubiq，Musicoin等等。</li>
<li>虽然大多数在协议级别兼容，但这些网络通常具有特殊要求，以太坊客户端软件的维护人员、需要进行微小更改、以支持每个网络的功能或属性</li>
</ul>
<h3 id="1-3、太坊的多种客户端"><a href="#1-3、太坊的多种客户端" class="headerlink" title="1.3、太坊的多种客户端"></a>1.3、太坊的多种客户端</h3><ul>
<li><a href="https://github.com/ethereum/go-ethereum" target="_blank" rel="noopener">go-ethereum ( Go )</a><br>官方推荐，开发使用最多</li>
<li>parity ( Rust )<br>最轻便客户端，在历次以太坊网络攻击中表现卓越</li>
<li><p>cpp-ethereum (C++)</p>
</li>
<li><p>pyethapp (python)</p>
</li>
<li><p>ethereumjs-lib ( javascript )</p>
</li>
<li><p>EthereumJ / Harmony ( Java )</p>
</li>
</ul>
<h3 id="1-4、以太坊全节点"><a href="#1-4、以太坊全节点" class="headerlink" title="1.4、以太坊全节点"></a>1.4、以太坊全节点</h3><ul>
<li>全节点是整个主链的一个副本，存储并维护链上的所有数据，并随时验证新区块的合法性。</li>
<li>区块链的健康和扩展弹性，取决于具有许多独立操作和地理上分散的全节点。每个全节点都可以帮助其他新节点获取区块数据，并提供所有交易和合约的独立验证。</li>
<li>运行全节点将耗费巨大的成本，包括硬件资源和带宽。</li>
<li>以太坊开发不需要在实时网络（主网）上运行的全节点。我们可以使用测试网络的节点来代替，也可以用本地私链，或者使用服务商提供的基于云的以太坊客户端；这些几乎都可以执行所有操作。</li>
</ul>
<h3 id="1-5、远程客户端和轻节点"><a href="#1-5、远程客户端和轻节点" class="headerlink" title="1.5、远程客户端和轻节点"></a>1.5、远程客户端和轻节点</h3><ul>
<li><p>远程客户端</p>
<p>不存储区块链的本地副本或验证块和交易。这些客户端一般只提供钱包的功能，可以创建和广播交易。远程客户端可用于连接到现有网络，MetaMask 就是一个这样的客户端。</p>
</li>
<li><p>轻节点</p>
<p>  不保存链上的区块历史数据，只保存区块链当前的状态。轻节点可以对块和交易进行验证。</p>
</li>
</ul>
<ul>
<li><p>全节点的优缺点</p>
<ul>
<li>优点<ul>
<li>为以太坊网络的灵活性和抗审查性提供有力支持</li>
<li>权威地验证所有交易</li>
<li>可以直接与公众区块链上的任何合约交互</li>
<li>可以离线查询区块链状态（账户、合约等）</li>
<li>可以直接把自己的合约部署到公共区块链中</li>
</ul>
</li>
<li>缺点<ul>
<li>需要巨大的硬件和带宽资源，而且会不断增长</li>
<li>第一次下载往往需要几天才能完全同步</li>
<li>必须及时维护、升级并保持在线状态以同步区块</li>
</ul>
</li>
</ul>
<h5 id="公共测试网络节点的优缺点"><a href="#公共测试网络节点的优缺点" class="headerlink" title="公共测试网络节点的优缺点"></a>公共测试网络节点的优缺点</h5><ul>
<li>优点<ul>
<li>一个testnet节点需要同步和存储更少的数据，大约10GB，具体取决于不同的网络</li>
<li>一个testnet节点一般可以在几个小时内完成同步</li>
<li>部署合约或进行交易只需要发送测试以太，可以从”水龙头“免费获得</li>
<li>测试网络是公共区块链，有许多其他用户和合约运行（区别于私链）</li>
</ul>
</li>
<li>缺点<ul>
<li>测试网络上使用测试以太没有价值。因此无法测试交易对手的安全性，因为没有任何利害关系</li>
<li>测试网络上的测试无法涵盖所有真实主网特性。例如：交易费用虽然是发送交易所必需的，但由于gas免费，因此 testnet上往往不会考虑。而且一般来说，测试网络不会像主网一样经常拥堵</li>
</ul>
</li>
</ul>
<h5 id="本地私链的优缺点"><a href="#本地私链的优缺点" class="headerlink" title="本地私链的优缺点"></a>本地私链的优缺点</h5><ul>
<li>优点<ul>
<li>磁盘上几乎没有数据，也不同步别的数据，是一个完全干净的环境</li>
<li>无需获取测试以太，可以分配任意以太，也可以随时自己挖矿获得</li>
<li>没有其他用户与合约，无外部干扰</li>
</ul>
</li>
<li>缺点<ul>
<li>没有其他用户意味与公链的行为不同，发送的交易并不存在空间或交易顺序的竞争</li>
<li>除自己之外没有矿工意味着挖矿更容易预测，因此无法测试公链上发生的某些情况</li>
<li>没有其他合约意味着必须部署要测试的所有内容，包括所有的依赖项和合约库</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>我们的教程主要基于本地私链的搭建，以后的交易等也主要基于我们的私链，因此以太坊客户端及私链的搭建在我们本次学习中至关重要。</p>
<p><strong>JSON-RPC</strong></p>
<ul>
<li>以太坊客户端提供了API 和一组远程调用（RPC）命令，这些命令被编码为 JSON。这被称为 JSON-RPC API。本质上，JSON-RPCAPI 就是一个接口，允许我们编写的程序使用以太坊客户端作为网关，访问以太坊网络和链上数据。</li>
<li>通常，RPC 接口作为一个 HTTP 服务，端口设定为 8545。出于安全原因，默认情况下，它仅限于接受来自localhost 的连接。</li>
<li>要访问JSON-RPC API，我们可以使用编程语言编写的专用库，例如JavaScript的 web3.js。</li>
<li>或者也可以手动构建HTTP请求并发送/接收JSON编码的请求，如：</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">curl -X POST -H <span class="string">"Content-Type:application/json"</span> --data <span class="string">'&#123;"jsonrpc":"2.0","method":"web3_clientVersion","params":[],"id":1&#125;'</span> http:<span class="comment">//127.0.0.1:8545</span></span><br></pre></td></tr></table></figure>
<h2 id="二、用-Geth-搭建以太坊私链"><a href="#二、用-Geth-搭建以太坊私链" class="headerlink" title="二、用 Geth 搭建以太坊私链"></a>二、用 Geth 搭建以太坊私链</h2><h3 id="2-1安装-go"><a href="#2-1安装-go" class="headerlink" title="2.1安装 go"></a>2.1安装 go</h3><p>大家首先输入<code>go version</code>查看自己是否配置成功go环境，若不成功参考下面博客：</p>
<p><a href="https://blog.csdn.net/qq_44702847/article/details/108597386" target="_blank" rel="noopener">go ： GoLand安装及环境配置</a></p>
<p>若成功则如下图所示</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part3-1.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 1</div>
</center>



<h3 id="2-2-安装-Geth"><a href="#2-2-安装-Geth" class="headerlink" title="2.2 安装 Geth"></a>2.2 安装 Geth</h3><p>安装 Geth 有很多种方式，这里主要就 Linux 环境给出两种方法：系统包管理器（apt-get）安装和源码安装。更加推荐大家用源码安装，在整个过程中可以看到 Geth 各组件的构建步骤。</p>
<p>其他OS安装方法见<a href="https://geth.ethereum.org/docs/install-and-build/installing-geth" target="_blank" rel="noopener">本教程</a></p>
<p><strong>方法一、apt-get</strong></p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">sudo apt-<span class="keyword">get</span> install software-properties-common</span><br><span class="line">sudo add-apt-repository -y ppa:ethereum/ethereum</span><br><span class="line">sudo apt-<span class="keyword">get</span> update</span><br><span class="line">sudo apt-<span class="keyword">get</span> install ethereum</span><br></pre></td></tr></table></figure>
<p><strong>方法二、源码安装</strong></p>
<ol>
<li>克隆 github 仓库我们的第一步是克隆 git 仓库，以获取源代码的副本。</li>
</ol>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">git clone https:<span class="comment">//github.com/ethereum/go-ethereum.git</span></span><br></pre></td></tr></table></figure>
<ol>
<li>从源码构建 Geth要构建 Geth，切换到下载源代码的目录并使用 make 命令：</li>
</ol>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">cd go-ethereum</span><br><span class="line">make geth</span><br></pre></td></tr></table></figure>
<p>如果一切顺利，我们将看到 Go 编译器构建每个组件，直到它生成 geth 可执行文件：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">build/env.sh go run build/ci.go install ./cmd/geth</span><br><span class="line">&gt;&gt;&gt; <span class="regexp">/usr/</span>local/go/bin/go install -ldflags -X</span><br><span class="line">main.gitCommit=<span class="number">58</span>a1e13e6dd7f52a1d5e67bee47d23fd6cfdee5c -v ./cmd/geth</span><br><span class="line">github.com/ethereum/go-ethereum/common/hexutil</span><br><span class="line">github.com/ethereum/go-ethereum/common/math</span><br><span class="line">github.com/ethereum/go-ethereum/crypto/sha3 github.com/ethereum/go-ethereum/rlp</span><br><span class="line">github.com/ethereum/go-ethereum/crypto/secp256k1</span><br><span class="line">github.com/ethereum/go-ethereum/common [...]</span><br><span class="line">github.com/ethereum/go-ethereum/cmd/utils</span><br><span class="line">github.com/ethereum/go-ethereum/cmd/geth Done building. Run <span class="string">"build/bin/geth"</span> to</span><br><span class="line">launch geth.</span><br></pre></td></tr></table></figure>
<p> 查看 geth version，确保在真正运行之前安装正常：</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part3-2.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 2</div>
</center>



<h3 id="启动节点同步"><a href="#启动节点同步" class="headerlink" title="启动节点同步"></a>启动节点同步</h3><p>安装好了 Geth，现在我们可以尝试运行一下它。执行下面的命令，geth 就会开始同步区块，并存储在当前目录下。</p>
<p>这里的 —syncmode fast 参数表示我们会以“快速”模式同步区块。在这种模式下，我们只会下载每个区块头和区块体，但不会执行验证所有的交易，直到所有区块同步完毕再去获取一个系统当前的状态。这样就节省了很多交易验证的时间。</p>
 <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">geth –datadir . --syncmode fast</span><br></pre></td></tr></table></figure>
<p>—datadir：后面的参数是区块数据及秘钥存放目录</p>
<p>通常，在同步以太坊区块链时，客户端会一开始就下载并验证每个块和每个交易，也就是说从创世区块开始。 毫无疑问，如果我们不加 —syncmode fast 参数，同步将花费很长时间并且具有很高的资源要求（它将需要更多的 RAM，如果你没有快速存储，则需要很长时间）。有些文章会把这个参数写成 —fast，这是以前快速同步模式的参数写法，现在已经被 –syncmode fast取代。如果我们想同步测试网络的区块，可以用下面的命令：</p>
 <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">geth --testnet --datadir . --syncmode fast</span><br></pre></td></tr></table></figure>
<p>—testnet 这个参数会告诉 geth 启动并连接到最新的测试网络，也就是 Ropsten。测试网络的区块和交易数量会明显少于主网，所以会更快一点。但即使是用快速模式同步测试网络，也会需要几个小时的时间</p>
<h3 id="2-3-搭建自己的私有链"><a href="#2-3-搭建自己的私有链" class="headerlink" title="2.3 搭建自己的私有链"></a>2.3 搭建自己的私有链</h3><p>因为公共网络的区块数量太多，同步耗时太长，我们为了方便快速了解 Geth，可以试着用它来搭一个只属于自己的私链。首先，我们需要创建网络的“创世”（genesis）状态，这写在一个小小的 JSON 文件里（例如，我们将其命名为 genesis.json，保存到当前目录下）：</p>
 <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="string">"config"</span>: &#123;</span><br><span class="line">    <span class="string">"chainId"</span>: <span class="number">15</span></span><br><span class="line">    &#125;,</span><br><span class="line"><span class="string">"difficulty"</span>: <span class="string">"2000"</span>,</span><br><span class="line"><span class="string">"gasLimit"</span>: <span class="string">"2100000"</span>,</span><br><span class="line"><span class="string">"alloc"</span>: &#123;</span><br><span class="line">    <span class="string">"7df9a875a174b3bc565e6424a0050ebc1b2d1d82"</span>: &#123;   <span class="string">"balance"</span>: <span class="string">"300000"</span> &#125;,</span><br><span class="line">    <span class="string">"f41c74c9ae680c1aa78f42e5647a62f353b7bdde"</span>: &#123; <span class="string">"balance"</span>: <span class="string">"400000"</span> &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>genesis.json介绍</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part3-3.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 3</div>
</center>



<p>要创建一条以它作为创世块的区块链，我们可以使用下面的命令：</p>
 <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">geth --datadir . init genesis.json</span><br></pre></td></tr></table></figure>
<p>初始化完成后目录下多了geth和keystore两个文件夹：</p>
<ul>
<li>geth：保存该链上的区块数据</li>
<li>keystore：保存该链上的账户信息</li>
</ul>
<p><strong>可能遇到问题</strong>：</p>
<ul>
<li><p>Fatal: invalid genesis file: missing 0x prefix for hex data：这个错误信息意思很明白，就是你的json文件中，对于16进制数据，需要加上0x前缀</p>
</li>
<li><p>Fatal: invalid genesis file: hex string has odd length: 从Geth 1.6版本开始，设置的十六进制数值，不能是奇数位， 比如不能是0x0，而应该是0x00。</p>
</li>
<li><p>Fatal: failed to write genesis block: genesis has no chain configuration ：这个错误信息，就是说，你的配置文件中，缺少config部分。</p>
</li>
<li><p>Error: invalid sender: 这个错误虽然不会导致私有链初始化时出现失败的情况，但是会在以后的转账（web3.eth.sendTransaction），或者部署智能合约的时候产生。解决方法就是chainId 不能设置为0。 如果你完全按照Geth官方文档上给出的配置文件进行配置，就会产生这个错误。</p>
</li>
</ul>
<p>在当前目录下运行 geth，就会启动这条私链，注意要将 networked 设置为与创世块配置里的chainId 一致。</p>
 <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 简单开启</span></span><br><span class="line">(base) haobo@haobo:~<span class="regexp">/home/m</span>nt/bitcoin/test$ geth --datadir . --networkid <span class="number">150</span> --nodiscover <span class="built_in">console</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 更一般的形式</span></span><br><span class="line">(base) haobo@haobo:~<span class="regexp">/home/m</span>nt/bitcoin/test$ geth --networkid <span class="number">150</span> --datadir <span class="string">"."</span> --identity <span class="string">"kexin"</span> --rpc --rpcport <span class="string">"8545"</span> --rpcaddr <span class="string">"localhost"</span> --port <span class="string">"30303"</span> --nodiscover --allow-insecure-unlock --rpcapi <span class="string">"eth,net,web3,personal,admin,shh,txpool,debug,miner"</span> <span class="built_in">console</span></span><br></pre></td></tr></table></figure>
<p>参数含义：</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part3-4.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 4</div>
</center>



<p>我们可以看到节点正常启动：</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part3-5.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 5</div>
</center>


<p>启动完之后，就可以通过<code>admin.nodeInfo.protocols.eth</code>来获取到刚启动的节点的一些信息（如下），比较上文初始化的配置，相关内容是一致的。</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part3-6.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 6</div>
</center>


<p>恭喜！我们已经成功启动了一条自己的私链。</p>
<h2 id="3、Geth-控制台命令"><a href="#3、Geth-控制台命令" class="headerlink" title="3、Geth 控制台命令"></a>3、Geth 控制台命令</h2><p><code>Geth Console</code> 是一个交互式的 JavaScript 执行环境，其中 &gt; 是命令提示符,里面内置了一些用来操作以太坊的 JavaScript对象，我们可以直接调用这些对象来获取区块链上的相关信息。</p>
<p><strong>这些对象主要包括：</strong></p>
<ul>
<li>eth：主要包含对区块链进行访问和交互相关的方法；</li>
<li>net：主要包含查看 p2p 网络状态的方法；</li>
<li>admin：主要包含与管理节点相关的方法；</li>
<li>miner：主要包含挖矿相关的一些方法；</li>
<li>personal：包含账户管理的方法；</li>
<li>txpool：包含查看交易内存池的方法；</li>
<li>web3：包含以上所有对象，还包含一些通用方法。</li>
</ul>
<p><strong>常用命令有：</strong></p>
<ul>
<li>personal.newAccount()：创建账户；</li>
<li>personal.unlockAccount()：解锁账户；</li>
<li>eth.accounts：枚举系统中的账户；</li>
<li>eth.getBalance()：查看账户余额，返回值的单位是 Wei（Wei 是以太坊中最小货币面额单位，类似比特币中的聪，1 ether = 10^18 Wei）；</li>
<li>eth.blockNumber：列出区块总数；</li>
<li>eth.getTransaction()：获取交易；</li>
<li>eth.getBlock()：获取区块；</li>
<li>miner.start()：开始挖矿；</li>
<li>miner.stop()：停止挖矿；</li>
<li>web3.fromWei()：Wei 换算成以太币；</li>
<li>web3.toWei()：以太币换算成 Wei；</li>
<li>txpool.status：交易池中的状态；</li>
<li>admin.addPeer()：连接到其他节点</li>
</ul>
<h3 id="3-1-操作测试"><a href="#3-1-操作测试" class="headerlink" title="3.1 操作测试"></a>3.1 操作测试</h3><p><strong>3.1.1 创建账户</strong></p>
<p>进入控制台后，可以通过使用命令来与私有链进行交互。创建一个新的账户：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; personal.newAccount()</span><br><span class="line">Passphrase:</span><br><span class="line">Repeat passphrase:</span><br><span class="line"><span class="string">"0xc8248c7ecbfd7c4104923275b99fafb308bbff92"</span></span><br></pre></td></tr></table></figure>
<p>输入两遍密码后，生成账户地址。以同样的方式，可创建多个账户，查看账户：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; eth.accounts</span><br></pre></td></tr></table></figure>
<p>查看账户余额</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; eth.getBalance(eth.accounts[<span class="number">0</span>])</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part3-7.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 7</div>
</center>


<p><strong>3.1.2 挖矿</strong></p>
<p>启动挖矿：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; miner.start(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>其中 <code>start</code> 的参数表示挖矿使用的线程数。第一次启动挖矿会先生成挖矿所需的 <code>DAG</code>文件，这个过程有点慢，等进度达到 100% 后，就会开始挖矿，此时屏幕会被挖矿信息刷屏。</p>
<p>停止挖矿，在 控制台 中输入：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; miner.stop()</span><br></pre></td></tr></table></figure>
<p>挖到一个区块会奖励以太币，挖矿所得的奖励会进入矿工的账户，这个账户叫做 coinbase，默认情况下 coinbase 是本地账户中的第一个账户，可以通过 miner.setEtherbase() 将其他账户设置成 coinbase。</p>
<p>可以使用以下命令，当新区块挖出后，挖矿即可结束。</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; miner.start(<span class="number">1</span>);admin.sleepBlocks(<span class="number">1</span>);miner.stop();</span><br></pre></td></tr></table></figure>
<p><strong>3.1.3 交易</strong></p>
<p>目前，账户 0 已经挖到了 3 个块的奖励，账户 1 的余额还是0：<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; eth.getBalance(eth.accounts[<span class="number">0</span>])</span><br><span class="line"><span class="number">15000000000000000000</span></span><br><span class="line">&gt; eth.getBalance(eth.accounts[<span class="number">1</span>])</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>我们要从账户 0 向账户 1 转账，先解锁账户 0，才能发起交易：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; personal.unlockAccount(eth.accounts[<span class="number">0</span>])</span><br><span class="line">Unlock account <span class="number">0x3443ffb2a5ce3f4b80080791e0fde16a3fac2802</span></span><br><span class="line">Passphrase: </span><br><span class="line"><span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p>发送交易，账户 0 -&gt; 账户 1：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; amount = web3.toWei(<span class="number">5</span>,<span class="string">'ether'</span>)</span><br><span class="line"><span class="string">"5000000000000000000"</span></span><br><span class="line">&gt; eth.sendTransaction(&#123;<span class="attr">from</span>:eth.accounts[<span class="number">0</span>],<span class="attr">to</span>:eth.accounts[<span class="number">1</span>],<span class="attr">value</span>:amount&#125;)</span><br><span class="line">INFO [<span class="number">09</span><span class="number">-12</span>|<span class="number">07</span>:<span class="number">38</span>:<span class="number">12</span>] Submitted transaction                    fullhash=<span class="number">0x9f5e61f3d686f793e2df6378d1633d7a9d1df8ec8c597441e1355112d102a6ce</span> recipient=<span class="number">0x02bee2a1582bbf58c42bbdfe7b8db4685d4d4c62</span></span><br><span class="line"><span class="string">"0x9f5e61f3d686f793e2df6378d1633d7a9d1df8ec8c597441e1355112d102a6ce"</span></span><br></pre></td></tr></table></figure>
<p>此时如果没有挖矿，用 <code>txpool.status</code> 命令可以看到本地交易池中有一个待确认的交易，可以使用 <code>eth.getBlock(&quot;pending&quot;, true).transactions</code>查看当前待确认交易。使用下面命令开始挖矿。</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt;miner.start(<span class="number">1</span>);admin.sleepBlocks(<span class="number">1</span>);miner.stop();</span><br></pre></td></tr></table></figure>
<p>新区块挖出后，挖矿结束，查看账户 1 的余额，已经收到了账户 0 的以太币：<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; web3.fromWei(eth.getBalance(eth.accounts[<span class="number">1</span>]),<span class="string">'ether'</span>)</span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure></p>
<p><strong>3.1.3 查看交易和区块</strong></p>
<p>查看当前区块总数：<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; eth.blockNumber</span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure></p>
<p>通过区块号查看区块：<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; eth.getBlock(<span class="number">4</span>)</span><br></pre></td></tr></table></figure></p>
<p>通过交易 Hash 查看交易（Hash 值包含在上面交易返回值中）：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt;  eth.getTransaction(<span class="string">"0x9f5e61f3d686f793e2df6378d1633d7a9d1df8ec8c597441e1355112d102a6ce"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>3.1.3 其他节点加入</strong></p>
<p>此时，私有链已经通过该节点创建好了，如果其他节点想加入，需要通过以太坊客户端连接到该私有区块网络，并连接该网络的节点来同步区块信息。在其他主机上安装以太坊客户端Geth，通过Geth命令进入该私有区块链，注意要指定相同的网络号。</p>
<p>假设有两个节点：节点一和节点二，NetWorkID 都是 6666，通过下面的步骤就可以从节点一连接到节点二。</p>
<p>首先要知道节点二的 enode 信息，在节点二的 Geth Console 中执行下面的命令查看 enode 信息：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; admin.nodeInfo.enode</span><br><span class="line"><span class="string">"enode://d465bcbd5c34da7f4b8e00cbf9dd18e7e2c38fbd6642b7435f340c7d5168947ff2b822146e1dc1b07e02f7c15d5ca09249a92f1d0caa34587c9b2743172259ee@[::]:30303"</span></span><br></pre></td></tr></table></figure>
<p>然后在节点一的 Geth Console 中执行 <code>admin.addPeer()</code>，就可以连接到节点二：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; admin.addPeer(<span class="string">"enode://d465bcbd5c34da7f4b8e00cbf9dd18e7e2c38fbd6642b7435f340c7d5168947ff2b822146e1dc1b07e02f7c15d5ca09249a92f1d0caa34587c9b2743172259ee@[::]:30303"</span>)</span><br></pre></td></tr></table></figure>
<p><code>addPeer()</code> 的参数就是节点二的 enode 信息，注意要把 enode 中的 <code>[::]</code> 替换成节点二的 IP 地址。连接成功后，节点二就会开始同步节点一的区块，同步完成后，任意一个节点开始挖矿，另一个节点会自动同步区块，向任意一个节点发送交易，另一个节点也会收到该笔交易。</p>
<p>通过 <code>admin.peers</code>可以查看连接到的其他节点信息，通过 <code>net.peerCount</code>可以查看已连接到的节点数量。</p>
<p>除了上面的方法，也可以在启动节点的时候指定<code>--bootnodes</code>选项连接到其他节点。</p>
<blockquote>
<p>如果只是自己测试开发使用，建议使用dev环境，在需要在启动时增加<code>–dev</code>参数即可，在dev模式下会监听交易，一旦有交易发送就会打包然后挖矿确认，且默认的<code>account[0]</code>开发者账户初始有一大堆以太币。</p>
</blockquote>
<h2 id="3、智能合约操作"><a href="#3、智能合约操作" class="headerlink" title="3、智能合约操作"></a>3、智能合约操作</h2><h3 id="3-1、创建和编译智能合约"><a href="#3-1、创建和编译智能合约" class="headerlink" title="3.1、创建和编译智能合约"></a>3.1、创建和编译智能合约</h3><p>经过part2的学习大家已经基本上掌握了Solidity，接下来我们编写一个智能合约：</p>
<p>该合约包含一个方法 multiply()，将输入的两个数相乘后输出：<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line">contract TestContract</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">multiply</span>(<span class="params">uint a, uint b</span>) <span class="title">returns</span> (<span class="params">uint</span>)</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> a * b;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>将上面的代码复制到Remix编辑器里，程序将自动完成编译。</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part3-8.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 8</div>
</center>


<p>点击 run 在Environment中设选择JavaScript VM, Value可设置为1，点击Deploy，则可创建该部署智能合约的交易。</p>
<p>因为我们要将该智能合约部署到私有链上，需要得到智能合约编译后的EVM二进制码和JSON ABI（Application Binary Interface）。将生成的交易保存到scenario.json文件，点击箭头所指按钮</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part3-9.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 9</div>
</center>



<p>其中38-65行为该智能合约的ABI（注意前面还有一个[符号），ABI指定了合约接口，包括可调用的合约方法、变量、事件等。</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part3-10.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 10</div>
</center>


<p>input`字段为合约EVM二进制码，可点击直接复制。</p>
<p>在Linux下可以直接使用安装好的编译器进行编译，把合约代码保存到文件名为testContract.sol 里,通过下面两个命令分别得到EVM二进制码和JSON ABI。</p>
<p>如果没有安装solc先执行</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">sudo snap install solc</span><br></pre></td></tr></table></figure>
<p>接下来执行<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">$solc --bin testContract.sol</span><br><span class="line">$solc --abi testContract.sol</span><br></pre></td></tr></table></figure></p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part3-11.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 11</div>
</center>


<h3 id="3-2、部署智能合约"><a href="#3-2、部署智能合约" class="headerlink" title="3.2、部署智能合约"></a>3.2、部署智能合约</h3><p>回到 Geth 的控制台，用变量 code 和 abi 记录上面两个值：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; code = <span class="string">"608060405234801561001057600080fd5b5060b88061001f6000396000f3fe6080604052348015600f57600080fd5b506004361060285760003560e01c8063165c4a1614602d575b600080fd5b606060048036036040811015604157600080fd5b8101908080359060200190929190803590602001909291905050506076565b6040518082815260200191505060405180910390f35b600081830290509291505056fea265627a7a7231582049ecffb2740a6e31f7c8fbf4a928b88d3a95f417b985dc23cd1ad4c06a9b043864736f6c63430005100032"</span></span><br><span class="line">&gt; abi = [&#123;</span><br><span class="line">    <span class="string">"0xd1ef8ab8f12bde83ebaee1be4183c75f45ab5835643812016a7751173bfb9dc0"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"constant"</span>: <span class="literal">true</span>,</span><br><span class="line">        <span class="string">"inputs"</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="string">"name"</span>: <span class="string">"a"</span>,</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="string">"name"</span>: <span class="string">"b"</span>,</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">          &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"multiply"</span>,</span><br><span class="line">        <span class="string">"outputs"</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="string">"name"</span>: <span class="string">""</span>,</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">          &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="string">"payable"</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">"stateMutability"</span>: <span class="string">"view"</span>,</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"function"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;]</span><br></pre></td></tr></table></figure>
<p>使用账户 0 来部署合约，首先解锁账户：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; personal.unlockAccount(eth.accounts[<span class="number">0</span>])</span><br><span class="line">Unlock account <span class="number">0xb51654f60dee35265558a1d2e61468fe00f12888</span></span><br><span class="line">Passphrase:</span><br><span class="line"><span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p>创建合约实例，发送部署合约的交易：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt; myContract = eth.contract(abi)   </span><br><span class="line">...</span><br><span class="line">&gt; contract = myContract.new(&#123;<span class="attr">from</span>:eth.accounts[<span class="number">0</span>],<span class="attr">data</span>:code,<span class="attr">gas</span>:<span class="number">1000000</span>&#125;)</span><br></pre></td></tr></table></figure>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part3-12.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 12</div>
</center>

<p>此时如果没有挖矿，用 <code>txpool.status</code> 命令可以看到本地交易池中有一个待确认的交易。使用 <code>miner.start()</code> 命令开始挖矿，一段时间后交易会被确认。通过查询该交易可得到合约地址，使用命令：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt;eth.getTransactionReceipt(<span class="string">"0x085b66b2591ee31c3ad58a66ca485bd19bea6c1fc8ca7550a896853ab52855a6"</span>)</span><br><span class="line">contractAddress: <span class="string">"0xd92845cc4bffc1d6a4b6a389933b88880d5ded24"</span></span><br></pre></td></tr></table></figure>
<h3 id="3-3、调用智能合约"><a href="#3-3、调用智能合约" class="headerlink" title="3.3、调用智能合约"></a>3.3、调用智能合约</h3><p>使用以下命令通过发送交易来调用合约，sendTransaction 方法的前几个参数应该与合约中 multiply 方法的输入参数对应。这种情况下，交易会通过挖矿记录到区块链中：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt;contract.multiply.sendTransaction(<span class="number">2</span>, <span class="number">4</span>, &#123;<span class="attr">from</span>:eth.accounts[<span class="number">0</span>]&#125;)</span><br></pre></td></tr></table></figure>
<p>在本地运行该方法可直接查看返回结果，不会记录到区块链中，命令如下：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt;contract.multiply.call(<span class="number">2</span>，<span class="number">4</span>)</span><br><span class="line"><span class="number">8</span></span><br></pre></td></tr></table></figure>
<p>如果其他节点要调用这个已经部署好的合约，需要知道该合约的地址以及ABI。可以通过发送交易调用，也可以本地调用。我们以本地调用为例。<br>创建合约实例：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt;abi = [&#123;</span><br><span class="line">    <span class="string">"0xd1ef8ab8f12bde83ebaee1be4183c75f45ab5835643812016a7751173bfb9dc0"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"constant"</span>: <span class="literal">true</span>,</span><br><span class="line">        <span class="string">"inputs"</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="string">"name"</span>: <span class="string">"a"</span>,</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="string">"name"</span>: <span class="string">"b"</span>,</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">          &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"multiply"</span>,</span><br><span class="line">        <span class="string">"outputs"</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="string">"name"</span>: <span class="string">""</span>,</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">          &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="string">"payable"</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">"stateMutability"</span>: <span class="string">"view"</span>,</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"function"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;]</span><br><span class="line">&gt;sample=eth.contract(abi)</span><br><span class="line">&gt;samplecontract=sample.at(<span class="string">"0xd92845cc4bffc1d6a4b6a389933b88880d5ded24"</span>)</span><br></pre></td></tr></table></figure>
<p>调用合约</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&gt;samplecontract.multiply.call(<span class="number">2</span>，<span class="number">4</span>)</span><br><span class="line"><span class="number">8</span></span><br></pre></td></tr></table></figure>
<h2 id="4、web3-js-简介"><a href="#4、web3-js-简介" class="headerlink" title="4、web3.js 简介"></a>4、web3.js 简介</h2><p>我们除了通过Geth的JavaScript Console进行交互以外，还有许多第三方库可以使用，方便开发基于以太坊区块链的应用：</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part3-13.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 13</div>
</center>


<p>本文使用web3.js与Geth客户端交互，首先搭建开发环境。</p>
<h3 id="4-1-环境搭建"><a href="#4-1-环境搭建" class="headerlink" title="4.1 环境搭建"></a>4.1 环境搭建</h3><p><strong>4.1.1 node.js安装</strong></p>
<p>更新源<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">sudo apt-<span class="keyword">get</span> update</span><br><span class="line">sudo apt-<span class="keyword">get</span> install -y python-software-properties software-properties-common</span><br><span class="line">sudo add-apt-repository ppa:chris-lea/node.js</span><br><span class="line">sudo apt-<span class="keyword">get</span> update</span><br></pre></td></tr></table></figure></p>
<p>node.js、npm安装<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">sudo apt-<span class="keyword">get</span> install nodejs</span><br><span class="line">sudo apt install nodejs-legacy</span><br><span class="line">sudo apt install npm</span><br></pre></td></tr></table></figure><br>安装完后，可以通过 <code>node --version npm --version</code> 查看是否安装成功及版本号。npm 包管理工具随 node 一起安装，如果版本太低，建议升到新版本。</p>
<p><strong>4.1.2 web3.js模块安装</strong><br>使用npm可完成本地安装、全局安装模块。<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">npm install -global &lt;package name&gt; <span class="comment">//全局安装</span></span><br><span class="line">npm install &lt;package name&gt; <span class="comment">//本地安装</span></span><br></pre></td></tr></table></figure></p>
<p>我这里选择使用本地安装模块，这样方便开发的应用移植、上线等。创建一个工程文件夹etherjs。在该文件夹下初始化一个新的 package.json 文件，使用下面命令自动生成。<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">npm init -y</span><br></pre></td></tr></table></figure></p>
<p>本地安装并添加模块名到 package.json<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">npm install &lt;package name&gt; --save</span><br><span class="line">或者npm install &lt;package name&gt; --save-dev</span><br></pre></td></tr></table></figure></p>
<p>区别在于—save-dev 是你开发时候依赖的东西，—save 是你发布之后还依赖的东西。一般使用—save。<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">npm install web3 --save</span><br></pre></td></tr></table></figure></p>
<p>如果这样安装不成功，使用下面命令安装指定版本：<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">npm install web3@^<span class="number">0.20</span><span class="number">.1</span> --save</span><br></pre></td></tr></table></figure></p>
<p><strong>4.1.3 solc.js模块安装</strong><br>solc是用来编译智能合约的模块<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">npm install solc --save</span><br></pre></td></tr></table></figure></p>
<p><strong>4.1.4 编译器——Visual Studio Code</strong></p>
<p>这里选择Visual Studio Code，适合node.js开发，集成的终端可以很方便运行程序。</p>
<p>安装Ubuntu Make<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:ubuntu-desktop/ubuntu-make</span><br><span class="line">sudo apt-<span class="keyword">get</span> update</span><br><span class="line">sudo apt-<span class="keyword">get</span> install ubuntu-make</span><br></pre></td></tr></table></figure><br>安装visual-studio-code<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">umake web visual-studio-code</span><br></pre></td></tr></table></figure><br>安装完成后，直接搜索Visual Studio Code应用，把图标拖拽到Unity启动器上，就可以方便使用了。</p>
<h3 id="4-2-web3-js-介绍"><a href="#4-2-web3-js-介绍" class="headerlink" title="4.2 web3.js 介绍"></a>4.2 web3.js 介绍</h3><p>web3js 的全称是Web3 JavaScript app API，它是一个JavaScript API库。要使DApper在以太坊上运行，我们可以使用web3.js库提供的web3对象。web3.js通过RPC调用与本地节点通信，它可以用于任何暴露了RPC层的以太坊节点，web3包含了eth对象 - web3.eth（专门与以太坊区块链交互）和 shh对象 - web3.shh（用于与 Whisper交互）[Whisper是以太坊生态系统的一部分，主要用来做消息传递]</p>
<p>如果我们想要在以太坊上开发合约，目前来说最方便的方法就是调用Web3.js库，它会给我们一个Web3对象。我们进入geth控制台，直接键入web3就可以看到所有的方法。下面主要介绍如何通过web3js创建合约并调用</p>
<p><strong>4.2.1异步回调（callback）</strong></p>
<ul>
<li>web3js API 设计的最初目的，主要是为了和本地 RPC 节点共同使用，所以默认情况下发送的是同步 HTTP 请求</li>
<li>如果要发送异步请求，可以在函数的最后一个参数位置上，传入一个回调函数。回调函数是可选（optioanl）的</li>
<li><p>我们一般采用的回调风格是所谓的“错误优先”，例如：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">web3.eth.getBlock(<span class="number">48</span>, <span class="function"><span class="keyword">function</span>(<span class="params">error, result</span>)</span>&#123;</span><br><span class="line"><span class="keyword">if</span>(!error)</span><br><span class="line">　　<span class="built_in">console</span>.log(<span class="built_in">JSON</span>.stringify(result));</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">　　<span class="built_in">console</span>.error(error);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>4.2.2 回调 Promise 事件（v1.0.0）</strong></p>
<ul>
<li>为了帮助 web3 集成到不同标准的所有类型项目中，1.0.0 版本提供了多种方式来处理异步函数。大多数的 web3 对象允许将一个回调函数作为最后一个函数参数传入，同时会返回一个promise 用于链式函数调用。</li>
<li>以太坊作为一个区块链系统，一次请求具有不同的结束阶段。为了满足这样的要求，1.0.0 版本将这类函数调用的返回值包成一个“承诺事件”（promiEvent），这是一个 promise 和EventEmitter 的结合体。</li>
<li>PromiEvent 的用法就像 promise 一样，另外还加入了.on，.once 和.off方法</li>
</ul>
 <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">web3.eth.sendTransaction(&#123;<span class="attr">from</span>: <span class="string">'0x123...'</span>, <span class="attr">data</span>: <span class="string">'0x432...'</span>&#125;)</span><br><span class="line">.once(<span class="string">'transactionHash'</span>, <span class="function"><span class="keyword">function</span>(<span class="params">hash</span>)</span>&#123; ... &#125;)</span><br><span class="line">.once(<span class="string">'receipt'</span>, <span class="function"><span class="keyword">function</span>(<span class="params">receipt</span>)</span>&#123; ... &#125;)</span><br><span class="line">.on(<span class="string">'confirmation'</span>, <span class="function"><span class="keyword">function</span>(<span class="params">confNumber, receipt</span>)</span>&#123; ... &#125;)</span><br><span class="line">.on(<span class="string">'error'</span>, <span class="function"><span class="keyword">function</span>(<span class="params">error</span>)</span>&#123; ... &#125;)</span><br><span class="line">.then(<span class="function"><span class="keyword">function</span>(<span class="params">receipt</span>)</span>&#123; <span class="comment">// will be fired once the receipt is mined &#125;);</span></span><br></pre></td></tr></table></figure>
<p><strong>4.2.3 应用二进制接口（ABI）</strong></p>
<ul>
<li>web3.js 通过以太坊智能合约的 json 接口（Application Binary Interface，ABI）创建一个 JavaScript 对象，用来在 js代码中描述\</li>
<li>函数（functions）</li>
<li>type：函数类型，默认“function”，也可能是“constructor”</li>
<li>constant, payable, stateMutability：函数的状态可变性</li>
<li>inputs, outputs: 函数输入、输出参数描述列表</li>
<li>事件（events）</li>
<li>type：类型，总是“event”</li>
<li>inputs：输入对象列表，包括 name、type、indexed</li>
</ul>
<p><strong>4.2.4 批处理请求（batch requests）</strong></p>
<ul>
<li>批处理请求允许我们将请求排序，然后一起处理它们。</li>
<li>注意：批量请求不会更快。实际上，在某些情况下，一次性地发出许多请求会更快，因为请求是异步处理的。</li>
<li>批处理请求主要用于确保请求的顺序，并串行处理。</li>
</ul>
 <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> batch = web3.createBatch();</span><br><span class="line">batch.add(web3.eth.getBalance.request(<span class="string">'0x0000000000000000</span></span><br><span class="line"><span class="string">000000000000000000000000'</span>, <span class="string">'latest'</span>, callback));</span><br><span class="line">batch.add(web3.eth.contract(abi).at(address).balance.request(a</span><br><span class="line">ddress, callback2));</span><br><span class="line">batch.execute();</span><br></pre></td></tr></table></figure>
<p><strong>4.2.5 大数处理（big numbers）</strong></p>
<ul>
<li>JavaScript 中默认的数字精度较小，所以web3.js 会自动添加一个依赖库 BigNumber，专门用于大数处理</li>
<li><p>对于数值，我们应该习惯把它转换成 BigNumber 对象来处理</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> balance = <span class="keyword">new</span></span><br><span class="line">BigNumber(<span class="string">'131242344353464564564574574567456'</span>);</span><br><span class="line"><span class="comment">// or var balance = web3.eth.getBalance(someAddress);</span></span><br><span class="line">balance.plus(<span class="number">21</span>).toString(<span class="number">10</span>);</span><br><span class="line"><span class="comment">//"131242344353464564564574574567477"</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>BigNumber.toString(10) 对小数只保留20位浮点精度。所以推荐的做法是，我们内部总是用 wei 来表示余额（大整数），只有在需要显示给用户看的时候才转换为ether或其它单位</p>
</li>
</ul>
<h3 id="4-3-常用-API-——-基本信息查询"><a href="#4-3-常用-API-——-基本信息查询" class="headerlink" title="4.3 常用 API —— 基本信息查询"></a>4.3 常用 API —— 基本信息查询</h3><p><strong>4.3.1 查看 web3 版本</strong></p>
<ul>
<li>v0.2x.x：web3.version.api</li>
<li>v1.0.0：web3.version</li>
</ul>
<p>查看 web3 连接到的节点版本（ clientVersion ）</p>
<ul>
<li>同步：web3.version.node</li>
<li>异步：web3.version.getNode((error,result)=&gt;{console.log(result)})</li>
<li>v1.0.0：web3.eth.getNodeInfo().then(console.log)</li>
</ul>
<p><strong>4.3.2 基本信息查询</strong></p>
<p>获取 network id</p>
<ul>
<li>同步：web3.version.network</li>
<li>异步：web3.version.getNetwork((err, res)=&gt;{console.log(res)})</li>
<li>v1.0.0：web3.eth.net.getId().then(console.log)</li>
</ul>
<p>获取节点的以太坊协议版本</p>
<ul>
<li>同步：web3.version.ethereum</li>
<li>异步：web3.version.getEthereum((err, res)=&gt;{console.log(res)}</li>
<li>v1.0.0：web3.eth.getProtocolVersion().then(console.log)</li>
</ul>
<p><strong>4.3.3 网络状态查询</strong></p>
<p>是否有节点连接 / 监听，返回 true/false</p>
<ul>
<li>同步：web3.isConnect() 或者 web3.net.listening</li>
<li>异步：web3.net.getListening((err,res)=&gt;console.log(res))</li>
<li>v1.0.0：web3.eth.net.isListening().then(console.log)</li>
</ul>
<p>查看当前连接的 peer 节点</p>
<ul>
<li>同步：web3.net.peerCount</li>
<li>异步：web3.net.getPeerCount((err,res)=&gt;console.log(res))</li>
<li>v1.0.0：web3.eth.net.getPeerCount().then(console.log)</li>
</ul>
<p><strong>4.3.4 Provider</strong></p>
<p>查看当前设置的 web3 provider</p>
<ul>
<li>web3.currentProvider</li>
</ul>
<p>查看浏览器环境设置的 web3 provider （ v1.0.0 ）</p>
<ul>
<li>web3.givenProvider</li>
</ul>
<p>设置 provider</p>
<ul>
<li>web3.setProvider(provider)</li>
<li>web3.setProvider(new web3.providers.HttpProvider(‘<a href="http://localhost:8545" target="_blank" rel="noopener">http://localhost:8545</a>‘))</li>
</ul>
<h3 id="4-4-web3-通用工具方法"><a href="#4-4-web3-通用工具方法" class="headerlink" title="4.4 web3 通用工具方法"></a>4.4 web3 通用工具方法</h3><p>以太单位转换</p>
<ul>
<li>web3.fromWei web3.toWei数据类型转换</li>
<li>web3.toString web3.toDecimal web3.toBigNumber字符编码转换</li>
<li>web3.toHex web3.toAscii web3.toUtf8 web3.fromUtf8地址相关</li>
<li>web3.isAddress web3.toChecksumAddress</li>
</ul>
<h3 id="4-5-web3-eth"><a href="#4-5-web3-eth" class="headerlink" title="4.5 web3.eth"></a>4.5 web3.eth</h3><p><strong>4.5.1 账户相关</strong></p>
<p>coinbase 查询</p>
<ul>
<li>同步：web3.eth.coinbase</li>
<li>异步：web3.eth.getCoinbase( (err, res)=&gt;console.log(res) )</li>
<li>v1.0.0：web3.eth.getCoinbase().then(console.log)</li>
</ul>
<p>账户查询</p>
<ul>
<li>同步：web3.eth.accounts</li>
<li>异步：web3.eth.getAccounts( (err, res)=&gt;console.log(res) )</li>
<li>v1.0.0：web3.eth.getAccounts().then(console.log)</li>
</ul>
<p><strong>4.5.2 区块相关</strong></p>
<p>区块高度查询</p>
<ul>
<li>同步：web3.eth. blockNumber</li>
<li>异步：web3.eth.getBlockNumber( callback )</li>
</ul>
<p>gasPrice 查询</p>
<ul>
<li>同步：web3.eth.gasPrice</li>
<li>异步：web3.eth.getGasPrice( callback )</li>
</ul>
<p>区块查询</p>
<ul>
<li>同步：web3.eth.getBlockNumber( hashStringOrBlockNumber[ ,returnTransactionObjects] )</li>
<li>异步：web3.eth.getBlockNumber( hashStringOrBlockNumber, callback )</li>
</ul>
<p>块中交易数量查询</p>
<ul>
<li>同步：web3.eth.getBlockTransactionCount( hashStringOrBlockNumber )</li>
<li>异步：web3.eth.getBlockTransactionCount( hashStringOrBlockNumber, callback )</li>
</ul>
<p><strong>4.5.3 交易相关</strong></p>
<p>余额查询</p>
<ul>
<li>同步：web3.eth.getBalance(addressHexString [, defaultBlock])</li>
<li>异步：web3.eth.getBalance(addressHexString [, defaultBlock][, callback])</li>
</ul>
<p>交易查询</p>
<ul>
<li>同步：web3.eth.getTransaction(transactionHash)</li>
<li>异步：web3.eth.getTransaction(transactionHash [, callback])</li>
</ul>
<p>交易执行相关</p>
<ul>
<li>交易收据查询（已进块）</li>
<li>同步：web3.eth.getTransactionReceipt(hashString)</li>
<li>异步：web3.eth.getTransactionReceipt(hashString [,callback])</li>
<li>估计 gas 消耗量</li>
<li>同步：web3.eth.estimateGas(callObject)</li>
<li>异步：web3.eth.estimateGas(callObject [, callback])</li>
</ul>
<p><strong>4.5.4 发送交易</strong></p>
<ul>
<li>web3.eth.sendTransaction(transactionObject [, callback])</li>
<li>交易对象：</li>
<li>from：发送地址</li>
<li>to：接收地址，如果是创建合约交易，可不填</li>
<li>value：交易金额，以wei为单位，可选</li>
<li>gas：交易消耗 gas 上限，可选</li>
<li>gasPrice：交易 gas 单价，可选</li>
<li>data：交易携带的字串数据，可选</li>
<li>nonce：整数 nonce 值，可选</li>
</ul>
<p><strong>4.5.5 消息调用</strong></p>
<ul>
<li>web3.eth.call(callObject [, defaultBlock] [, callback])</li>
<li>参数：<ul>
<li>调用对象：与交易对象相同，只是from也是可选的</li>
<li>默认区块：默认“latest”，可以传入指定的区块高度</li>
<li>回调函数，如果没有则为同步调用</li>
</ul>
</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> result = web3.eth.call(&#123; <span class="attr">to</span>:<span class="string">"0xc4abd0339eb8d57087278718986382264244252f"</span>,</span><br><span class="line">data:<span class="string">"0xc6888fa1000000000000000000000000000000000000000000000000000 0000000000003"</span> &#125;);</span><br><span class="line"><span class="built_in">console</span>.log(result);</span><br></pre></td></tr></table></figure>
<p><strong>4.5.6 日志过滤（事件监听）</strong><br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">web3.eth.filter( filterOptions [ , callback ] )</span><br><span class="line"><span class="comment">// filterString 可以是 'latest' or 'pending'</span></span><br><span class="line"><span class="keyword">var</span> filter = web3.eth.filter(filterString);</span><br><span class="line"><span class="comment">// 或者可以填入一个日志过滤 options</span></span><br><span class="line"><span class="keyword">var</span> filter = web3.eth.filter(options);</span><br><span class="line"><span class="comment">// 监听日志变化</span></span><br><span class="line">filter.watch(<span class="function"><span class="keyword">function</span>(<span class="params">error, result</span>)</span>&#123; <span class="keyword">if</span> (!error) <span class="built_in">console</span>.log(result); &#125;);</span><br><span class="line"><span class="comment">// 还可以用传入回调函数的方法，立刻开始监听日志</span></span><br><span class="line">web3.eth.filter(options, <span class="function"><span class="keyword">function</span>(<span class="params">error, result</span>)</span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!error) <span class="built_in">console</span>.log(result);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p>
<p><strong>4.5.7 合约相关 —— 创建合约</strong></p>
<p>web3.eth.contract</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> MyContract = web3.eth.contract(abiArray);</span><br><span class="line"><span class="comment">// 通过地址初始化合约实例</span></span><br><span class="line"><span class="keyword">var</span> contractInstance = MyContract.at(address);</span><br><span class="line"><span class="comment">// 或者部署一个新合约</span></span><br><span class="line"><span class="keyword">var</span> contractInstance = MyContract.new([constructorParam1][, constructorParam2], &#123;<span class="attr">data</span>: <span class="string">'0x12345...'</span>, <span class="attr">from</span>:myAccount, <span class="attr">gas</span>: <span class="number">1000000</span>&#125;);</span><br></pre></td></tr></table></figure>
<p><strong>4.5.8 调用合约函数</strong></p>
<p>可以通过已创建的合约实例，直接调用合约函数</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 直接调用，自动按函数类型决定用 sendTransaction 还是 call</span></span><br><span class="line">myContractInstance.myMethod(param1 [, param2, ...] [,transactionObject] [, defaultBlock] [, callback]);</span><br><span class="line"><span class="comment">// 显式以消息调用形式 call 该函数</span></span><br><span class="line">myContractInstance.myMethod.call(param1 [, param2, ...] [,transactionObject] [, defaultBlock] [, callback]);</span><br><span class="line"><span class="comment">// 显式以发送交易形式调用该函数</span></span><br><span class="line">myContractInstance.myMethod.sendTransaction(param1 [,param2, ...] [, transactionObject] [, callback]);</span><br></pre></td></tr></table></figure>
<p><strong>4.5.9 监听合约事件</strong></p>
<p>合约的 event 类似于 filter，可以设置过滤选项来监听</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> event = myContractInstance.MyEvent(&#123;<span class="attr">valueA</span>: <span class="number">23</span>&#125;[, additionalFilterObject])</span><br><span class="line"><span class="comment">// 监听事件</span></span><br><span class="line">event.watch(<span class="function"><span class="keyword">function</span>(<span class="params">error, result</span>)</span>&#123; </span><br><span class="line"><span class="keyword">if</span> (!error) </span><br><span class="line">　　<span class="built_in">console</span>.log(result); </span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 还可以用传入回调函数的方法，立刻开始监听事件</span></span><br><span class="line"><span class="keyword">var</span> event = myContractInstance.MyEvent([&#123;<span class="attr">valueA</span>: <span class="number">23</span>&#125;][, additionalFilterObject] , <span class="function"><span class="keyword">function</span>(<span class="params">error, result</span>)</span>&#123;</span><br><span class="line">　　<span class="keyword">if</span> (!error) <span class="built_in">console</span>.log(result);</span><br><span class="line">&#125;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<h2 id="5、交互实现——部署智能合约"><a href="#5、交互实现——部署智能合约" class="headerlink" title="5、交互实现——部署智能合约"></a>5、交互实现——部署智能合约</h2><p>通过编写一个depoly.js程序实现自动化的部署智能合约。首先要保持Geth客户端正常运行，并开启rpc。</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">geth --identity <span class="string">"TestNode"</span> --rpc --rpcport <span class="string">"8545"</span> --datadir data0 --port <span class="string">"30303"</span> --nodiscover --networkid <span class="number">6666</span> --rpcapi admin,eth,miner,personal,txpool,eth,web3,net <span class="built_in">console</span></span><br></pre></td></tr></table></figure>
<p>合约应该在智能合约编译器（如remix）调试好，然后将其写到test.sol文件里。<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line">contract TestContract</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">multiply</span>(<span class="params">uint a, uint b</span>) <span class="title">returns</span> (<span class="params">uint</span>)</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> a * b;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>使用solc模块生成合约的code和abi，我将该过程自定义为一个模块test.js，方便depoly.js调用。<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> fs = <span class="built_in">require</span>(<span class="string">'fs'</span>);</span><br><span class="line"><span class="keyword">var</span> solc = <span class="built_in">require</span>(<span class="string">'solc'</span>);</span><br><span class="line"><span class="comment">//compile smart contract to get bytecode and abi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> source = fs.readFileSync(<span class="string">"./test.sol"</span>,<span class="string">'utf8'</span>);  <span class="comment">//读取代码</span></span><br><span class="line">    <span class="comment">//console.log("compiling contract...");</span></span><br><span class="line"><span class="keyword">var</span> compiledcontract = solc.compile(source); <span class="comment">//编译</span></span><br><span class="line">    <span class="comment">//console.log('done');</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">var</span> contractName <span class="keyword">in</span> compiledcontract.contracts)&#123;</span><br><span class="line">    <span class="keyword">var</span> bytecode = compiledcontract.contracts[contractName].bytecode;</span><br><span class="line">    <span class="keyword">var</span> abi = <span class="built_in">JSON</span>.parse(compiledcontract.contracts[contractName].interface);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//console.log(JSON.stringify(abi, undefined, 2));</span></span><br><span class="line"><span class="comment">//console.log(bytecode);</span></span><br><span class="line"><span class="comment">//console.log(abi);</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">bytecode</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(bytecode);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">abi</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(abi);</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">module</span>.exports = &#123;<span class="attr">bytecode</span>:bytecode,<span class="attr">abi</span>:abi&#125;;</span><br></pre></td></tr></table></figure><br>depoly.js通过与Geth交互部署智能合约。当合约被区块链确认后，会直接返回合约地址。<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> Web3 = <span class="built_in">require</span>(<span class="string">'web3'</span>);</span><br><span class="line"><span class="keyword">var</span> contract = <span class="built_in">require</span>(<span class="string">'./test'</span>);</span><br><span class="line"><span class="keyword">var</span> web;</span><br><span class="line"></span><br><span class="line"><span class="comment">//connect to node</span></span><br><span class="line"><span class="keyword">var</span> ethereumUri = <span class="string">'http://localhost:8545'</span>;</span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">typeof</span> web3 !== <span class="string">'undefined'</span>) &#123;</span><br><span class="line">    web3 = <span class="keyword">new</span> Web3(web3.currentProvider);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// set the provider you want from Web3.providers</span></span><br><span class="line">    web3 = <span class="keyword">new</span> Web3(<span class="keyword">new</span> Web3.providers.HttpProvider(ethereumUri));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//查询区块链中基本的账户信息</span></span><br><span class="line"><span class="keyword">if</span>(!web3.isConnected())&#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="built_in">Error</span>(<span class="string">'unable to connect to ethereum node at '</span>+ ethereumUri);</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'connected to etherum node at '</span>+ ethereumUri);</span><br><span class="line">    <span class="keyword">var</span> coinbase = web3.eth.accounts[<span class="number">0</span>];</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'coinbase:'</span> + coinbase);</span><br><span class="line">    <span class="keyword">var</span> balance = web3.eth.getBalance(coinbase);</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'balance:'</span> + web3.fromWei(balance, <span class="string">'ether'</span>) + <span class="string">" ETH"</span>);</span><br><span class="line">    <span class="keyword">var</span> accounts = web3.eth.accounts;</span><br><span class="line">    <span class="built_in">console</span>.log(accounts);    </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//通过coinbase部署智能合约</span></span><br><span class="line"><span class="keyword">var</span> abi = contract.abi;</span><br><span class="line"><span class="keyword">var</span> bytecode = contract.bytecode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (web3.personal.unlockAccount(coinbase, <span class="string">'123'</span>)) &#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">`<span class="subst">$&#123;coinbase&#125;</span> is unlocaked`</span>);</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">`unlock failed, <span class="subst">$&#123;coinbase&#125;</span>`</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> gasEstimate = web3.eth.estimateGas(&#123;<span class="attr">data</span>: <span class="string">'0x'</span> + bytecode&#125;); <span class="comment">//gas估计</span></span><br><span class="line"><span class="built_in">console</span>.log(<span class="string">'gasEstimate = '</span> + gasEstimate);</span><br><span class="line"><span class="keyword">var</span> MyContract = web3.eth.contract(abi);</span><br><span class="line"><span class="built_in">console</span>.log(<span class="string">'deploying contract...'</span>);</span><br><span class="line"><span class="keyword">var</span> myContractReturned = MyContract.new(&#123;</span><br><span class="line">    <span class="keyword">from</span>: coinbase,</span><br><span class="line">    data: <span class="string">'0x'</span>+ bytecode,</span><br><span class="line">    gas: gasEstimate + <span class="number">50000</span></span><br><span class="line">&#125;, <span class="function"><span class="keyword">function</span> (<span class="params">err, myContract</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!err) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!myContract.address) &#123;</span><br><span class="line">            <span class="built_in">console</span>.log(<span class="string">`myContract.transactionHash = <span class="subst">$&#123;myContract.transactionHash&#125;</span>`</span>); <span class="comment">// The hash of the transaction, which deploys the contract</span></span><br><span class="line">        <span class="comment">// check address on the second call (contract deployed)</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="built_in">console</span>.log(<span class="string">`myContract.address = <span class="subst">$&#123;myContract.address&#125;</span>`</span>); <span class="comment">// the contract address</span></span><br><span class="line">            global.contractAddress = myContract.address;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="built_in">console</span>.log(err);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p>
<p><strong>参考自：</strong></p>
<p><a href="https://geth.ethereum.org/docs/install-and-build/installing-geth" target="_blank" rel="noopener">Go Ethereum</a></p>
<p><a href="https://www.jianshu.com/p/9fa31e4cdf4d" target="_blank" rel="noopener">以太坊私有链Geth控制台操作教程</a></p>
<p><a href="https://www.bilibili.com/video/BV1sJ411D72u" target="_blank" rel="noopener">尚硅谷区块链全套Go语言→GoWeb→以太坊→项目实战</a><br><a href="http://cw.hubwiz.com/card/c/web3.js-1.0/" target="_blank" rel="noopener">web3.js 1.0中文手册</a></p>
]]></content>
      <categories>
        <category>Blockchain</category>
      </categories>
  </entry>
  <entry>
    <title>区块链入门（三）Solidity基础</title>
    <url>/posts/f6ff6959.html</url>
    <content><![CDATA[<font color="red"> 注：本教程为技术教程，不谈论且不涉及炒作任何数字货币 </font>

<h1 id="Solidity-入门教学"><a href="#Solidity-入门教学" class="headerlink" title="Solidity 入门教学"></a>Solidity 入门教学</h1><h2 id="1、-简介"><a href="#1、-简介" class="headerlink" title="1、 简介"></a>1、 简介</h2><h3 id="1-1-Solidity是什么"><a href="#1-1-Solidity是什么" class="headerlink" title="1.1 Solidity是什么"></a>1.1 Solidity是什么</h3><ul>
<li>Solidity 是一门面向合约的、为实现智能合约而创建的高级编程语言。这门语言受到了 C++，Python 和 Javascript 语言的影响，设计的目的是能在以太坊虚拟机（EVM）上运行。</li>
<li>Solidity 是静态类型语言，支持继承、库和复杂的用户定义类型等特性。</li>
<li>内含的类型除了常见编程语言中的标准类型，还包括 <code>address</code>等以太坊独有的类型，Solidity 源码文件通常以 .sol 作为扩展名</li>
<li>目前尝试 Solidity 编程的推荐方式是使用 Remix。Remix是一个基于 Web 浏览器的 IDE，它可以让你编写 Solidity 智能合约，然后部署并运行该智能合约。</li>
</ul>
<h3 id="1-2-Solidity语言特性"><a href="#1-2-Solidity语言特性" class="headerlink" title="1.2 Solidity语言特性"></a>1.2 Solidity语言特性</h3><p>Solidity的语法接近于JavaScript，是一种面向对象的语言。但作为一种真正意义上运行在网络上的去中心合约，它又有很多的不同：</p>
<ul>
<li>以太坊底层基于帐户，而不是 <a href="https://cloud.tencent.com/developer/article/1367743" target="_blank" rel="noopener">UTXO</a>，所以增加了一个特殊的address 的数据类型用于定位用户和合约账户。</li>
<li>语言内嵌框架支持支付。提供了 <code>payable</code> 等关键字，可以在语言层面直接支持支付。</li>
<li>使用区块链进行数据存储。数据的每一个状态都可以永久存储，所以在使用时需要确定变量使用内存，还是区块链存储。</li>
<li>运行环境是在去中心化的网络上，所以需要强调合约或函数执行的调用的方式。</li>
<li>不同的异常机制。一旦出现异常，所有的执行都将会被回撤，这主要是为了保证合约执行的原子性，以避免中间状态出现的数据不一致。</li>
</ul>
<h3 id="1-3-Solidity源码和智能合约"><a href="#1-3-Solidity源码和智能合约" class="headerlink" title="1.3 Solidity源码和智能合约"></a>1.3 Solidity源码和智能合约</h3><p>Solidity 源代码要成为可以运行在以太坊上的智能合约需要经历如下的</p>
<p><strong>步骤：</strong></p>
<ol>
<li>用 Solidity 编写的智能合约源代码需要先使用编译器编译为字节码（Bytecode），编译过程中会同时产生智能合约的二进制接口规范（Application Binary Interface，简称为ABI）；</li>
<li>通过交易（Transaction）的方式将字节码部署到以太坊网络，每次成功部署都会产生一个新的智能合约账户；</li>
<li>使用 Javascript 编写的 DApp 通常通过 web3.js + ABI去调用智能合约中的函数来实现数据的读取和修改。</li>
</ol>
<h3 id="1-4-合约结构"><a href="#1-4-合约结构" class="headerlink" title="1.4 合约结构"></a>1.4 合约结构</h3><ul>
<li>状态变量（State Variables）作为合约状态的一部分，值会永久保存在存储空间内。</li>
<li>函数（Functions）合约中可执行的代码块。</li>
<li>函数修饰器（Function Modifiers）在函数声明中，用来补充修饰函数的语义。</li>
<li>事件（Events）非常方便的 EVM 日志工具接口。</li>
</ul>
<h2 id="2、-Solidity编译器安装以及简单使用"><a href="#2、-Solidity编译器安装以及简单使用" class="headerlink" title="2、 Solidity编译器安装以及简单使用"></a>2、 Solidity编译器安装以及简单使用</h2><p>Remix 是一个开源的 IDE,是一个浏览器在线编辑器。作为 Solidity 智能合约开发环境，Solidity IDE  Remix(在线浏览器编辑器)提供基本的编译、部署至本地或测试网络、执行合约等功能。</p>
<h3 id="2-1-remix安装以及使用"><a href="#2-1-remix安装以及使用" class="headerlink" title="2.1 remix安装以及使用"></a>2.1 remix安装以及使用</h3><ol>
<li><strong>浏览器端配置</strong></li>
</ol>
<p>在浏览器端有俩个选择，分别为英文版与中文版（有些许差别）</p>
<ul>
<li><p>Remix中文版地址：<a href="http://remix.hubwiz.com" target="_blank" rel="noopener">http://remix.hubwiz.com</a></p>
</li>
<li><p>Remix英文版地址（<strong>推荐</strong>）：<a href="https://remix.ethereum.org/" target="_blank" rel="noopener">https://remix.ethereum.org/</a></p>
</li>
</ul>
<p><strong>PS.可能需要科学上网</strong></p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-1.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 1</div>
</center>



<p>下面都以<code>英文版</code>为例子介绍</p>
<p>1、<strong>浏览器输入 <a href="https://remix.ethereum.org/" target="_blank" rel="noopener">https://remix.ethereum.org/</a></strong></p>
<p>如果出现加载慢，加载不完全的情况，刷新几次即可</p>
<p>2、左侧可以看到我们所有的文件，下面是我们的remix控制台</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-2.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 2</div>
</center>



<p>上图小图标从左到右依次为：</p>
<ul>
<li>创建新文件</li>
<li>创建新文件夹</li>
<li>Github代码片段分享</li>
<li>表示打开一个本地文件</li>
</ul>
<p>控制台图片如下：</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-3.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 3</div>
</center>



<ul>
<li>1 从左至右表示隐藏控制台、清除控制台输出、pending的交易数量</li>
<li>2 表示监听所有交易</li>
<li>3 表示搜索框</li>
<li>4 表示输出区域</li>
<li>5 表示使用JavaScript与以太坊交互的区域，可以使用Web3对象</li>
</ul>
<p>3、点击文件样式图标输入我们的文件名即可(以.sol为后缀)</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-4.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 4</div>
</center>


<p>4、安装必要的插件</p>
<p>点击插件管理器，页面中为这个图标</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-5.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 5</div>
</center>




<ul>
<li><p>安装compiler</p>
<p>  搜索关键字compiler</p>
<center>
  <img style="border-radius: 0.3125em;
  box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-6.png">
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;
  display: inline-block;
  color: #999;
  padding: 2px;">图 6</div>
</center>



</li>
</ul>
<p>5、写一个简单的样例</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line"></span><br><span class="line">contract SimpleStorage &#123;</span><br><span class="line">    uint storedData;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">set</span>(<span class="params">uint x</span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        storedData = x;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">get</span>(<span class="params"></span>) <span class="title">public</span> <span class="title">view</span> <span class="title">returns</span> (<span class="params">uint</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> storedData;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第一行就是告诉大家源代码使用Solidity版本0.4.0写的，并且使用0.4.0以上版本运行也没问题（最高到0.5.0，但是不包含0.5.0）。这是为了确保合约不会在新的编译器版本中突然行为异常。关键字 <code>pragma</code> 的含义是，一般来说，pragmas（编译指令）是告知编译器如何处理源代码的指令的。</p>
<p>Solidity中合约的含义就是一组代码（它的 函数 )和数据（它的 状态 ），它们位于以太坊区块链的一个特定地址上。 代码行 <code>uint storedData</code>; 声明一个类型为 <code>uint</code> (256位无符号整数）的状态变量，叫做 <code>storedData</code> 。 你可以认为它是数据库里的一个位置，可以通过调用管理数据库代码的函数进行查询和变更。对于以太坊来说，上述的合约就是拥有合约（owning contract）。在这种情况下，函数 <code>set</code> 和 <code>get</code> 可以用来变更或取出变量的值。</p>
<p>要访问一个状态变量，并不需要像 <code>this.</code> 这样的前缀，虽然这是其他语言常见的做法。</p>
<p>该合约能完成的事情并不多（由于以太坊构建的基础架构的原因）：它能允许任何人在合约中存储一个单独的数字，并且这个数字可以被世界上任何人访问，且没有可行的办法阻止你发布这个数字。当然，任何人都可以再次调用 <code>set</code> ，传入不同的值，覆盖你的数字，但是这个数字仍会被存储在区块链的历史记录中。</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-7.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 7</div>
</center>



<p>点击<code>compile test.sol</code>，可以看到编译按钮，建议将<code>Auto compile</code>打钩（自动编译）,之后会在编译图标上看到一个以绿色为背景的对勾。</p>
<p>编译组件说明：</p>
<ul>
<li><code>Compiler</code>可以选择Solidity的编译器版本</li>
<li><code>Language</code>可以选择编程语言</li>
<li><code>EVM Version</code>可以选择EVM虚拟机版本</li>
<li><code>Auto compile</code>可以设置自动编译，修改完代码后自动执行编译操作</li>
<li><code>Enable optimization</code>可以设置对编译进行优化</li>
<li><code>Hide warnings</code>可以设置隐藏警告信息。</li>
<li><code>Contract</code>选择需要编译的合约</li>
<li><code>Publish on Swarm</code>和<code>Publish on Ipfs</code>分别将合约上传到Swarm和Ipfs这两个分布式文件系统上去</li>
<li><code>Compilation Details</code>很重要，可以查看编译的信息，包括ABI、字节码、函数Hash等</li>
<li><code>ABI</code>和<code>Bytecode</code>分别复制ABI和字节码。</li>
<li>再下面的部分空白用来显示编译的Warnings和Errors。</li>
</ul>
<p>我们点击<code>Compilation Details</code>就能看到编译之后的一些信息，如下图所示（部分）</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-8.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 8</div>
</center>


<ul>
<li><code>NAME</code>：合约名</li>
<li><code>METADATA</code>：一些编译相关的信息，比如版本、所用的语言、设置等</li>
<li><code>BYTECODE</code>：写入区块的字节码</li>
<li><code>ABI</code>：此智能合约对应的 ABI ，也就是我们合约里面定义的一些接口</li>
<li><code>WEB3DEPLOY</code>：智能合约编译之后的发布命令，这个就是比较重要的，之后的web3就是调用这段命令来部署合约的</li>
<li><code>METADATAHASH</code>：数据的一个哈希值</li>
<li><code>SWARMLOCATION</code>：Swarm网络的一个地址</li>
<li><code>FUNCTIONHASHES</code>：合约定义的方法的hash，其实我们执行合约的时候就是通过这个hash去找到对应的方法进行执行的</li>
<li><code>GASESTIMATES</code>：关于矿工费的一个预算，在ETH上进行合约的部署，执行等都是需要矿工费的。一般合约代码越多矿工费越高。</li>
</ul>
<p>点击下面的run图标，可以看到部署，以及账户信息，环境等等</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-9.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 9</div>
</center>



<p>点击deploy之后天可以看到自己的合约已经部署完成，打开之后可以看见我们写的函数<code>set</code>,<code>get</code>了，给<code>set</code>函数输入一个值，点击<code>get</code>会得到相应的值</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-10.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 10</div>
</center>



<ul>
<li><code>Environment</code> 表示合约部署的环境。<code>Javascript VM</code>是虚拟了一个节点，而<code>Injected Web3</code>和<code>Web3 Provider</code>则真正连接一个节点。</li>
<li><code>Account</code>代表不同的虚拟账户，每个虚拟账户每个有 100 ETH</li>
<li><code>Deploy</code>表示合约部署按钮</li>
<li><code>Deployed Contracts</code>表示已经部署的合约</li>
</ul>
<p>中文版界面与英文版界面有些许不一致，但都大同小异，想了解同学可以查看本博客(界面与中文版大致相同）：<br><a href="https://cloud.tencent.com/developer/article/1182404" target="_blank" rel="noopener">Solidity语言编辑器REMIX指导大全</a></p>
<ol>
<li><strong>本地配置：</strong><ul>
<li><a href="https://cloud.tencent.com/developer/article/1374376" target="_blank" rel="noopener">win下</a></li>
<li><a href="https://blog.csdn.net/qq_41944960/article/details/100134020" target="_blank" rel="noopener">ubuntu下</a></li>
</ul>
</li>
</ol>
<ol>
<li><strong>Docker</strong></li>
</ol>
<p>我们为编译器提供了最新的docker构建。 stable 仓库里的是已发布的版本，nightly 仓库则是在开发分支中的带有不稳定变更的版本。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run ethereum&#x2F;solc:stable solc --version</span><br></pre></td></tr></table></figure>
<p>目前，docker 镜像只含有 solc 的可执行程序，因此你需要额外的工作去把源代码和输出目录连接起来。</p>
<h2 id="3、Solidity基础操作"><a href="#3、Solidity基础操作" class="headerlink" title="3、Solidity基础操作"></a>3、Solidity基础操作</h2><p><strong>由于篇幅有限，以下只会讲解一些较基础、重要的概念(足够后面使用)，有些可能会一带而过或者“忽略”，如果大家途中有没太明白地方建议先百度、Google，或者查看此教程<a href="https://solidity-cn.readthedocs.io/zh/develop/index.html" target="_blank" rel="noopener">Solifity中文文档</a>、<a href="https://remix-ide.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">Solidity英文文档</a></strong></p>
<h3 id="3-1-Solidity源文件布局"><a href="#3-1-Solidity源文件布局" class="headerlink" title="3.1 Solidity源文件布局"></a>3.1 Solidity源文件布局</h3><p><strong>源文件可以被版本杂注pragma所注解，表明要求的编译器版本</strong></p>
<ul>
<li>例如：<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br></pre></td></tr></table></figure>
这样，源文件将既不允许低于 0.4.0 版本的编译器编译， 也不允许高于（包含） 0.5.0 版本的编译器编译（第二个条件因使用 ^ 被添加）。 这种做法的考虑是，编译器在 0.5.0 版本之前不会有重大变更，所以可确保源代码始终按预期被编译。 上面例子中不固定编译器的具体版本号，因此编译器的补丁版也可以使用。</li>
</ul>
<p><strong>import（导入其它源文件）</strong></p>
<ul>
<li>Solidity 所支持的导入语句import，语法同 JavaScript（从ES6 起）非常类似</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> <span class="string">"filename"</span>;</span><br></pre></td></tr></table></figure>
<p>从“filename”中导入所有的全局符号到当前全局作用域中<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> * <span class="keyword">as</span> symbolName <span class="keyword">from</span> <span class="string">"filename"</span>;</span><br></pre></td></tr></table></figure><br>创建一个新的全局符号 symbolName，其成员均来自 “filename”中全局符号<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> &#123;symbol1 <span class="keyword">as</span> alias, symbol2&#125; <span class="keyword">from</span> <span class="string">"filename"</span>;</span><br></pre></td></tr></table></figure><br>创建新的全局符号 alias 和 symbol2，分别从 “filename” 引用 symbol1 和 symbol2<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> <span class="string">"filename"</span> <span class="keyword">as</span> symbolName;</span><br></pre></td></tr></table></figure><br>这条语句等同于 import * as symbolName from “filename”;</p>
<p><strong>注释</strong></p>
<p>可以使用单行注释（//）和多行注释（/<em>…</em>/）</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 这是一个单行注释。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">这是一个</span></span><br><span class="line"><span class="comment">多行注释。</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<h3 id="3-2-数据类型与运算符"><a href="#3-2-数据类型与运算符" class="headerlink" title="3.2 数据类型与运算符"></a>3.2 数据类型与运算符</h3><h3 id="3-2-1-Solidity值类型介绍"><a href="#3-2-1-Solidity值类型介绍" class="headerlink" title="3.2.1 Solidity值类型介绍"></a>3.2.1 Solidity值类型介绍</h3><ul>
<li><strong>布尔（bool）</strong>：</li>
</ul>
<p>可能的取值为字符常量值 true 或 false</p>
<p>例子：<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line"></span><br><span class="line">contract helloworld &#123;</span><br><span class="line">    bool boola=<span class="literal">true</span>; <span class="comment">//声明一个布尔类型的值，只用一个等号</span></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">booltesta</span>(<span class="params"></span>) <span class="title">public</span> <span class="title">view</span> <span class="title">returns</span>(<span class="params">bool</span>)</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> boola;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">booltestb</span>(<span class="params">int a,int b</span>) <span class="title">public</span> <span class="title">pure</span> <span class="title">returns</span>(<span class="params">bool</span>)</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> a==b;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-12.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 11</div>
</center>



<ul>
<li>整型（int/uint）**：</li>
</ul>
<p><code>int</code> / <code>uint</code> ：分别表示有符号和无符号的不同位数的整型变量。 支持关键字 <code>uint8</code> 到 <code>uint256</code> （无符号，从 8 位到 256 位）以及 <code>int8</code> 到 <code>int256</code>，以 8 位为步长递增。 <code>uint</code> 和 <code>int</code> 分别是 <code>uint256</code> 和 <code>int256</code> 的别名。</p>
<ul>
<li><strong>定长浮点型（fixed / ufixed）</strong>： </li>
</ul>
<p><code>fixed</code>/ <code>ufixed</code>：表示各种大小的有符号和无符号的定长浮点型。 在关键字 <code>ufixedMxN</code> 和 <code>fixedMxN</code> 中，<code>M</code> 表示该类型占用的位数，<code>N</code> 表示可用的小数位数。 <code>M</code>必须能整除 8，即 8 到 256 位。 <code>N</code>则可以是从 0 到 80 之间的任意数。 <code>ufixed</code> 和 <code>fixed</code> 分别是 <code>ufixed128x19</code> 和 <code>fixed128x19</code> 的别名。</p>
<ul>
<li><strong>地址（address 重点，后面细讲）</strong>：</li>
</ul>
<p>地址类型存储一个 20 字节的值（以太坊地址的大小）。 地址类型也有成员变量，并作为所有合约的基础。</p>
<p> <strong>地址类型成员变量</strong>:<code>balance</code> 和 <code>transfer</code></p>
<p> 可以使用 balance 属性来查询一个地址的余额， 也可以使用 transfer 函数向一个地址发送 以太币 （以 wei 为单位）：</p>
 <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">address x = <span class="number">0x123</span>;</span><br><span class="line">address myAddress = <span class="keyword">this</span>;</span><br><span class="line"><span class="keyword">if</span> (x.balance &lt; <span class="number">10</span> &amp;&amp; myAddress.balance &gt;= <span class="number">10</span>) x.transfer(<span class="number">10</span>);</span><br></pre></td></tr></table></figure>
<p>注：如果 <code>x</code> 是一个合约地址，它的代码（更具体来说是它的 fallback 函数，如果有的话）会跟 <code>transfer</code> 函数调用一起执行（这是 EVM 的一个特性，无法阻止）。 如果在执行过程中用光了 gas 或者因为任何原因执行失败，以太币 交易会被打回，当前的合约也会在终止的同时抛出异常。</p>
<ul>
<li><strong>定长字节数组</strong>：</li>
</ul>
<p>关键字有 bytes1， bytes2， bytes3， …， bytes32<br><code>.length</code> 表示这个字节数组的长度（只读）.</p>
<p>注：可以将 <code>byte[]</code> 当作字节数组使用，但这种方式非常浪费存储空间，准确来说，是在传入调用时，每个元素会浪费 31 字节。 更好地做法是使用 <code>bytes</code>。</p>
<ul>
<li><strong>变长字节数组</strong></li>
</ul>
<p><code>bytes</code>:变长字节数组。它并不是值类型。</p>
<p><code>string</code>:变长 UTF-8 编码字符串类型。并不是值类型。 </p>
<ul>
<li><strong>地址字面常数（Address Literals）</strong></li>
</ul>
<p>比如像 <code>0xdCad3a6d3569DF655070DEd06cb7A1b2Ccd1D3AF</code> 这样的通过了地址校验和测试的十六进制字面常数属于 <code>address</code> 类型。 长度在 39 到 41 个数字的，没有通过校验和测试而产生了一个警告的十六进制字面常数视为正常的有理数字面常数。</p>
<ul>
<li><strong>有理数和整数字面常数</strong></li>
</ul>
<p>整数字面常数由范围在 0-9 的一串数字组成，表现成十进制。 例如，69 表示数字 69。 Solidity 中是没有八进制的，因此前置 0 是无效的。</p>
<p>十进制小数字面常数带有一个 .，至少在其一边会有一个数字。 比如：<code>1.，.1</code>，和 <code>1.3</code>。</p>
<p>科学符号也是支持的，尽管指数必须是整数，但底数可以是小数。 比如：<code>2e10， -2e10， 2e-10， 2.5e1</code>。</p>
<p>数值字面常数表达式本身支持任意精度，除非它们被转换成了非字面常数类型（也就是说，当它们出现在非字面常数表达式中时就会发生转换）。 这意味着在数值常量表达式中, 计算不会溢出而除法也不会截断。</p>
<p>例如， <code>(2**800 + 1) - 2**800</code> 的结果是字面常数 1 （属于 <code>uint8</code> 类型），尽管计算的中间结果已经超过了 以太坊虚拟机 的机器字长度。 此外， <code>.5 * 8</code> 的结果是整型 4 （尽管有非整型参与了计算）</p>
<ul>
<li><strong>字符串字面常数</strong></li>
</ul>
<p>字符串字面常数是指由双引号或单引号引起来的字符串（<code>&quot;foo&quot;</code>或者 <code>&#39;bar&#39;</code>）。 不像在 C 语言中那样带有结束符；<code>&quot;foo&quot;</code> 相当于 3 个字节而不是 4 个。 和整数字面常数一样，字符串字面常数的类型也可以发生改变，但它们可以隐式地转换成 <code>bytes1，……，bytes32</code>，如果合适的话，还可以转换成 <code>bytes</code> 以及 <code>string</code>。</p>
<ul>
<li><strong>十六进制字面常数</strong></li>
</ul>
<p>十六进制字面常数以关键字 <code>hex</code> 打头，后面紧跟着用单引号或双引号引起来的字符串（例如，<code>hex&quot;001122FF&quot;</code>）。 字符串的内容必须是一个十六进制的字符串，它们的值将使用二进制表示。</p>
<ul>
<li><strong>枚举（enum）</strong>：</li>
</ul>
<p>一种用户可以定义类型的方法，与C语言类似，默认从0开始递增，一般用来模拟合约的状态</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.16</span>;</span><br><span class="line"></span><br><span class="line">contract test &#123;</span><br><span class="line">    enum ActionChoices &#123; GoLeft, GoRight, GoStraight, SitStill &#125;；</span><br><span class="line">    ActionChoices choice;</span><br><span class="line">    ActionChoices constant defaultChoice = ActionChoices.GoStraight;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">setGoStraight</span>(<span class="params"></span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        choice = ActionChoices.GoStraight;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 由于枚举类型不属于 |ABI| 的一部分，因此对于所有来自 Solidity 外部的调用，</span></span><br><span class="line">    <span class="comment">// "getChoice" 的签名会自动被改成 "getChoice() returns (uint8)"。</span></span><br><span class="line">    <span class="comment">// 整数类型的大小已经足够存储所有枚举类型的值，随着值的个数增加，</span></span><br><span class="line">    <span class="comment">// 可以逐渐使用 `uint16` 或更大的整数类型。</span></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">getChoice</span>(<span class="params"></span>) <span class="title">public</span> <span class="title">view</span> <span class="title">returns</span> (<span class="params">ActionChoices</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> choice;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">getDefaultChoice</span>(<span class="params"></span>) <span class="title">public</span> <span class="title">pure</span> <span class="title">returns</span> (<span class="params">uint</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> uint(defaultChoice);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>函数（function）</strong>：</li>
</ul>
<p>函数类型是一种表示函数的类型。可以将一个函数赋值给另一个函数类型的变量，也可以将一个函数作为参数进行传递，还能在函数调用中返回函数类型变量。 函数类型有两类：- 内部（<code>internal</code>） 函数和 外部（<code>external</code>） 函数：</p>
<p>内部函数只能在当前合约内被调用（更具体来说，在当前代码块内，包括内部库函数和继承的函数中），因为它们不能在当前合约上下文的外部被执行。 调用一个内部函数是通过跳转到它的入口标签来实现的，就像在当前合约的内部调用一个函数。</p>
<p>外部函数由一个地址和一个函数签名组成，可以通过外部函数调用传递或者返回。</p>
<p>函数类型表示成如下的形式</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="\Pic\Blockchain_Pic\part2-11.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 12</div>
</center>



<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> (<span class="params">&lt;parameter types&gt;</span>) </span>&#123;internal|external&#125; [pure|constant|view|payable] [returns (<span class="xml"><span class="tag">&lt;<span class="name">return</span> <span class="attr">types</span>&gt;</span>)]</span></span><br></pre></td></tr></table></figure>
<p>与参数类型相反，返回类型不能为空 —— 如果函数类型不需要返回，则需要删除整个 <code>returns (&lt;return types&gt;)</code> 部分。</p>
<p>函数类型默认是内部函数，因此不需要声明 <code>internal</code> 关键字。 与此相反的是，合约中的函数本身默认是 <code>public</code>的，只有当它被当做类型名称时，默认才是内部函数。</p>
<p>有两种方法可以访问当前合约中的函数：一种是直接使用它的名字，<code>f</code> ，另一种是使用 <code>this.f</code> 。 前者适用于内部函数，后者适用于外部函数。</p>
<p>如果当函数类型的变量还没有初始化时就调用它的话会引发一个异常。 如果在一个函数被 <code>delete</code> 之后调用它也会发生相同的情况。</p>
<p>如果外部函数类型在 Solidity 的上下文环境以外的地方使用，它们会被视为 <code>function</code> 类型。 该类型将函数地址紧跟其函数标识一起编码为一个 <code>bytes24</code> 类型。</p>
<p>请注意，当前合约的 public 函数既可以被当作内部函数也可以被当作外部函数使用。 如果想将一个函数当作内部函数使用，就用 <code>f</code> 调用，如果想将其当作外部函数，使用 <code>this.f</code> 。</p>
<p><strong>Solidity函数可见性</strong></p>
<p>函数的可见性可以指定为 external，public ，internal 或者 private；对于状态变量，不能设置为 external ，默认是 internal。</p>
<ul>
<li>external ：外部函数作为合约接口的一部分，意味着我们可以从其他合约和交易中调用。 一个外部函数 f不能从内部调用（即 f 不起作用，但 this.f() 可以）。 当收到大量数据的时候，外部函数有时候会更有效率。</li>
<li>public ：public 函数是合约接口的一部分，可以在内部或通过消息调用。对于 public 状态变量， 会自动生成一个 getter 函数。</li>
<li>internal ：这些函数和状态变量只能是内部访问（即从当前合约内部或从它派生的合约访问），不使用 this 调用。</li>
<li>private ：private 函数和状态变量仅在当前定义它们的合约中使用，并且不能被派生合约使用。</li>
</ul>
<p><strong>Solidity函数状态可变性</strong></p>
<ul>
<li>pure：纯函数，不允许修改或访问状态</li>
<li>view：不允许修改状态</li>
<li>payable：允许从消息调用中接收以太币Ether 。</li>
<li>constant：与view相同，一般只修饰状态变量，不允许赋值（除初始化以外）</li>
</ul>
<p><strong>内部函数调用</strong></p>
<p>当前合约中的函数可以直接（“从内部”）调用，也可以递归调用，就像下边这个荒谬的例子一样<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.16</span>;</span><br><span class="line"></span><br><span class="line">contract C &#123;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">g</span>(<span class="params">uint a</span>) <span class="title">public</span> <span class="title">pure</span> <span class="title">returns</span> (<span class="params">uint ret</span>) </span>&#123; <span class="keyword">return</span> f(); &#125;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">f</span>(<span class="params"></span>) <span class="title">internal</span> <span class="title">pure</span> <span class="title">returns</span> (<span class="params">uint ret</span>) </span>&#123; <span class="keyword">return</span> g(<span class="number">7</span>) + f(); &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>这些函数调用在 EVM 中被解释为简单的跳转。这样做的效果就是当前内存不会被清除，也就是说，通过内部调用在函数之间传递内存引用是非常有效的。</p>
<p><strong>外部函数调用</strong></p>
<p>表达式 <code>this.g(8)</code>; 和 <code>c.g(2)</code>; （其中 c 是合约实例）也是有效的函数调用，但是这种情况下，函数将会通过一个消息调用来被“外部调用”，而不是直接的跳转。 请注意，不可以在构造函数中通过 this 来调用函数，因为此时真实的合约实例还没有被创建。</p>
<p>如果想要调用其他合约的函数，需要外部调用。对于一个外部调用，所有的函数参数都需要被复制到内存。</p>
<p>当调用其他合约的函数时，随函数调用发送的 Wei 和 gas 的数量可以分别由特定选项 <code>.value()</code> 和 <code>.gas()</code> 指定:</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line"></span><br><span class="line">contract InfoFeed &#123;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">info</span>(<span class="params"></span>) <span class="title">public</span> <span class="title">payable</span> <span class="title">returns</span> (<span class="params">uint ret</span>) </span>&#123; <span class="keyword">return</span> <span class="number">42</span>; &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">contract Consumer &#123;</span><br><span class="line">    InfoFeed feed;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">setFeed</span>(<span class="params">address addr</span>) <span class="title">public</span> </span>&#123; feed = InfoFeed(addr); &#125;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">callFeed</span>(<span class="params"></span>) <span class="title">public</span> </span>&#123; feed.info.value(<span class="number">10</span>).gas(<span class="number">800</span>)(); &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>payable</code> 修饰符要用于修饰 <code>info</code>，否则，.<code>value()</code> 选项将不可用。</p>
<p>注意，表达式 <code>InfoFeed(addr)</code> 进行了一个的显式类型转换，说明”我们知道给定地址的合约类型是 <code>InfoFeed</code> “并且这不会执行构造函数。 显式类型转换需要谨慎处理。绝对不要在一个你不清楚类型的合约上执行函数调用。</p>
<p>我们也可以直接使用 <code>function setFeed(InfoFeed _feed) { feed = _feed; }</code> 。 注意一个事实，<code>feed.info.value(10).gas(800)</code> 只（局部地）设置了与函数调用一起发送的 Wei 值和 gas 的数量，只有最后的圆括号执行了真正的调用。</p>
<p>如果被调函数所在合约不存在（也就是账户中不包含代码）或者被调用合约本身抛出异常或者 gas 用完等，函数调用会抛出异常。</p>
<h3 id="3-2-2-引用类型介绍"><a href="#3-2-2-引用类型介绍" class="headerlink" title="3.2.2 引用类型介绍"></a>3.2.2 引用类型介绍</h3><p>比起之前讨论过的值类型，在处理复杂的类型（即占用的空间超过 256 位的类型）时，我们需要更加谨慎。 由于拷贝这些类型变量的开销相当大，我们不得不考虑它的存储位置，是将它们保存在 <strong>内存</strong> （并不是永久存储）中， 还是 <strong>存储</strong> （保存状态变量的地方）中。</p>
<ul>
<li><strong>数据位置</strong></li>
</ul>
<p>所有的复杂类型，即 <strong>数组</strong> 和 <strong>结构</strong> 类型，都有一个额外属性，“数据位置”，说明数据是保存在 <strong>内存</strong> 中还是 <strong>存储</strong> 中。 根据上下文不同，大多数时候数据有默认的位置，但也可以通过在类型名后增加关键字 <code>storage</code> 或 <code>memory</code> 进行修改。 函数参数（包括返回的参数）的数据位置默认是 <code>memory</code>， 局部变量的数据位置默认是 <code>storage</code>，状态变量的数据位置强制是 <code>storage</code>。</p>
<p>也存在第三种数据位置， <code>calldata</code> ，这是一块只读的，且不会永久存储的位置，用来存储函数参数。 外部函数的参数（非返回参数）的数据位置被强制指定为 <code>calldata</code>，效果跟 <code>memory</code> 差不多。</p>
<p>例子：<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line"></span><br><span class="line">contract C &#123;</span><br><span class="line">    uint[] x; <span class="comment">// x 的数据存储位置是 storage</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// memoryArray 的数据存储位置是 memory</span></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">f</span>(<span class="params">uint[] memoryArray</span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        x = memoryArray; <span class="comment">// 将整个数组拷贝到 storage 中，可行</span></span><br><span class="line">        <span class="keyword">var</span> y = x;  <span class="comment">// 分配一个指针（其中 y 的数据存储位置是 storage），可行</span></span><br><span class="line">        y[<span class="number">7</span>]; <span class="comment">// 返回第 8 个元素，可行</span></span><br><span class="line">        y.length = <span class="number">2</span>; <span class="comment">// 通过 y 修改 x，可行</span></span><br><span class="line">        <span class="keyword">delete</span> x; <span class="comment">// 清除数组，同时修改 y，可行</span></span><br><span class="line">        <span class="comment">// 下面的就不可行了；需要在 storage 中创建新的未命名的临时数组， /</span></span><br><span class="line">        <span class="comment">// 但 storage 是“静态”分配的：</span></span><br><span class="line">        <span class="comment">// y = memoryArray;</span></span><br><span class="line">        <span class="comment">// 下面这一行也不可行，因为这会“重置”指针，</span></span><br><span class="line">        <span class="comment">// 但并没有可以让它指向的合适的存储位置。</span></span><br><span class="line">        <span class="comment">// delete y;</span></span><br><span class="line"></span><br><span class="line">        g(x); <span class="comment">// 调用 g 函数，同时移交对 x 的引用</span></span><br><span class="line">        h(x); <span class="comment">// 调用 h 函数，同时在 memory 中创建一个独立的临时拷贝</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">g</span>(<span class="params">uint[] storage storageArray</span>) <span class="title">internal</span> </span>&#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">h</span>(<span class="params">uint[] memoryArray</span>) <span class="title">public</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>归纳：</p>
<p>强制指定的数据位置：</p>
<ol>
<li>外部函数的参数（不包括返回参数）： calldata</li>
<li>状态变量： storage</li>
</ol>
<p>默认数据位置：</p>
<ol>
<li>函数参数（包括返回参数）： memory</li>
<li>所有其它局部变量： storage</li>
</ol>
<ul>
<li><strong>数组</strong></li>
</ul>
<p>数组可以在声明时指定长度，也可以动态调整大小。 对于 <strong>存储</strong> 的数组来说，元素类型可以是任意的（即元素也可以是数组类型，映射类型或者结构体）。 对于 <strong>内存</strong> 的数组来说，元素类型不能是映射类型，如果作为 <code>public</code> 函数的参数，它只能是 <code>ABI</code> 类型。</p>
<p>一个元素类型为 <code>T</code>，固定长度为 <code>k</code> 的数组可以声明为 <code>T[k]</code>，而动态数组声明为 <code>T[]</code>。 </p>
<p>举个例子，一个长度为 5，元素类型为 <code>uint</code> 的动态数组的数组，应声明为 <code>uint[][5]</code> （注意这里跟其它语言比，数组长度的声明位置是反的）。 要访问第三个动态数组的第二个元素，你应该使用 <code>x[2][1]</code>（数组下标是从 0 开始的，且访问数组时的下标顺序与声明时相反，也就是说，<code>x[2]</code> 是从右边减少了一级）。。</p>
<p><code>bytes</code> 和 <code>string</code> 类型的变量是特殊的数组。 <code>bytes</code> 类似于 <code>byte[]</code>，但它在 <code>calldata</code> 中会被“紧打包”（译者注：将元素连续地存在一起，不会按每 32 字节一单元的方式来存放）。 <code>string</code> 与 <code>bytes</code> 相同，但（暂时）不允许用长度或索引来访问。</p>
<p>注：<br>如果想要访问以字节表示的字符串 s，请使用 <code>bytes(s)</code>.<code>length / bytes(s)[7] = &#39;x&#39;</code>;。 注意这时你访问的是 <code>UTF-8</code> 形式的低级<code>bytes</code> 类型，而不是单个的字符。</p>
<p><strong>成员</strong></p>
<p><code>length</code>:</p>
<p>数组有 length 成员变量表示当前数组的长度。 动态数组可以在 <strong>存储</strong> （而不是 <strong>内存</strong> ）中通过改变成员变量 .length 改变数组大小。 并不能通过访问超出当前数组长度的方式实现自动扩展数组的长度。 一经创建，<strong>内存</strong> 数组的大小就是固定的（但却是动态的，也就是说，它依赖于运行时的参数）。<br><code>push</code>:<br>    变长的 <strong>存储</strong> 数组以及 bytes 类型（而不是 string 类型）都有一个叫做 push 的成员函数，它用来附加新的元素到数组末尾。 这个函数将返回新的数组长度。 </p>
<p>例子：<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.16</span>;</span><br><span class="line"></span><br><span class="line">contract ArrayContract &#123;</span><br><span class="line">    uint[<span class="number">2</span>**<span class="number">20</span>] m_aLotOfIntegers;</span><br><span class="line">    <span class="comment">// 注意下面的代码并不是一对动态数组，</span></span><br><span class="line">    <span class="comment">// 而是一个数组元素为一对变量的动态数组（也就是数组元素为长度为 2 的定长数组的动态数组）。</span></span><br><span class="line">    bool[<span class="number">2</span>][] m_pairsOfFlags;</span><br><span class="line">    <span class="comment">// newPairs 存储在 memory 中 —— 函数参数默认的存储位置</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">setAllFlagPairs</span>(<span class="params">bool[<span class="number">2</span>][] newPairs</span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 向一个 storage 的数组赋值会替代整个数组</span></span><br><span class="line">        m_pairsOfFlags = newPairs;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">setFlagPair</span>(<span class="params">uint index, bool flagA, bool flagB</span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 访问一个不存在的数组下标会引发一个异常</span></span><br><span class="line">        m_pairsOfFlags[index][<span class="number">0</span>] = flagA;</span><br><span class="line">        m_pairsOfFlags[index][<span class="number">1</span>] = flagB;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">changeFlagArraySize</span>(<span class="params">uint newSize</span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 如果 newSize 更小，那么超出的元素会被清除</span></span><br><span class="line">        m_pairsOfFlags.length = newSize;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">clear</span>(<span class="params"></span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 这些代码会将数组全部清空</span></span><br><span class="line">        <span class="keyword">delete</span> m_pairsOfFlags;</span><br><span class="line">        <span class="keyword">delete</span> m_aLotOfIntegers;</span><br><span class="line">        <span class="comment">// 这里也是实现同样的功能</span></span><br><span class="line">        m_pairsOfFlags.length = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    bytes m_byteData;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">byteArrays</span>(<span class="params">bytes data</span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 字节的数组（语言意义中的 byte 的复数 ``bytes``）不一样，因为它们不是填充式存储的，</span></span><br><span class="line">        <span class="comment">// 但可以当作和 "uint8[]" 一样对待</span></span><br><span class="line">        m_byteData = data;</span><br><span class="line">        m_byteData.length += <span class="number">7</span>;</span><br><span class="line">        m_byteData[<span class="number">3</span>] = byte(<span class="number">8</span>);</span><br><span class="line">        <span class="keyword">delete</span> m_byteData[<span class="number">2</span>];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">addFlag</span>(<span class="params">bool[<span class="number">2</span>] flag</span>) <span class="title">public</span> <span class="title">returns</span> (<span class="params">uint</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> m_pairsOfFlags.push(flag);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">createMemoryArray</span>(<span class="params">uint size</span>) <span class="title">public</span> <span class="title">pure</span> <span class="title">returns</span> (<span class="params">bytes</span>) </span>&#123;</span><br><span class="line">        <span class="comment">// 使用 `new` 创建动态 memory 数组：</span></span><br><span class="line">        uint[<span class="number">2</span>][] memory arrayOfPairs = <span class="keyword">new</span> uint[<span class="number">2</span>][](size);</span><br><span class="line">        <span class="comment">// 创建一个动态字节数组：</span></span><br><span class="line">        bytes memory b = <span class="keyword">new</span> bytes(<span class="number">200</span>);</span><br><span class="line">        <span class="keyword">for</span> (uint i = <span class="number">0</span>; i &lt; b.length; i++)</span><br><span class="line">            b[i] = byte(i);</span><br><span class="line">        <span class="keyword">return</span> b;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong>结构体</strong></li>
</ul>
<p>Solidity 支持通过构造结构体的形式定义新的类型，以下是一个结构体的示例：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Funder</span> &#123;</span></span><br><span class="line">    address addr;</span><br><span class="line">    uint amount;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Campaign</span> &#123;</span></span><br><span class="line">    address beneficiary;</span><br><span class="line">    uint fundingGoal;</span><br><span class="line">    uint numFunders;</span><br><span class="line">    uint amount;</span><br><span class="line">    mapping (uint =&gt; Funder) funders;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong>映射</strong><br>映射类型在声明时的形式为 <code>mapping(_KeyType =&gt; _ValueType)</code>。 其中 <code>_KeyType</code> 可以是除了映射、变长数组、合约、枚举以及结构体以外的几乎所有类型。 <code>_ValueType</code> 可以是包括映射类型在内的任何类型。</li>
</ul>
<p>映射可以视作 哈希表 <a href="https://en.wikipedia.org/wiki/Hash_table" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Hash_table</a>，它们在实际的初始化过程中创建每个可能的 key， 并将其映射到字节形式全是零的值：一个类型的 默认值。然而下面是映射与哈希表不同的地方： 在映射中，实际上并不存储 key，而是存储它的 <code>keccak256</code> 哈希值，从而便于查询实际的值。</p>
<p>正因为如此，映射是没有长度的，也没有 <code>key</code> 的集合或 <code>value</code> 的集合的概念。</p>
<p>只有状态变量（或者在 internal 函数中的对于存储变量的引用）可以使用映射类型。。</p>
<p>可以将映射声明为 <code>public</code>，然后来让 Solidity 创建一个 getter。<code>_KeyType</code> 将成为 getter 的必须参数，并且 getter 会返回 <code>_ValueType</code>。</p>
<p><code>_ValueType</code> 也可以是一个映射。这时在使用 getter 时将将需要递归地传入每个 <code>_KeyType</code>参数。</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line"></span><br><span class="line">contract MappingExample &#123;</span><br><span class="line">    mapping(<span class="function"><span class="params">address</span> =&gt;</span> uint) public balances;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">update</span>(<span class="params">uint newBalance</span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        balances[msg.sender] = newBalance;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">contract MappingUser &#123;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">f</span>(<span class="params"></span>) <span class="title">public</span> <span class="title">returns</span> (<span class="params">uint</span>) </span>&#123;</span><br><span class="line">        MappingExample m = <span class="keyword">new</span> MappingExample();</span><br><span class="line">        m.update(<span class="number">100</span>);</span><br><span class="line">        <span class="keyword">return</span> m.balances(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-2-3-涉及-LValues-的运算符"><a href="#3-2-3-涉及-LValues-的运算符" class="headerlink" title="3.2.3 涉及 LValues 的运算符"></a>3.2.3 涉及 LValues 的运算符</h4><ul>
<li><strong>删除</strong></li>
</ul>
<p><code>delete a</code> 的结果是将 <code>a</code> 的类型在初始化时的值赋值给 <code>a</code>。即对于整型变量来说，相当于 <code>a = 0</code>， 但 delete 也适用于数组，对于动态数组来说，是将数组的长度设为 0，而对于静态数组来说，是将数组中的所有元素重置。 如果对象是结构体，则将结构体中的所有属性重置。</p>
<p>delete 对整个映射是无效的（因为映射的键可以是任意的，通常也是未知的）。 因此在你删除一个结构体时，结果将重置所有的非映射属性，这个过程是递归进行的，除非它们是映射。 然而，单个的键及其映射的值是可以被删除的。</p>
<p>理解 <code>delete a</code>的效果就像是给 <code>a</code> 赋值很重要，换句话说，这相当于在 <code>a</code>中存储了一个新的对象。</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line"></span><br><span class="line">contract DeleteExample &#123;</span><br><span class="line">    uint data;</span><br><span class="line">    uint[] dataArray;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">f</span>(<span class="params"></span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        uint x = data;</span><br><span class="line">        <span class="keyword">delete</span> x; <span class="comment">// 将 x 设为 0，并不影响数据</span></span><br><span class="line">        <span class="keyword">delete</span> data; <span class="comment">// 将 data 设为 0，并不影响 x，因为它仍然有个副本</span></span><br><span class="line">        uint[] storage y = dataArray;</span><br><span class="line">        <span class="keyword">delete</span> dataArray;</span><br><span class="line">        <span class="comment">// 将 dataArray.length 设为 0，但由于 uint[] 是一个复杂的对象，y 也将受到影响，</span></span><br><span class="line">        <span class="comment">// 因为它是一个存储位置是 storage 的对象的别名。</span></span><br><span class="line">        <span class="comment">// 另一方面："delete y" 是非法的，引用了 storage 对象的局部变量只能由已有的 storage 对象赋值。</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-单位和全局变量"><a href="#3-3-单位和全局变量" class="headerlink" title="3.3 单位和全局变量"></a>3.3 单位和全局变量</h3><h3 id="3-3-1-以太币单位"><a href="#3-3-1-以太币单位" class="headerlink" title="3.3.1 以太币单位"></a>3.3.1 以太币单位</h3><p>以太币 单位之间的换算就是在数字后边加上 <code>wei</code>、 <code>finney</code>、 <code>szabo</code> 或 <code>ether</code> 来实现的，如果后面没有单位，缺省为 <code>Wei</code>。例如 <code>2 ether == 2000 finney</code> 的逻辑判断值为 <code>true</code>。</p>
<h3 id="3-3-2-时间单位"><a href="#3-3-2-时间单位" class="headerlink" title="3.3.2 时间单位"></a>3.3.2 时间单位</h3><p>秒是缺省时间单位，在时间单位之间，数字后面带有 <code>seconds</code>、 <code>minutes</code>、 <code>hours</code>、 <code>days</code>、 <code>weeks</code> 和 <code>years</code> 的可以进行换算，基本换算关系与现实生活相符。</p>
<h3 id="3-3-3-特殊变量和函数"><a href="#3-3-3-特殊变量和函数" class="headerlink" title="3.3.3 特殊变量和函数"></a>3.3.3 特殊变量和函数</h3><p>在全局命名空间中已经存在了（预设了）一些特殊的变量和函数，他们主要用来提供关于区块链的信息或一些通用的工具函数。</p>
<p><strong>区块和交易属性</strong></p>
<ul>
<li><code>block.blockhash(uint blockNumber) returns (bytes32)</code>：指定区块的区块哈希——仅可用于最新的 256 个区块且不包括当前区块；而 blocks 从 0.4.22 版本开始已经不推荐使用，由 <code>blockhash(uint blockNumber)</code> 代替</li>
<li><code>block.coinbase (address)</code>: 挖出当前区块的矿工地</li>
<li><code>block.difficulty (uint)</code>: 当前区块难度</li>
<li><code>block.gaslimit (uint)</code>: 当前区块 <code>gas</code> 限额</li>
<li><code>block.number (uint</code>): 当前区块号</li>
<li><code>block.timestamp (uint)</code>: 自 <code>unix epoch</code> 起始当前区块以秒计的时间戳</li>
<li><code>gasleft() returns (uint256)</code>：剩余的 <code>gas</code></li>
<li><code>msg.data (bytes)</code>: 完整的 <code>calldata</code></li>
<li><code>msg.gas (uint)</code>: 剩余 <code>gas</code> - 自 0.4.21 版本开始已经不推荐使用，由 gesleft() 代替</li>
<li><strong><code>msg.sender (address)</code>:</strong> 消息发送者（当前调用）</li>
<li><code>msg.sig (bytes4)</code>: calldata 的前 4 字节（也就是函数标识符）</li>
<li><code>msg.value (uint)</code>: 随消息发送的 wei 的数量</li>
<li><code>now (uint)</code>: 目前区块时间戳（<code>block.timestamp</code>）</li>
<li><code>tx.gasprice (uint)</code>: 交易的<code>gas</code> 价格</li>
<li><code>tx.origin (address)</code>: 交易发起者（完全的调用链）</li>
</ul>
<p><strong><a href="https://solidity-cn.readthedocs.io/zh/develop/abi-spec.html#abi" target="_blank" rel="noopener">ABI 编码函数</a></strong></p>
<ul>
<li><code>abi.encode(...) returns (bytes)</code>： ABI - 对给定参数进行编码</li>
<li><code>abi.encodePacked(...) returns (bytes)</code>：对给定参数执行 紧打包编码</li>
<li><code>abi.encodeWithSelector(bytes4 selector, ...) returns (bytes)：</code> ABI - 对给定参数进行编码，并以给定的函数选择器作为起始的 4 字节数据一起返回</li>
<li><code>abi.encodeWithSignature(string signature, ...) returns (bytes)</code>：等价于 <code>abi.encodeWithSelector(bytes4(keccak256(signature), ...)</code></li>
</ul>
<p><strong>错误处理</strong></p>
<ul>
<li><code>assert(bool condition)</code>:<br>  如果条件不满足，则使当前交易没有效果 — 用于检查内部错误。</li>
<li><code>require(bool condition)</code>:<br>  如果条件不满足则撤销状态更改 - 用于检查由输入或者外部组件引起的错误。</li>
<li><code>require(bool condition, string message)</code>:<br>  如果条件不满足则撤销状态更改 - 用于检查由输入或者外部组件引起的错误，可以同时提供一个错误消息。</li>
<li><code>revert()</code>:<br>  终止运行并撤销状态更改。</li>
<li><code>revert(string reason)</code>:<br>  终止运行并撤销状态更改，可以同时提供一个解释性的字符串。</li>
</ul>
<p><strong>地址相关</strong></p>
<ul>
<li><code>&lt;address&gt;.balance (uint256)</code>:<br>  以 Wei 为单位的 地址类型 的余额。</li>
<li><code>&lt;address&gt;.transfer(uint256 amount)</code>:<br>  向 地址类型 发送数量为 amount 的 Wei，失败时抛出异常，发送 2300 gas 的矿工费，不可调节。</li>
<li><code>&lt;address&gt;.send(uint256 amount) returns (bool)</code>:<br>  向 地址类型 发送数量为 amount 的 Wei，失败时返回 false，发送 2300 gas 的矿工费用，不可调节。</li>
<li><code>&lt;address&gt;.call(...) returns (bool)</code>:<br>  发出低级函数 CALL，失败时返回 false，发送所有可用 gas，可调节。</li>
<li><code>&lt;address&gt;.callcode(...) returns (bool)</code>：<br>  发出低级函数 CALLCODE，失败时返回 false，发送所有可用 gas，可调节。</li>
<li><code>&lt;address&gt;.delegatecall(...) returns (bool):</code><br>  发出低级函数 DELEGATECALL，失败时返回 false，发送所有可用 gas，可调节。 </li>
</ul>
<h3 id="3-4-表达式和控制结构"><a href="#3-4-表达式和控制结构" class="headerlink" title="3.4 表达式和控制结构(*)"></a>3.4 表达式和控制结构(*)</h3><h3 id="3-4-1-控制结构"><a href="#3-4-1-控制结构" class="headerlink" title="3.4.1 控制结构"></a>3.4.1 控制结构</h3><p>avaScript 中的大部分控制结构在 Solidity 中都是可用的，除了 <code>switch</code> 和 <code>goto</code>。 因此 Solidity 中有 <code>if，else，while，do，for，break，continue，return，? :</code>这些与在 C 或者 JavaScript 中表达相同语义的关键词。</p>
<p>用于表示条件的括号 <strong>不可以</strong> 被省略，单语句体两边的花括号可以被省略。</p>
<p>注意，与 C 和 JavaScript 不同， Solidity 中非布尔类型数值不能转换为布尔类型，因此 <code>if (1) { ... }</code> 的写法在 Solidity 中 无效 。</p>
<p>当一个函数有多个输出参数时， <code>return (v0, v1, ...,vn)</code> 写法可以返回多个值。不过元素的个数必须与输出参数的个数相同</p>
<h3 id="3-4-2-通过-new-创建合约"><a href="#3-4-2-通过-new-创建合约" class="headerlink" title="3.4.2 通过 new 创建合约"></a>3.4.2 通过 new 创建合约</h3><p>使用关键字 <code>new</code> 可以创建一个新合约。待创建合约的完整代码必须事先知道，因此递归的创建依赖是不可能的。</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line"></span><br><span class="line">contract D &#123;</span><br><span class="line">    uint x;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">D</span>(<span class="params">uint a</span>) <span class="title">public</span> <span class="title">payable</span> </span>&#123;</span><br><span class="line">        x = a;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">contract C &#123;</span><br><span class="line">    D d = <span class="keyword">new</span> D(<span class="number">4</span>); <span class="comment">// 将作为合约 C 构造函数的一部分执行</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">createD</span>(<span class="params">uint arg</span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        D newD = <span class="keyword">new</span> D(arg);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">createAndEndowD</span>(<span class="params">uint arg, uint amount</span>) <span class="title">public</span> <span class="title">payable</span> </span>&#123;</span><br><span class="line">        <span class="comment">//随合约的创建发送 ether</span></span><br><span class="line">        D newD = (<span class="keyword">new</span> D).value(amount)(arg);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如示例中所示，使用 <code>.value（）</code> 选项创建 <code>D</code> 的实例时可以转发 <code>Ether</code>，但是不可能限制 <code>gas</code> 的数量。如果创建失败（可能因为栈溢出，或没有足够的余额或其他问题），会引发异常。</p>
<h3 id="3-4-3-错误处理：Assert-Require-Revert-and-Exceptions"><a href="#3-4-3-错误处理：Assert-Require-Revert-and-Exceptions" class="headerlink" title="3.4.3 错误处理：Assert, Require, Revert and Exceptions"></a>3.4.3 错误处理：Assert, Require, Revert and Exceptions</h3><p><code>Solidity</code> 使用状态恢复异常来处理错误。这种异常将撤消对当前调用（及其所有子调用）中的状态所做的所有更改，并且还向调用者标记错误。 便利函数 <code>assert</code> 和 <code>require</code> 可用于检查条件并在条件不满足时抛出异常。<code>assert</code> 函数只能用于测试内部错误，并检查非变量。</p>
<p> <code>require</code> 函数用于确认条件有效性，例如输入变量，或合约状态变量是否满足条件，或验证外部合约调用返回的值。 如果使用得当，分析工具可以评估你的合约，并标示出那些会使 <code>assert</code> 失败的条件和函数调用。 正常工作的代码不会导致一个 <code>assert</code>语句的失败；如果这发生了，那就说明出现了一个需要你修复的 bug。</p>
<p>还有另外两种触发异常的方法：<code>revert</code> 函数可以用来标记错误并恢复当前的调用。 <code>revert</code> 调用中包含有关错误的详细信息是可能的，这个消息会被返回给调用者。已经不推荐的关键字 <code>throw</code> 也可以用来替代 <code>revert()</code> （但无法返回错误消息）。</p>
<p>在下例中，你可以看到如何轻松使用<code>require</code>检查输入条件以及如何使用<code>assert</code>检查内部错误，注意，你可以给 require 提供一个消息字符串，而 assert 不行。</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.22</span>;</span><br><span class="line"></span><br><span class="line">contract Sharer &#123;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">sendHalf</span>(<span class="params">address addr</span>) <span class="title">public</span> <span class="title">payable</span> <span class="title">returns</span> (<span class="params">uint balance</span>) </span>&#123;</span><br><span class="line">        <span class="built_in">require</span>(msg.value % <span class="number">2</span> == <span class="number">0</span>, <span class="string">"Even value required."</span>);</span><br><span class="line">        uint balanceBeforeTransfer = <span class="keyword">this</span>.balance;</span><br><span class="line">        addr.transfer(msg.value / <span class="number">2</span>);</span><br><span class="line">                    <span class="comment">//由于转移函数在失败时抛出异常并且不能在这里回调，因此我们应该没有办法仍然有一半的钱。</span></span><br><span class="line">        assert(<span class="keyword">this</span>.balance == balanceBeforeTransfer - msg.value / <span class="number">2</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.balance;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-5-合约"><a href="#3-5-合约" class="headerlink" title="3.5 合约"></a>3.5 合约</h3><p>Solidity 合约类似于面向对象语言中的类。合约中有用于数据持久化的状态变量，和可以修改状态变量的函数。 调用另一个合约实例的函数时，会执行一个 EVM 函数调用，这个操作会切换执行时的上下文，这样，前一个合约的状态变量就不能访问了。</p>
<h3 id="3-5-1-创建合约"><a href="#3-5-1-创建合约" class="headerlink" title="3.5.1 创建合约"></a>3.5.1 创建合约</h3><p>可以通过以太坊交易“从外部”或从 Solidity 合约内部创建合约。<br>创建合约时，会执行一次构造函数（与合约同名的函数）。构造函数是可选的。只允许有一个构造函数，这意味着不支持重载。</p>
<p>在内部，构造函数参数在合约代码之后通过 <code>ABI</code> 编码 传递，但是如果你使用 <code>web3.js</code> 则不必关心这个问题。</p>
<p>如果一个合约想要创建另一个合约，那么创建者必须知晓被创建合约的源代码(和二进制代码)。 这意味着不可能循环创建依赖项。</p>
<h3 id="3-5-2-getter-函数"><a href="#3-5-2-getter-函数" class="headerlink" title="3.5.2 getter 函数"></a>3.5.2 getter 函数</h3><p>编译器自动为所有 <code>public</code> 状态变量创建 <code>getter</code> 函数。对于下面给出的合约，编译器会生成一个名为 <code>data</code> 的函数， 该函数不会接收任何参数并返回一个 <code>uint</code> ，即状态变量 <code>data</code> 的值。可以在声明时完成状态变量的初始化</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line"></span><br><span class="line">contract C &#123;</span><br><span class="line">    uint public data = <span class="number">42</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">contract Caller &#123;</span><br><span class="line">    C c = <span class="keyword">new</span> C();</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">f</span>(<span class="params"></span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        uint local = c.data();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>getter 函数具有外部可见性。如果在内部访问 getter（即没有 this. ），它被认为一个状态变量。 如果它是外部访问的（即用 this. ），它被认为为一个函数。</p>
<h3 id="3-5-3-View-函数"><a href="#3-5-3-View-函数" class="headerlink" title="3.5.3 View 函数"></a>3.5.3 View 函数</h3><p>可以将函数声明为 view 类型，这种情况下要保证不修改状态。</p>
<p>下面的语句被认为是修改状态：</p>
<ol>
<li>修改状态变量。</li>
<li>产生事件。</li>
<li>创建其它合约。</li>
<li>使用 selfdestruct。</li>
<li>通过调用发送以太币。</li>
<li>调用任何没有标记为 view 或者 pure 的函数。</li>
<li>使用低级调用。</li>
<li>使用包含特定操作码的内联汇编。</li>
</ol>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.16</span>;</span><br><span class="line"></span><br><span class="line">contract C &#123;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">f</span>(<span class="params">uint a, uint b</span>) <span class="title">public</span> <span class="title">view</span> <span class="title">returns</span> (<span class="params">uint</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> a * (b + <span class="number">42</span>) + now;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-5-4-Pure-函数"><a href="#3-5-4-Pure-函数" class="headerlink" title="3.5.4 Pure 函数"></a>3.5.4 Pure 函数</h3><p>函数可以声明为 pure ，在这种情况下，承诺不读取或修改状态。</p>
<p>除了上面解释的状态修改语句列表之外，以下被认为是从状态中读取：</p>
<ol>
<li>读取状态变量。</li>
<li>访问 this.balance 或者 <address>.balance。</address></li>
<li>访问 block，tx， msg 中任意成员 （除 msg.sig 和 msg.data 之外）。</li>
<li>调用任何未标记为 pure 的函数。</li>
<li>使用包含某些操作码的内联汇编。</li>
</ol>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.16</span>;</span><br><span class="line"></span><br><span class="line">contract C &#123;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">f</span>(<span class="params">uint a, uint b</span>) <span class="title">public</span> <span class="title">pure</span> <span class="title">returns</span> (<span class="params">uint</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> a * (b + <span class="number">42</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="四、练习题"><a href="#四、练习题" class="headerlink" title="四、练习题"></a>四、练习题</h2><h3 id="4-1-将固定长度字节数组转化为string类型"><a href="#4-1-将固定长度字节数组转化为string类型" class="headerlink" title="4.1 将固定长度字节数组转化为string类型"></a>4.1 将固定长度字节数组转化为<code>string</code>类型</h3><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.0</span>;</span><br><span class="line"></span><br><span class="line">contract bytes32tostring&#123;</span><br><span class="line">    </span><br><span class="line">    bytes10 testword=<span class="number">0x68656c6c6f776f726c64</span>; <span class="comment">//为helloworld</span></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">bytes32tostringF</span>(<span class="params"></span>) <span class="title">public</span> <span class="title">view</span> <span class="title">returns</span>(<span class="params">string</span>)</span>&#123;</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-2-实现一个带有简单逻辑判断及多种数学运算的Solidity程序"><a href="#4-2-实现一个带有简单逻辑判断及多种数学运算的Solidity程序" class="headerlink" title="4.2 实现一个带有简单逻辑判断及多种数学运算的Solidity程序"></a>4.2 实现一个带有简单逻辑判断及多种数学运算的Solidity程序</h3><p><strong>参考自：</strong></p>
<ol>
<li><p>黄皮书：<a href="https://github.com/yuange1024/ethereum_yellowpaper/blob/master/ethereum_yellow_paper_cn.pdf" target="_blank" rel="noopener">https://github.com/yuange1024/ethereum_yellowpaper/blob/master/ethereum_yellow_paper_cn.pdf</a></p>
</li>
<li><p>白皮书：<a href="https://github.com/ethereum/wiki/wiki/White-Paper" target="_blank" rel="noopener">https://github.com/ethereum/wiki/wiki/White-Paper</a><br> <a href="https://blog.csdn.net/weixin_45067603" target="_blank" rel="noopener">INlinKC</a><br> <a href="https://ethfans.org/wikis/Home" target="_blank" rel="noopener">https://ethfans.org/wikis/Home</a></p>
</li>
<li>以太坊solidity学习记录: <a href="https://blog.csdn.net/weixin_45067603/article/details/105726491" target="_blank" rel="noopener">https://blog.csdn.net/weixin_45067603/article/details/105726491</a></li>
<li><a href="https://www.bilibili.com/video/BV1sJ411D72u" target="_blank" rel="noopener">尚硅谷区块链全套Go语言→GoWeb→以太坊→项目实战</a></li>
</ol>
]]></content>
      <categories>
        <category>Blockchain</category>
      </categories>
  </entry>
  <entry>
    <title>区块链入门（二）基础知识介绍</title>
    <url>/posts/fb46f828.html</url>
    <content><![CDATA[<font color="red"> 注：本教程为技术教程，不谈论且不涉及炒作任何数字货币 </font>

<p>本次组队学习重点在于以太坊基础知识、以太坊客户端以及以太坊solidity编程，因此本节教程重点在于以太坊核心知识点的掌握，区块链部分的基础知识可以作为补充，请学习者量力而行。另外若学习者觉得本节内容难度太高，可以先对基本知识点有一个概览，在第二节以及第三节实战内容学习完成之后再深入学习本节内容。</p>
<h1 id="一、区块链简介"><a href="#一、区块链简介" class="headerlink" title="一、区块链简介"></a>一、区块链简介</h1><h2 id="1-1、区块链与区块链技术"><a href="#1-1、区块链与区块链技术" class="headerlink" title="1.1、区块链与区块链技术"></a>1.1、区块链与区块链技术</h2><p>在阅读本教程之前，<a href="http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html" target="_blank" rel="noopener">大家对比特币原理不太了解同学可以先阅读下此博客~</a>,大家对比特币有简单了解后对于区块链会有更好的认识。</p>
<p><strong>区块链</strong>是将记录（区块）通过密码学串联并加密的链式数据结构。而<strong>区块链技术</strong>，是通过P2P网络和区块链来实现数据存储的<strong>去中心化</strong>、<strong>不可逆</strong>和<strong>不可篡改</strong>。比特币正是构建在区块链技术上的典型应用。通过区块链技术，我们可以将信息（数据、程序）保存在区块上并接入到区块链中，这样就实现了信息的去中心化存储、不可逆和不可篡改。<strong>区块链应用</strong>是指利用区块链技术开发的应用。</p>
<h2 id="1-2、区块链历史"><a href="#1-2、区块链历史" class="headerlink" title="1.2、区块链历史"></a>1.2、区块链历史</h2><p>2008年，一个网名叫中本聪（Satoshi Nakamoto）的人发表了一篇名为《比特币：一种点对点电子货币系统》的论文，论文中首次提到了“区块链”这一概念。2009年，中本聪创立了以区块链为底层技术的比特币网络，开发出了第一个区块，被称为“创世区块”。该阶段被称为“区块链1.0”。</p>
<p>由于比特币是一个电子货币系统，所以主要功能就是记账。但随后人们发现，区块链技术作为比特币的底层技术，功能可以远远不止于记账，许多关于“未知的信任”的问题，都可以通过区块链来解决，例如电子存证、信息记录等。于是在比特币的基础上，诞生了带有智能合约的区块链系统，即允许开发者通过编写智能合约来实现特定的逻辑，这一阶段被称为“区块链2.0”。这一阶段的主要代表是以太坊。</p>
<p>随后，人们想要提升区块链应用的性能，于是出现了EOS、ArcBlock等系统，其特点是高性能、大吞吐量，但由于引入了超级节点、云节点等特性，弱化了“去中心化”这一特点，因此受到较大的争议。这一阶段被称为“区块链3.0”。</p>
<p>由于比特币是一款电子货币，可扩展性较低，而所谓的“区块链3.0”目前受到较大争议，且部分项目的底层算法完全不同于典型的区块链，因此学习区块链2.0中的以太坊是目前学习区块链的最佳方式。</p>
<h2 id="1-3、区块链基础技术与算法"><a href="#1-3、区块链基础技术与算法" class="headerlink" title="1.3、区块链基础技术与算法"></a>1.3、区块链基础技术与算法</h2><p>区块链技术不是单独的一项技术，而是一系列技术组成的技术栈，其具有以下的特点：</p>
<ul>
<li>数据分布式存储</li>
<li>存储的数据不可逆、不可篡改、可回溯</li>
<li>数据的创建和维护由所有参与方共同参与</li>
</ul>
<p>为了实现这些特点、维护区块链应用的稳定运行，区块链技术中包含了分布式存储技术、密码学技术、共识机制以及区块链2.0提出的智能合约。</p>
<h3 id="1-3-1、区块"><a href="#1-3-1、区块" class="headerlink" title="1.3.1、区块"></a>1.3.1、区块</h3><p>区块链由一个个区块（block）组成。区块很像数据库的记录，每次写入数据，就是创建一个区块。</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\bg2017122703.png" width="300">
</center>
<center>中心化存储</center>

<p>每个区块包含两个部分。</p>
<blockquote>
<ul>
<li>区块头（Head）：记录当前区块的特征值</li>
<li>区块体（Body）：实际数据</li>
</ul>
</blockquote>
<p>区块头包含了当前区块的多项特征值。</p>
<blockquote>
<ul>
<li>生成时间</li>
<li>实际数据（即区块体）的哈希</li>
<li>上一个区块的哈希</li>
<li>…</li>
</ul>
</blockquote>
<h3 id="1-3-2、分布式存储技术"><a href="#1-3-2、分布式存储技术" class="headerlink" title="1.3.2、分布式存储技术"></a>1.3.2、分布式存储技术</h3><p>与传统的数据存储技术不同，在区块链技术中，数据并不是集中存放在某个数据中心上，也不是由某个权威机构或是大多数节点来存储，而是分散存储在区块链网络中的每一个节点上。</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image2.png" width="300">
</center>
<center>中心化存储</center>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image3.png" width="300">
</center>
<center>分布式存储</center>

<p><strong>节点和区块的关系是什么？</strong></p>
<p>可以用共享文档来简单描述：所有可以访问共享文档的账号就叫做节点，当然全节点需要同步共享文档，也就是拥有全部的区块数据区块就是共享文档。每个人更新了，所有人都可以查看最新的文档</p>
<h3 id="1-3-3、密码学技术"><a href="#1-3-3、密码学技术" class="headerlink" title="1.3.3、密码学技术"></a>1.3.3、密码学技术</h3><p>为了实现数据的不可逆、不可篡改和可回溯，区块链技术采用了一系列密码学算法和技术，包括哈希算法、Merkle 树、非对称加密算法。</p>
<h5 id="哈希算法"><a href="#哈希算法" class="headerlink" title="哈希算法"></a>哈希算法</h5><p>哈希算法是一个单向函数，可以将任意长度的输入数据转化为固定长度的输出数据（哈希值），哈希值就是这段输入数据唯一的数值表现。由于在计算上不可能找到哈希值相同而输入值不同的字符串，因此两段数据的哈希值相同，就可以认为这两段数据也是相同的，所以哈希算法常被用于对数据进行验证。</p>
<p>在区块链中，数据存储在区块里。每个区块都有一个区块头，区块头中存储了一个将该区块所有数据经过哈希算法得到的哈希值，同时，每个区块中还存储了前一个区块的哈希值，这样就形成了区块链。如果想要篡改某一个区块A中的数据，就会导致A的哈希值发生变化，后一个区块B就无法通过哈希值正确地指向A，这样篡改者又必须篡改B中的数据……也就是说，篡改者需要篡改被篡改的区块以及后面的所有区块，才能让所有的节点都接受篡改。</p>
<h5 id="Merkle树"><a href="#Merkle树" class="headerlink" title="Merkle树"></a>Merkle树</h5><p>Merkle树是一种树形结构，在区块链中，Merkle树的叶子节点是区块中数据的哈希值，非叶子节点是其子结点组合后的哈希值，这样由叶子节点开始逐层往上计算，最终形成一个Merkle根，记录在区块的头部，这样就可以保证每一笔交易都无法篡改。</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image4.png" width="500">
</center>
<center>Merkle 树</center>

<h5 id="非对称加密技术"><a href="#非对称加密技术" class="headerlink" title="非对称加密技术"></a>非对称加密技术</h5><p>非对称加密技术使用两个非对称密钥：公钥和私钥。公钥和私钥具有两个特点：</p>
<ol>
<li>通过其中一个密钥加密信息后，使用另一个密钥才能解开</li>
<li>公钥一般可以公开，私钥则保密</li>
</ol>
<p>在区块链中，非对称加密技术主要用于信息加密、数字签名和登录认证。在信息加密场景中，信息发送者A使用接收者B提供的公钥对信息进行加密，B收到加密的信息后再通过自己的私钥进行解密。再数字签名场景中，发送者A通过自己的私钥对信息进行加密，其他人通过A提供的公钥来对信息进行验证，证明信息确实是由A发出。在登录认证场景中，客户端使用私钥加密登录信息后进行发送，其他人通过客户端公钥来认证登录信息。</p>
<ul>
<li><p>RSA 算法</p>
<p>​        RSA加密算法是最常用的非对称加密算法，CFCA在证书服务中离不了它。但是有不少新来的同事对它不太了解，恰好看到一本书中作者用实例对它进行了简化而生动的描述，使得高深的数学理论能够被容易地理解。<br>​       RSA是第一个比较完善的公开密钥算法，它既能用于加密，也能用于数字签名。RSA以它的三个发明者Ron Rivest, Adi Shamir, Leonard Adleman的名字首字母命名，这个算法经受住了多年深入的密码分析，虽然密码分析者既不能证明也不能否定RSA的安全性，但这恰恰说明该算法有一定的可信性，目前它已经成为最流行的公开密钥算法。<br>　　RSA的安全基于大数分解的难度。其公钥和私钥是一对大素数（100到200位十进制数或更大）的函数。从一个公钥和密文恢复出明文的难度，等价于分解两个大素数之积（这是公认的数学难题）。 </p>
</li>
<li><p>ECC 椭圆曲线算法</p>
<p>具体可以参见此文章：<a href="https://zhuanlan.zhihu.com/p/36326221" target="_blank" rel="noopener">ECC椭圆曲线加密算法：介绍</a></p>
</li>
</ul>
<h3 id="1-3-4、共识机制"><a href="#1-3-4、共识机制" class="headerlink" title="1.3.4、共识机制"></a>1.3.4、共识机制</h3><p>区块链系统是一个分布式系统，分布式系统要解决都首要问题就是一致性问题，也就是如何使多个孤立的节点达成共识。在中心化系统中，由于有一个中心服务器这样的“领导”来统一各个节点，因此达成一致性几乎没有问题。但在去中心化场景下，由于各个节点是相互独立的，就可能会出现许多不一致的问题，例如由于网络状况等因素部分节点可能会有延迟、故障甚至宕机，造成节点之间通信的不可靠，因此一致性问题是分布式系统中一个很令人头疼的问题。</p>
<p>由 Eirc Brewer 提出，Lynch 等人证明的 CAP 定理为解决分布式系统中的一致性问题提供了思路。CAP 定理的描述如下：在分布式系统中，<strong>一致性</strong>、<strong>可用性</strong>和<strong>分区容错性</strong>三者不可兼得。这三个术语的解释如下：</p>
<ul>
<li>一致性（<strong>C</strong>onsistency）：所有节点在同一时刻拥有同样的值（等同于所有节点访问同一份最新的数据副本</li>
<li>可用性（<strong>A</strong>vailability）：每个请求都可以在有限时间内收到确定其是否成功的响应</li>
<li>分区容错性（<strong>P</strong>artition tolerance）：分区是指部分节点因为网络原因无法与其他节点达成一致。分区容错性是指由网络原因导致的系统分区不影响系统的正常运行。例如，由于网络原因系统被分为 A, B, C, D 四个区，A, B 中的节点无法正常工作，但 C, D 组成的分区仍能提供正常服务。</li>
</ul>
<p>在某些场景下，对一致性、可用性和分区容错性中的某一个特性要求不高时，就可以考虑弱化该特性，来保证整个系统的容错能力。区块链中常见的共识机制的基本思路正是来自 CAP 定理，部分区块链应用中用到的共识机制如下表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>共识机制</th>
<th>应用</th>
</tr>
</thead>
<tbody>
<tr>
<td>PoW</td>
<td>比特币、莱特币、以太坊的前三个阶段</td>
</tr>
<tr>
<td>PoS</td>
<td>PeerCoin、NXT、以太坊的第四个阶段</td>
</tr>
<tr>
<td>PBFT</td>
<td>Hyperledger Fabric</td>
</tr>
</tbody>
</table>
</div>
<h5 id="PoW（Proof-of-Work，工作量证明）"><a href="#PoW（Proof-of-Work，工作量证明）" class="headerlink" title="PoW（Proof of Work，工作量证明）"></a>PoW（Proof of Work，工作量证明）</h5><p>PoW 机制的大致流程如下：</p>
<ol>
<li>向所有节点广播新交易和一个数学问题</li>
<li>最先解决了数学问题的节点将交易打包成区块，对全网广播</li>
<li>其他节点验证广播区块的节点是否解决了数学问题（完成了一定的工作量），验证通过则接受该区块，并将该区块的哈希值放入下一个区块中，表示承认该区块</li>
</ol>
<p>由于在 PoW 机制中，区块的产生需要解决一个数学问题，也就是所谓的<strong>挖矿</strong>，这往往要消耗较大的算力和电力，因此节点们倾向于在<strong>最长的链</strong>的基础上添加区块，因为如果节点想在自己的链上添加新的区块，那么就需要重新计算 1 个或 $n$ 个这样的数学问题（每添加一个区块就需要计算一个）。因此在比特币中最长的链被认为是合法的链，这样节点间就形成了一套“共识”。</p>
<p>PoW 机制的优点是完全去中心化，缺点是需要依赖数学运算，资源的消耗会比其他的共识机制高，可监管性弱，同时每次达成共识需要全网共同参与运算，性能较低。</p>
<h5 id="PoS（Proof-of-Stack，股权证明）"><a href="#PoS（Proof-of-Stack，股权证明）" class="headerlink" title="PoS（Proof of Stack，股权证明）"></a>PoS（Proof of Stack，股权证明）</h5><p>PoS 针对 PoW 的缺点做出了改进。PoS 要求参与者预先放置一些货币在区块链上用于换取“股权”，从而成为<strong>验证者（Validator）</strong>，验证者具有产生区块的权利。PoS 机制会按照存放货币的量和时间给验证者分配相应的利息，同时还引入了奖惩机制，打包错误区块的验证者将失去他的股权——即投入的货币以及产生区块的权利。PoS 机制的大致流程如下：</p>
<ol>
<li>加入 PoS 机制的都是持币人，称为验证者</li>
<li>PoS 算法根据验证者持币的多少在验证者中挑选出一个给予产生区块的权利</li>
<li>如果一定时间内没有产生区块，PoS 就挑选下一个验证者，给予产生区块的权利</li>
<li>如果某个验证者打包了一份欺诈性交易，PoS 将剥夺他的股权</li>
</ol>
<p>PoS 的优点在于：</p>
<ol>
<li>引入了利息，使得像比特币这样发币总数有限的通货紧缩系统在一定时间后不会“无币可发”</li>
<li>引入了奖惩机制使节点的运行更加可控，同时更好地防止攻击</li>
<li>与 PoW 相比，不需要为了生成新区块而消耗大量电力和算力</li>
<li>与 PoW 相比，缩短了达成共识所需的时间</li>
</ol>
<p>由于 PoS 机制需要用户已经持有一定数量的货币，没有提供在区块链应用创立初始阶段处理数字货币的方法，因此使用 PoS 机制的区块链应用会在发布时预先出售货币，或在初期采用 PoW，让矿工获得货币后再转换成 PoS，例如以太坊现阶段采用的是 PoW 机制，在第四阶段“宁静”（Serenity）中将过渡到 PoS。</p>
<h5 id="拜占庭将军问题（Byzantine-Generals-Problem）"><a href="#拜占庭将军问题（Byzantine-Generals-Problem）" class="headerlink" title="拜占庭将军问题（Byzantine Generals Problem）"></a>拜占庭将军问题（Byzantine Generals Problem）</h5><p>拜占庭将军问题是分布式网络中的通信容错问题，可以描述为：</p>
<blockquote>
<p>一组拜占庭将军各领一支队伍共同围困一座城市。各支军队的行动策略限定为进攻或撤离两种。因为部分军队进攻而部分军队撤离可能会造成灾难性的后果，因此各将军决定通过投标来达成一致策略，即“共进退”。因为各将军位于城市不同方向，他们只能通过信使互相联系。在投票过程中每位将军都将自己的选择（进攻或撤退）通过信使分别通知其他所有将军，这样一来每位将军根据自己的投票和其他所有将军送来的信息就可以知道共同投票的结果，进而做出行动。</p>
</blockquote>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image1.png" width="500">
</center>



<p>拜占庭将军的问题在于，将军中可能出现叛徒。假设3名将军中有1名叛徒，2名忠诚将军一人投进攻票，一人投撤退票，这时叛徒可能会故意给投进攻的将军投进攻票，而给投撤退的将军投撤退票。这就导致一名将军带队发起进攻，而另外一名将军带队撤退。</p>
<p>另外，由于将军之间通过信使进行通讯，即使所有将军都忠诚，也不能排除信使被敌人截杀，甚至信使叛变等情况。</p>
<p>假设存在叛变将军或信使出问题等情况，如果忠诚将军仍然能够通过投票来决定他们的战略，便称系统达到了<strong>拜占庭容错（Byzantine Fault Tolerance）</strong>。</p>
<p>拜占庭问题对应到区块链中，将军就是节点，信使就是网络等通信系统，要解决的是存在恶意节点、网络错误等情况下系统的一致性问题。</p>
<p><strong>PBFT（Practical Byzantine Fault Tolerance）</strong> 是第一个得到广泛应用且比较高效的拜占庭容错算法，能够在节点数量不小于 $n=3f+1$ 的情况下容忍 $f$ 个拜占庭节点（恶意节点）。</p>
<h1 id="二、以太坊介绍"><a href="#二、以太坊介绍" class="headerlink" title="二、以太坊介绍"></a>二、以太坊介绍</h1><p>首先我们要知道我们为什么要学习以太坊，主要有以下四个原因：</p>
<ul>
<li>以太坊是区块链2.0的代表，学习以太坊能了解到区块链技术的所有知识</li>
<li>引入了智能合约，拓宽了区块链的应用场景</li>
<li>对开发者友好、对用户友好，容易编写出简单的区块链应用，学习趣味性高</li>
<li>Solidity 语法与 Javascript、Go 等语言接近，易上手</li>
</ul>
<h2 id="2-1、以太坊简介"><a href="#2-1、以太坊简介" class="headerlink" title="2.1、以太坊简介"></a>2.1、以太坊简介</h2><p>区块链技术常常被认为是自互联网诞生以来最具颠覆性的技术，然而，自比特币诞生后一直没有很好的区块链应用开发平台。想要在比特币基础上开发区块链应用是非常复杂繁琐的，因为比特币仅仅是一个加密数字货币系统，无法用来实现更广阔的业务需求。以太坊是目前使用最广泛的支持完备应用开发的共有区块链系统。</p>
<p>和比特币不同，比特币只适合加密数字货币场景，不具备图灵完备性，也缺乏保存实时状态的账户概念，以及存在 PoW 机制带来的效率和资源浪费的问题，而以太坊作为区块链2.0的代表，目标是扩展智能合约和建立一个去中心化应用平台，具有图灵完备的特性、更高效的共识机制、支持智能合约等多种应用场景，使得开发者能够很方便地在以太坊上开发出基于区块链的应用。</p>
<h3 id="2-1-1、以太坊的发展"><a href="#2-1-1、以太坊的发展" class="headerlink" title="2.1.1、以太坊的发展"></a>2.1.1、以太坊的发展</h3><p>2014年， Vitalik Buterin 发表了文章《以太坊：一个下一代智能合约和去中心化应用平台》。同年，Buterin 在迈阿密比特币会议中宣布启动以太坊项目，并提出了多项创新性的区块链技术。2015年，以太坊CCO Stephan Tual 在官方博客上宣布以太坊系统诞生，主网上线。</p>
<p>以太坊发展至今经历了“前沿”（Frontier）、“家园”（Homestead）以及现在所处的“大都会”（Metropolis）三个阶段。第四阶段“宁静”（Serenity）将作为以太坊的最后一个阶段，目前尚未有计划发布日期。</p>
<h3 id="2-1-2、以太坊的特点"><a href="#2-1-2、以太坊的特点" class="headerlink" title="2.1.2、以太坊的特点"></a>2.1.2、以太坊的特点</h3><p>以太坊团队和外界对以太坊的描述都是“世界计算机”，这代表它是一个开源的、全球的去中心化计算架构。它执行称为智能合约的程序，并使用区块链来同步和存储系统状态，以及使用名为以太币的加密数字货币来计量和约束执行操作的资源成本。同时，以太坊提供了一系列的接口，使得开发者能够通过以太坊来开发去中心化 Web 应用DApps。</p>
<h3 id="2-1-3、智能合约"><a href="#2-1-3、智能合约" class="headerlink" title="2.1.3、智能合约"></a>2.1.3、智能合约</h3><p>相比比特币，以太坊最大的特点就是引入了<strong>智能合约</strong>。智能合约本质上就是一段编写好的程序，可以在特定的条件下被触发并执行特定的操作。由于区块链具有不可逆和不可篡改的特点，因此智能合约与区块链结合后，就成了一份“强制执行”的合约。</p>
<p>以太坊能够作为一个去中心化应用平台和”世界计算机”，其核心就是智能合约。智能合约的引入，使得开发者能够实现许多（理论上是任何）业务逻辑。如果说比特币是通过区块链技术开发的特定计算器，那么引入了智能合约的以太坊就是基于区块链技术的通用计算机。可以简单的理解成：比特币的交易系统就是一份写死的智能合约，而以太坊则将智能合约的开发权限交给开发者。</p>
<p>以太坊提供了对智能合约的全面支持，包括编写智能合约编程语言 <strong>Solidity</strong> 和运行智能合约的<strong>以太坊虚拟机（Ethereum Virtual Machine，EVM）</strong>。</p>
<h3 id="2-1-4、幽灵协议"><a href="#2-1-4、幽灵协议" class="headerlink" title="2.1.4、幽灵协议"></a>2.1.4、幽灵协议</h3><p>幽灵合约的英文是“Greedy Heaviest Observed Subtree” (GHOST) protocol，在介绍幽灵协议之前，先介绍以太坊中的叔区块、叔块奖励和叔块引用奖励这三个概念。</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image5.png" width="400">
</center>


<p>假设目前以太坊区块链中的区块高度（区块链上的区块个数）为6，现在产生了一笔新的交易，矿工A先将该笔交易打包成了区块 Block 7，在矿工A将 Block 7 广播到其他节点的这段时间里，矿工B和矿工C又分别产生了 Block 8 和 Block 9。Block 7、Block 8、Block 9 都指向 Block 6，即 Block 6 是他们的父区块。由于 Block 7 是最先产生的，因此 Block 7 被认为是有效区块，Block 8 和 Block 9 就是<strong>叔区块</strong>（作废区块）。</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image6.png" width="300">
</center>


<p>现在链上的区块高度为7，在这基础上又产生了新的交易，并被打包成了 Block 10。在以太坊中，Block 10 除了可以引用它的父区块 Block 7 外，还可以引用叔区块 Block 8 和 Block 9。并且，Block 8 和 Block 9 的矿工会因此获得一笔奖励，称为<strong>叔块奖励</strong>，Block 10 的矿工除了基础奖励之外，由于引用了叔区块，还会获得一笔额外的<strong>叔块引用奖励</strong>。</p>
<p><strong>幽灵协议</strong>是以太坊的一大创新。由于在比特币中的出块时间被设计为10分钟，而以太坊为了提高出块速度，将出块时间设计为12秒（实际14~15秒左右），这样的高速出块意味着高速确认，高速确认会带来区块的<strong>高作废率</strong>和<strong>低安全性</strong>。因为区块需要花一定的时间才能广播至全网，如果矿工 A 挖出了一个区块，而矿工 B 碰巧在 A 的区块扩散至 B 之前挖出了另一个区块，矿工 B 的区块就会作废并且没有对区块链的网络安全做出贡献。此外，这样的高速确认还会带来<strong>中心化</strong>的问题：如果 A 拥有全网 30% 的算力而 B 拥有 10% 的算力，那么 A 将会在 70% 的时间内都在产生作废区块，而 B 在 90% 的时间内都在产生作废区块，这样，B 永远追不上 A，后果是 A 通过其算力份额拥有对挖矿过程实际上的控制权，出现了算力垄断，弱化了去中心化。</p>
<p>幽灵协议正是为了解决上述问题而引入的，协议的主要内容如下：</p>
<ul>
<li>计算最长链时，不仅包括当前区块的父区块和祖区块，还包括祖先块的作废的后代区块（叔区块），将它们综合考虑来计算哪一个区块拥有支持其的最大工作量证明。这解决了网络安全性的问题</li>
<li>以太坊付给以“叔区块”身份为新块确认作出贡献的废区块87.5%的奖励（叔块奖励），把它们纳入计算的“侄子区块”将获得奖励的12.5%（叔块引用奖励）。这就使得即使产生作废区块的矿工也能够参与区块链网络贡献并获得奖励，解决了中心化倾向的问题</li>
<li>叔区块最深可以被其父母的第二代至第七代后辈区块引用。这样做是为了：<ul>
<li>降低引用叔区块的计算复杂性</li>
<li>过多的叔块引用奖励会剥夺矿工在主链上挖矿的激励，使得矿工有转向公开攻击者链上挖矿的倾向（即公开攻击者可能会恶意产生大量作废区块，无限引用将会诱使矿工转移到攻击者的链上，从而抛弃合法的主链）</li>
<li>计算表明带有激励的五层幽灵协议即使在出块时间为15s的情况下也实现了了95%以上的效率，而拥有25%算力的矿工从中心化得到的益处小于3%</li>
</ul>
</li>
</ul>
<h3 id="2-1-5、以太坊的组成部分"><a href="#2-1-5、以太坊的组成部分" class="headerlink" title="2.1.5、以太坊的组成部分"></a>2.1.5、以太坊的组成部分</h3><p>在以太坊中，包括了 P2P 网络、共识机制、交易、状态机、客户端这几个组成部分。</p>
<ul>
<li>P2P 网络：在以太坊主网上运行，可通过TCP端口30303访问，并运行称为 ÐΞVp2p 的协议。</li>
<li>共识机制：以太坊目前使用名为 Ethash 的 POW 算法，计划在将来会过渡到称为 Casper 的 POS 算法。</li>
<li>交易：以太坊中的交易本质上是网络消息，包括发送者、接收者、值和数据载荷（payload）。</li>
<li>状态机：以太坊的状态转移由以太坊虚拟机（Ethereum Virtual Machine，EVM）处理，EVM 能够将智能合约编译成机器码并执行。</li>
<li>客户端：用于用户和以太坊进行交互操作的软件实现，最突出的是 Go-Ethereum(Geth) 和 Parity。</li>
</ul>
<h3 id="2-1-6、以太坊中的概念"><a href="#2-1-6、以太坊中的概念" class="headerlink" title="2.1.6、以太坊中的概念"></a>2.1.6、以太坊中的概念</h3><ul>
<li>账户：以太坊中的账户类似于银行账户、应用账户，每个账户有一个20字节的地址。账户又分为<strong>普通账户</strong>（又叫外部账户，External Owned Account, EOA）和<strong>合约账户</strong>（Contract）。普通账户是由以太坊使用者创建的账户，包含地址、余额和随机数；合约账户是创建智能合约时建立的账户，包含存储空间和合约代码</li>
<li>状态：状态是由账户和两个账户之间价值的转移以及信息的状态转换构成的</li>
<li>地址：地址是一个账户 ECDSA 公钥的 Keccak 散列最右边的160位，通过地址可以在以太坊上接收或发送交易。在 Etherscan 上，可以通过地址来查询一个账户的信息</li>
<li>交易：以太坊中的交易不仅包括发送和接收以太币，还包括向合约账户发送交易来调用合约代码、向空用户发送交易来生成以交易信息为代码块的合约账户</li>
<li>Gas：Gas 是以太坊中的一种机制，用于执行智能合约或交易操作的虚拟燃料。由于以太坊是图灵完备的，为了避免开发者无意或恶意编写出死循环等浪费资源或滥用资源的情况，以太坊中的每一笔交易都需支付一定的 Gas （燃料费），即需支付一定的以太币作为 Gas。Gas 的金额通常是由交易的发起者指定并支付的</li>
<li>挖矿：和比特币类似，以太坊同样通过挖矿来产生区块。在以太坊目前的 PoW 机制下，每当一笔交易发出并广播，就会吸引矿工来将该交易打包成区块。每产生一个区块都会有一笔<strong>固定奖励</strong>给矿工，目前的固定奖励是3个以太。同时，区块中所有操作所需的 Gas 也会作为奖励给矿工。与比特币不同的是，以太坊中产生叔块的矿工可能会获得叔块奖励，引用叔块的矿工会获得叔块引用奖励</li>
<li>DApp（去中心化应用）：通过智能合约，开发者能够设计想要的逻辑，相当于是网站的后端。而 DApp 则相当于是一个完整的网站（前端+后端），因此 DApp = 智能合约 + Web 前端。以太坊提供了一个名为 web3.js 的 Javascript 库，通过 web3.js 可以实现 Web 与以太坊区块链的交互和与智能合约的交互，方便开发者创建 DApp</li>
</ul>
<h2 id="2-2、以太坊基础"><a href="#2-2、以太坊基础" class="headerlink" title="2.2、以太坊基础"></a>2.2、以太坊基础</h2><h3 id="2-2-1、以太坊中的货币"><a href="#2-2-1、以太坊中的货币" class="headerlink" title="2.2.1、以太坊中的货币"></a>2.2.1、以太坊中的货币</h3><p>以太坊中的货币称为 <strong>以太币</strong>，单位为<strong>以太（Ether）</strong>，也称 ETH 或符号 Ξ。以太可以被分割为更小的单位，最小的单位是 wei，1 以太 =  $10^18$  wei。以太币各单位的名称及之间的关系如下表：</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image-20210219000835894.png">
</center>





<h3 id="2-2-2、以太坊钱包"><a href="#2-2-2、以太坊钱包" class="headerlink" title="2.2.2、以太坊钱包"></a>2.2.2、以太坊钱包</h3><p>以太坊钱包是用于创建和广播交易的应用程序，常用的钱包有</p>
<ul>
<li>MetaMask，一款基于浏览器扩展的钱包，可以很方便地添加到 Chrome, FireFox 等支持扩展的浏览器中</li>
<li>Jaxx，一款跨平台、多币种的钱包</li>
<li>MyEtherWallet(MEW)，一款基于 Web 的钱包，可以在任何浏览器中运行</li>
<li>Emerald Wallet，一款被设计来用于以太坊经典区块链的钱包，但也与其他以太坊区块链兼容</li>
</ul>
<h4 id="MetaMask-基础"><a href="#MetaMask-基础" class="headerlink" title="MetaMask 基础"></a>MetaMask 基础</h4><p>以 Chrome 为例，访问 <a href="https://chrome.google.com/webstore/category/extensions" target="_blank" rel="noopener">Google 网上应用商店</a>，搜索 MetaMask 并添加至 Chrome</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image-20210219101124978.png">
</center>


<p>添加完成后 Chrome 会自动打开初始化页面</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image-20210219101226095.png">
</center>



<p>初次使用创建钱包</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image-20210219101300792.png">
</center>



<p>为钱包设置密码</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image-20210219101332089.png">
</center>




<p>创建密码后，MetaMask 会生成一串密语，密语是12个随机的英文单词，用于防止密码忘记。密语可以直接当成密码使用，因此需要妥善保管</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image-20210219102028033.png">
</center>



<p>注册完毕后就可以在 Chrome 地址栏右边的扩展程序栏点击 🦊 图标使用 MetaMask 了</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image-20210219102255927.png">
</center>

<center class="half">
    <img src="\Pic\Blockchain_Pic\image-20210219102322360.png">
</center>

<h4 id="获取测试以太"><a href="#获取测试以太" class="headerlink" title="获取测试以太"></a>获取测试以太</h4><p>除了以太坊主网以外，以太坊还提供了 Ropsten, Kovan, Rinkeby, Goerli 这几个公共测试网络，另外还支持局域网测试网络和自建测试网络。在这里我们切换到 Ropsten 测试网络</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image-20210219105616335.png">
</center>




<p>随后点击 <strong>Buy</strong> 按钮，点击<strong>测试水管</strong>下方的获取以太</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image-20210219105824087.png">
</center>




<p>在打开的页面中点击 request 1 ether from faucet 就可以得到1个测试以太，当然，可以多次点击。</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\image-20210219105911910.png">
</center>


<center class="half">
    <img src="\Pic\Blockchain_Pic\2021-02-19_110327.png">
</center>


<p>测试以太仅供测试使用，除此之外没有任何价值，测试完毕后剩下的以太可以发送到水龙头账户捐赠给水龙头，以供他人测试使用。</p>
<h2 id="2-3、以太坊交易的数据结构"><a href="#2-3、以太坊交易的数据结构" class="headerlink" title="2.3、以太坊交易的数据结构"></a>2.3、以太坊交易的数据结构</h2><p>在以太坊网络中，交易执行属于一个事务。具有原子性、一致性、隔离性、持久性特点。</p>
<ul>
<li>原子性： 是不可分割的最小执行单位，要么做，要么不做。</li>
<li>一致性： 同一笔交易执行，必然是将以太坊账本从一个一致性状态变到另一个一致性状态。</li>
<li>隔离性： 交易执行途中不会受其他交易干扰。</li>
<li>持久性： 一旦交易提交，则对以太坊账本的改变是永久性的。后续的操作不会对其有任何影响。</li>
</ul>
<p>以太坊交易的本质是由外部拥有的账户发起的签名消息，由以太坊网络传输，并被序列化后记录在以太坊区块链上，<strong>交易是唯一可以触发状态更改或导致合约在EVM中执行的事物</strong></p>
<h3 id="2-3-1、交易的数据结构"><a href="#2-3-1、交易的数据结构" class="headerlink" title="2.3.1、交易的数据结构"></a>2.3.1、交易的数据结构</h3><p>以太坊的数据结构主要可以分为四部分：<code>nonce</code>、<code>gas</code>、交易目标和消息（主要部分）、交易签名</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\transaction-struct.png">
</center>



<p>开头是一个 uint64 类型的数字，称之为随机数。用于撤销交易、防止双花和修改以太坊账户的 Nonce 值。</p>
<p>第二部分是关于交易执行限制的设置，gas 为愿意供以太坊虚拟机运行的燃料上限。 <code>gasPrice</code> 是愿意支付的燃料单价。<code>gasPrcie * gas</code> 则为愿意为这笔交易支付的最高手续费。</p>
<p>第三部分是交易发送者输入以太坊虚拟机执行此交易的初始信息： 虚拟机操作对象（接收方 To）、从交易发送方转移到操作对象的资产（Value），以及虚拟机运行时入参(input)。其中 To 为空时，意味着虚拟机无可操作对象，<strong>此时虚拟机将利用 input 内容部署一个新合约</strong>。</p>
<p>第四部分是交易发送方对交易的签名结果，可以利用交易内容和签名结果反向推导出签名者，即交易发送方地址。以上总结如下：</p>
<ul>
<li><code>nonce</code>：由发起人EOA发出的序列号，用于防止交易消息重播。</li>
<li><code>gas price</code>：交易发起人愿意支付的gas单价（wei）。</li>
<li><code>start gas</code>：交易发起人愿意支付的最大gas量。</li>
<li><code>to</code>：目的以太坊地址。</li>
<li><code>value</code>：要发送到目的地的以太数量。</li>
<li><code>data</code>：可变长度二进制数据负载（payload）。</li>
<li><code>v,r,s</code>：发起人EOA的ECDSA签名的三个组成部分。</li>
<li>交易消息的结构使用递归长度前缀（RLP）编码方案进行序列化，该方案专为在以太坊中准确和字节完美的数据序列化而创建。</li>
</ul>
<h3 id="2-3-2、交易中的nonce"><a href="#2-3-2、交易中的nonce" class="headerlink" title="2.3.2、交易中的nonce"></a>2.3.2、交易中的<code>nonce</code></h3><p>按以太坊黄皮书的定义， <code>nonce</code>是一个标量值，它等于从这个地址发送的交易数，或者对于关联code的帐户来说，是这个帐户创建合约的数量。因此<code>nonce</code>便有以下特征：</p>
<ul>
<li><code>nonce</code>不会明确存储为区块链中帐户状态的一部分。相反，它是通过计算发送地址的已确认交易的数量来动态计算的。</li>
<li><code>nonce</code>值还用于防止错误计算账户余额。<code>nonce</code>强制来自任何地址的交易按顺序处理，没有间隔，无论节点接收它们的顺序如何。</li>
<li>使用<code>nonce</code>确保所有节点计算相同的余额和正确的序列交易，等同于用于防止比特币“双重支付”（“重放攻击”）的机制。但是，由于以太坊跟踪账户余额并且不单独跟踪 <code>UTXO</code> ，因此只有在错误地计算账户余额时才会发生“双重支付”。<code>nonce</code>机制可以防止这种情况发生。</li>
</ul>
<h3 id="2-3-3、并发和nonce"><a href="#2-3-3、并发和nonce" class="headerlink" title="2.3.3、并发和nonce"></a>2.3.3、并发和<code>nonce</code></h3><p>以太坊是一个允许操作（节点，客户端，DApps）并发的系统，但强制执行单例状态。例如，出块的时候只有一个系统状态。假如我们有多个独立的钱包应用或客户端，比如 MetaMask 和 Geth，它们可以使用相同的地址生成交易。如果我们希望它们都够同时发送交易，该怎么设置交易的<code>nonce</code>呢？一般有以下两种做法：</p>
<ul>
<li>用一台服务器为各个应用分配<code>nonce</code>，先来先服务——可能出现单点故障，并且失败的交易会将后续交易阻塞。</li>
<li>生成交易后不分配<code>nonce</code>，也不签名，而是把它放入一个队列等待。另起一个节点跟踪<code>nonce</code>并签名交易。同样会有单点故障的可能，而且跟踪<code>nonce</code>和签名的节点是无法实现真正并发的。</li>
</ul>
<h3 id="2-3-4、交易中的gas"><a href="#2-3-4、交易中的gas" class="headerlink" title="2.3.4、交易中的gas"></a>2.3.4、交易中的<code>gas</code></h3><p>Gas 中译是：瓦斯、汽油，代表一种可燃气体。 这形象地比喻以太坊的交易手续费计算模式，不同于比特币中<strong>直接</strong>支付比特币作为转账手续费， 以太坊视为一个去中心化的计算网络，当你发送Token、执行合约、转移以太币或者在此区块上干其他的时候，计算机在处理这笔交易时需要进行计算消耗网络资源，这样你必须支付燃油费购买燃料才能让计算机为你工作。最终燃料费作为手续费支付给矿工。</p>
<blockquote>
<p>注：可以在Etherscan上查询gas price与confirmation time的关系，如下图</p>
</blockquote>
<center class="half">
    <img src="\Pic\Blockchain_Pic\gas.jpg">
</center>


<p>因为手续费等于<code>gasPrice * gasUsed</code>，用户在转账，特别是执行智能合约时 gasUsed 无法提前预知。 这样存在一个风险，当用户的交易涉及一个恶意的智能合约，该合约执行将消耗无限的燃料， 这样会导致交易方的余额全部消耗（恶意的智能合约有可能是程序Bug，如合约执行陷入一个死循环）。</p>
<p>为了避免合约中的错误引起不可预计的燃料消耗，用户需要在发送交易时设定允许消耗的燃料上限，即 gasLimit。 这样不管合约是否良好，最坏情况也只是消耗 gasLimit 量的燃料。</p>
<p>然而，一笔交易所必须支付的燃料已经在区块中通过该交易已执行的计算量记录。 如果你不想支出太多燃料，而故意设置过低的 gasLimit 是没太多帮助的。 你必须支付足够燃料来支付本交易所必要的计算资源。如果交易尚未执行完成，而燃料已用完， 将出现一个 <code>Out of Gas</code> 的错误。特别注意的是，即使交易失败，你也必须为已占用的计算资源所支付手续费。 比如，你通过合约给 TFBOYS 投票，设置 gasPrice=2 gwei，gasLimit=40000（实现投票需要40001的燃料开销）， 最终你投票失败且仍然需要支付 40000*2 gwei= 80000 gwei= 0.00008 ETH。</p>
<p>另外，如果最终 gasUsed 低于 gasLimit，即燃料未用完。则剩余燃料(gasLimit - gasUsed )将在交易后退还给你。 比如你发送 1 Ether 到另一个账户B，设置 gas limit 为 400000，将有 400000 - 21000 返回给你。</p>
<blockquote>
<p>注意：21000 是标准转账交易的gasUsed。因此一笔标准的转账交易你可以设置 gasLimit 为21000</p>
</blockquote>
<h2 id="2-4、以太坊账户"><a href="#2-4、以太坊账户" class="headerlink" title="2.4、以太坊账户"></a>2.4、以太坊账户</h2><p>对比比特币的UTXO余额模型，以太坊使用“账户”余额模型。 以太坊丰富了账户内容，除余额外还能自定义存放任意多数据。 并利用账户数据的可维护性，构建智能合约账户。下面我们首先将比特币的UTXO余额模型与以太坊账户进行比较，说明其各自的优缺点以及适用性。</p>
<h3 id="2-4-1、比特币UTXO和以太坊账户结构比较"><a href="#2-4-1、比特币UTXO和以太坊账户结构比较" class="headerlink" title="2.4.1、比特币UTXO和以太坊账户结构比较"></a>2.4.1、比特币UTXO和以太坊账户结构比较</h3><p>在当前的区块链项目中，主要有两种记录保存方式，<strong>一种是账户/余额模型，一种是UTXO模型</strong>。比特币采用就是UTXO模型，以太坊、EOS等则采用的是账户/余额模型。</p>
<p><img src="\Pic/utxo_com.jpg" style="zoom:67%;"></p>
<h3 id="2-4-2、比特币UTXO"><a href="#2-4-2、比特币UTXO" class="headerlink" title="2.4.2、比特币UTXO"></a>2.4.2、比特币UTXO</h3><p>UTXO是 Unspent Transaction Output的缩写，意思是<strong>未花费的输出，</strong>可以简单理解为还没有用掉的收款。比如韩梅梅收到一笔比特币，她没有用掉，这笔比特币对她来说就是一个UTXO。关于UTXO的具体介绍大家可以查看<a href="https://zhuanlan.zhihu.com/p/74050135" target="_blank" rel="noopener">这篇文章</a>。</p>
<p><strong>UTXO 核心设计思路是：它记录交易事件，而不记录最终状态。</strong>要计算某个用户有多少比特币，就要对其钱包里所有的UTXO求和，得到结果就是他的持币数量。UTXO模型在转账交易时，是以UTXO为单位的，也就是说在支付时，调用的是整数倍UTXO，比如1个UTXO，3个UTXO，没有0.5个UTXO的说法。</p>
<ul>
<li>比特币在基于UTXO的结构中存储有关用户余额的数据，系统的整个状态就是一组UTXO的集合，每个UTXO都有一个所有者和一个面值（就像不同的硬币），而交易会花费若干个输入的UTXO，并根据规则创建若干个新的UTXO</li>
<li>每个引用的输入必须有效并且尚未花费，对于一个交易，必须包含有每个输入的所有者匹配的签名，总输入必须大于等于总输出值。所以系统中用户的余额是用户具有私钥的UTXO的总值</li>
</ul>
<h3 id="2-4-3、以太坊账户"><a href="#2-4-3、以太坊账户" class="headerlink" title="2.4.3、以太坊账户"></a>2.4.3、以太坊账户</h3><p>为什么以太坊不用UTXO呢？显然是因为麻烦，以太坊的做法更符合直觉，以太坊中的状态就是系统中所有账户的列表，每个账户都包含了一个余额和以太坊<strong>特殊定义的数据</strong>（代码和内部存储）。如果发送账户有足够多的余额来进行支付，则交易有效，在这种情况下发送账户先扣款，而收款账户将记入这笔收入。<strong>如果接受账户有相关代码，则代码会自动运行，并且它的内部存储也可能被更改，或者代码还可能向其他账户发送额外的消息，这就会导致进一步的借贷资金关系。</strong></p>
<h3 id="2-4-4、优缺点比较"><a href="#2-4-4、优缺点比较" class="headerlink" title="2.4.4、优缺点比较"></a>2.4.4、优缺点比较</h3><p><strong>比特币UTXO的优点</strong>：</p>
<ul>
<li>更高程度的隐私：如果用户为他们收到的每笔交易使用新地址，那么通常很难将账户互相链接。这很大程度上适用于货币，但不太适用于任何dapps，因为dapps通常涉及跟踪和用户绑定的复杂状态，可能不存在像货币那样简单的用户状态划分方案</li>
<li>潜在的可扩展性：UTXO在理论上更符合可扩展性要求，因为我们只需要依赖拥有UTXO的那些人去维护基于Merkle树的所有权证明就够了，即使包括所有者在内的每个人都决定忘记该数据，那么也只有所有者受到对应的UTXO的损失，不影响接下来的交易。而在账户模式中，如果每个人都丢失了与账户相对应的Merkle树的部分，那将会使得和该账户有关的消息完全无法处理，包括发币给它。</li>
</ul>
<p><strong>以太坊账户模式的优点</strong>：</p>
<ul>
<li>可以节省大量空间：不将UTXOs分开存储，而是合成一个账户；每个交易只需要一个输入、一个签名并产生一个输出</li>
<li>更好的可替代性：货币本质上都是同质化、可替代的；UTXO的设计使得货币从来源分成了“可花费”和“不可花费”两类，这在实际应用中很难有对应模型</li>
<li>更加简单：更容易编码和理解，特别是设计复杂脚本的时候，UTXO的脚本逻辑复杂时更令人费解</li>
<li>便于维护持久轻节点：只要沿着特定方向扫描状态树，轻节点 可以很容易地随时访问账户相关的所有数据。而UTXO地每个交易都会使得状态引用发生改变，这对应节点来说长时间运行Dapp会有很大压力</li>
</ul>
<h3 id="2-4-5、总结"><a href="#2-4-5、总结" class="headerlink" title="2.4.5、总结"></a>2.4.5、总结</h3><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>BitCoin</th>
<th>Ethereum</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>设计定位</strong></td>
<td>现金系统</td>
<td>去中心化应用平台</td>
</tr>
<tr>
<td><strong>数据组成</strong></td>
<td>交易列表（账本）</td>
<td>交易和账户状态</td>
</tr>
<tr>
<td><strong>交易对象</strong></td>
<td>UTXO</td>
<td>Accounts</td>
</tr>
<tr>
<td><strong>代码控制</strong></td>
<td>脚本</td>
<td>智能合约</td>
</tr>
</tbody>
</table>
</div>
<h2 id="2-5、以太坊账户类型"><a href="#2-5、以太坊账户类型" class="headerlink" title="2.5、以太坊账户类型"></a>2.5、以太坊账户类型</h2><p>以太坊作为智能合约操作平台，将账户划分为两类：外部账户（EOAs）和合约账户（contract account），下面分别做简要介绍：</p>
<center class="half">
    <img src="\Pic\Blockchain_Pic\EOA_CA.png">
</center>



<h3 id="2-5-1、外部账户（EOA）"><a href="#2-5-1、外部账户（EOA）" class="headerlink" title="2.5.1、外部账户（EOA）"></a>2.5.1、外部账户（EOA）</h3><p>外部账户是由人来控制的，也就是常规理解的普通账户，外部账户包含以太币余额，主要作用就是发送交易（是广义的交易，包括转币和触发合约代码），是由用户私钥控制的，没有关联代码，所有在以太坊上交易的发起者都是外部账户。</p>
<p>外部账户特点总结：</p>
<ol>
<li>拥有以太余额。</li>
<li>能发送交易，包括转账和执行合约代码。</li>
<li>被私钥控制。</li>
<li>没有相关的可执行代码。</li>
</ol>
<h3 id="2-5-2、合约账户（CA）"><a href="#2-5-2、合约账户（CA）" class="headerlink" title="2.5.2、合约账户（CA）"></a>2.5.2、合约账户（CA）</h3><p>合约账户有时也叫内部账户，有对应的以太币余额和关联代码，它是由代码控制的，可以通过交易或来自其他合约的调用消息来触发代码执行，执行代码时可以操作自己的存储空间，也可以调用其他合约</p>
<p>合约账户特点总结：</p>
<ol>
<li>拥有以太余额。</li>
<li>有相关的可执行代码（合约代码）。</li>
<li>合约代码能够被交易或者其他合约消息调用。</li>
<li>合约代码被执行时可再调用其他合约代码。</li>
<li>合约代码被执行时可执行复杂运算，可永久地改变合约内部的数据存储。</li>
</ol>
<p>如果大家对概念还理解不深可以先尝试学习后面部分，本教程内容有限，推荐大家有精力阅读以下读物：</p>
<ul>
<li><a href="https://www.zhihu.com/question/61156867" target="_blank" rel="noopener">区块链学习的书籍</a></li>
<li><a href="https://www.ruanyifeng.com/blog/2017/12/blockchain-tutorial.html" target="_blank" rel="noopener">区块链入门教程</a></li>
<li><a href="https://developer.ibm.com/zh/technologies/blockchain/tutorials/" target="_blank" rel="noopener">IBM教程</a></li>
</ul>
<p><strong>参考自：</strong></p>
<ol>
<li>[比特币白皮书]<a href="https://www.8btc.com/wiki/bitcoin-a-peer-to-peer-electronic-cash-system" target="_blank" rel="noopener">https://www.8btc.com/wiki/bitcoin-a-peer-to-peer-electronic-cash-system</a>)</li>
<li><a href="https://ethfans.org/posts/ethereum-whitepaper" target="_blank" rel="noopener">以太坊白皮书</a></li>
<li><a href="https://www.chainnode.com/doc/399" target="_blank" rel="noopener">超级账本白皮书</a></li>
<li><a href="https://www.chainnode.com/doc/399" target="_blank" rel="noopener">闪电网络白皮书</a></li>
</ol>
]]></content>
      <categories>
        <category>Blockchain</category>
      </categories>
  </entry>
  <entry>
    <title>区块链入门（一）Linux基础</title>
    <url>/posts/cf0a3f0.html</url>
    <content><![CDATA[<h1 id="新手建议"><a href="#新手建议" class="headerlink" title="新手建议"></a>新手建议</h1><h2 id="学习Linux的注意事项"><a href="#学习Linux的注意事项" class="headerlink" title="学习Linux的注意事项"></a>学习Linux的注意事项</h2><ul>
<li><p>Linux严格区分大小写（命令全都是小写）—— 命令、文件名、选项等均区分大小写</p>
</li>
<li><p>Linux中<strong>所有内容</strong>以文件形式保存，包括硬件</p>
<ul>
<li>硬盘文件是/dev/sd[a-p]</li>
<li>光盘文件是/dev/sr0等</li>
</ul>
</li>
<li><p>Windows通过扩展名区分文件类型，还有图标可以区分；Linux不靠扩展名区分文件类型，靠文件权限区分，但也有一些约定俗成的扩展名：</p>
<ul>
<li>压缩包：”<em>.gz”, “</em>.bz2”, “<em>.tar.bz2”, “</em>.tgz”等</li>
<li>二进制软件包：”.rpm”</li>
<li>网页文件：”*.sh”</li>
<li>配置文件：”*.conf”</li>
</ul>
<p>注意：这些扩展名不是必要的，即时不加扩展名也没有影响，只是便于管理而已</p>
</li>
<li><p>Linux所有存储设备都必须挂在之后用户才能使用，包括硬盘、U盘、光盘（将设备与挂载点连接的过程就是挂载）</p>
</li>
<li><p>Windows下的程序不能直接在Linux中安装和运行</p>
</li>
</ul>
<h2 id="服务器管理和维护建议"><a href="#服务器管理和维护建议" class="headerlink" title="服务器管理和维护建议"></a>服务器管理和维护建议</h2><h3 id="服务器管理"><a href="#服务器管理" class="headerlink" title="服务器管理"></a>服务器管理</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">目录名</th>
<th style="text-align:center">目录作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">/bin/</td>
<td style="text-align:center">存放系统命令的目录，普通用户和超级用户都可以执行，不过放在/bin下的命令在单用户模式下也可以执行</td>
</tr>
<tr>
<td style="text-align:center">/sbin/</td>
<td style="text-align:center">保存和系统环境设置相关的命令，只有超级用户可以使用这些命令进行系统环境设置，但是有些命令可以允许普通用户查看</td>
</tr>
<tr>
<td style="text-align:center">/usr/bin/</td>
<td style="text-align:center">存放系统命令的目录，普通用户和超级用户都可以执行，这些命令和系统启动无关，在单用户模式下不能执行</td>
</tr>
<tr>
<td style="text-align:center">/usr/sbin/</td>
<td style="text-align:center">存放根文件系统不必要的系统管理命令，例如多数服务程序。只有超级用户可以使用</td>
</tr>
<tr>
<td style="text-align:center">/boot/</td>
<td style="text-align:center">系统启动目录，保存系统启动相关的文件，如内核文件和启动引导程序（grub）文件等</td>
</tr>
<tr>
<td style="text-align:center">/dev/</td>
<td style="text-align:center">设备文件保存位置，我们已经说过Linux中所有内容以文件形式保存，包括硬件，这个目录就是用来 保存所有硬件设备的</td>
</tr>
<tr>
<td style="text-align:center">/etc/</td>
<td style="text-align:center">配置文件保存位置，系统内所有采用默认安装方式（npm安装）的服务的配置文件全部保存在这个目录中，如用户账户和密码，服务的启动脚本，常用服务的配置文件等</td>
</tr>
<tr>
<td style="text-align:center">/home/</td>
<td style="text-align:center">每个用户的默认登陆位置，普通用户的home目录就是在/home下建立一个和用户名相同的目录</td>
</tr>
<tr>
<td style="text-align:center">/lib/</td>
<td style="text-align:center">系统调用的函数库保存位置</td>
</tr>
<tr>
<td style="text-align:center">/lost+found/</td>
<td style="text-align:center">当系统意外崩溃或机器意外关机时，产生的一些文件碎片放在这里，当系统启动的过程中fsck工具会对其进行检查，并修复已经损坏的文件系统。这个目录只在每个分区中出现，例如/lost+found就是根分区的备份恢复目录，/boot/lost+found就是/boot分区的备份恢复目录</td>
</tr>
<tr>
<td style="text-align:center">/media/</td>
<td style="text-align:center">挂载目录，系统建议是用来挂载媒体设备的，例如软盘和光盘</td>
</tr>
<tr>
<td style="text-align:center">/mnt/</td>
<td style="text-align:center">挂载目录，建议挂载额外设备，如U盘，移动硬盘和其他操作系统的分区</td>
</tr>
<tr>
<td style="text-align:center">/misc/</td>
<td style="text-align:center">挂载目录，系统建议用来挂载NFS服务的共享目录</td>
</tr>
<tr>
<td style="text-align:center">/opt/</td>
<td style="text-align:center">第三方安装的软件保存位置，但现在更多的是保存在/usr/local中</td>
</tr>
<tr>
<td style="text-align:center">/proc/</td>
<td style="text-align:center">虚拟文件系统，该目录的数据不保存到硬盘中，而是保存到内存中。主要保存系统的内核、进程、外部设备状态和网络状态灯，如/proc/cpuinfo是保存CPU信息的，/proc/devices是保存设备驱动的列表的，/proc/filesystems是保存 文件系统列表的，/proc/net/是保存网络协议信息的</td>
</tr>
<tr>
<td style="text-align:center">/sys/</td>
<td style="text-align:center">虚拟文件系统，主要保存内核相关信息</td>
</tr>
<tr>
<td style="text-align:center">/root/</td>
<td style="text-align:center">超级用户的家目录</td>
</tr>
<tr>
<td style="text-align:center">/srv/</td>
<td style="text-align:center">服务数据目录， 一些系统服务启动后可以在这个目录保存需要的数据</td>
</tr>
<tr>
<td style="text-align:center">/tmp/</td>
<td style="text-align:center">临时目录，系统存放临时文件的目录，该目录下所有用户都可以访问和写入，我们建议此目录不能保存重要数据，最好每次开机都把该目录清空</td>
</tr>
<tr>
<td style="text-align:center">/usr/</td>
<td style="text-align:center">系统软件资源目录，注意usr不是user的缩写，而是”Unix Software Resource”的缩写，所以不是存放用户数据，而是存放系统软件资源的目录。系统中安装的软件大多数都在这里</td>
</tr>
<tr>
<td style="text-align:center">/var/</td>
<td style="text-align:center">动态数据保存位置，主要保存缓存、日志以及软件运行所产生的文件</td>
</tr>
</tbody>
</table>
</div>
<h3 id="服务器注意事项"><a href="#服务器注意事项" class="headerlink" title="服务器注意事项"></a>服务器注意事项</h3><ol>
<li>远程服务器不允许关机，只能重启</li>
<li>重启时应该关闭服务</li>
<li>不要在服务器的访问高峰运行高负载命令</li>
<li>远程配置防火墙时不要把自己踢出服务器（可以设置每五分钟将防火墙规则重置一次，配置完之后再取消该设置）</li>
<li>指定合理的密码规范并定期更新</li>
<li>合理分配权限</li>
<li>定期备份重要数据和日志</li>
</ol>
<p>磁盘分区是用分区编辑器在磁盘上划分几个逻辑部分，碟片一旦划分成数个分区，不同类的目录和文件 可以存储进不同的分区。</p>
<h1 id="系统分区"><a href="#系统分区" class="headerlink" title="系统分区"></a>系统分区</h1><h2 id="分区类型"><a href="#分区类型" class="headerlink" title="分区类型"></a>分区类型</h2><ul>
<li>主分区：最多只能有4个</li>
<li>扩展分区：<ul>
<li>最多只能有1个</li>
<li>主分区加扩展分区最多有4个</li>
<li>不能写入数据，只能包含逻辑分区（这种限制是硬盘的限制）</li>
</ul>
</li>
<li>逻辑分区</li>
</ul>
<h2 id="格式化"><a href="#格式化" class="headerlink" title="格式化"></a>格式化</h2><p>硬盘经过正确分区后仍不能写入数据，我们的硬盘还必须经过格式化之后才能写入数据。格式化又称逻辑格式化，它是根据用户选定的文件系统（如FAT16、FAT32、NTFS、EXT 2、EXT3、EXT4等），在磁盘的特定区域写入特定数据，在分区中划分出一片用于存放文件分配表、目录表等用于文件管理的磁盘空间。格式化就是按照文件系统的规则将硬盘分成等大小的数据块，我们把数据块称为block。</p>
<blockquote>
<p>注：Windows可以识别的系统有FAT16、FAT32、NTFS；Linux可以识别的系统有EXT2、EXT3、EXT4</p>
</blockquote>
<h2 id="设备文件名"><a href="#设备文件名" class="headerlink" title="设备文件名"></a>设备文件名</h2><h4 id="硬盘设备文件名"><a href="#硬盘设备文件名" class="headerlink" title="硬盘设备文件名"></a>硬盘设备文件名</h4><p>Windows是直接分区——&gt;格式化——&gt;分配盘符即可使用，Linux需要分区——&gt;格式化——&gt;给分区建立设备文件名——&gt;分配盘符才能使用。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>硬件</th>
<th>设备文件名</th>
</tr>
</thead>
<tbody>
<tr>
<td>IDE硬盘</td>
<td>/dev/hd[a-d]</td>
</tr>
<tr>
<td>SCSI/SATA/USB硬盘</td>
<td>/dev/sd[a-p]</td>
</tr>
<tr>
<td>光驱</td>
<td>/dev/cdrom或dev/sr0</td>
</tr>
<tr>
<td>软盘</td>
<td>/dev/fd[0-1]</td>
</tr>
<tr>
<td>打印机（25针）</td>
<td>/dev/lp[0-2]</td>
</tr>
<tr>
<td>打印机（USB）</td>
<td>/dev/usb/lp[0-15]</td>
</tr>
<tr>
<td>鼠标</td>
<td>/dev/mouse</td>
</tr>
</tbody>
</table>
</div>
<h4 id="分区设备文件名"><a href="#分区设备文件名" class="headerlink" title="分区设备文件名"></a>分区设备文件名</h4><p>分区设备文件名直接<strong>在硬盘设备文件名后面加分区号</strong>即可，如</p>
<ul>
<li>IDE硬盘接口第一个分区：/dev/hda1（如今几乎看不到）</li>
<li>SCSI硬盘接口、SATA硬盘接口的第一个分区：/dev/sda1</li>
</ul>
<blockquote>
<p>IDE硬盘是最古老的硬盘，理论最高传输速度是133M/s</p>
<p>SCSI硬盘接口与IDE硬盘同时代，更加昂贵但速度更快，理论最高传输速度可达200M/s，但这种硬盘主要用在服务器上</p>
<p>但上两种硬盘接口如今已经基本淘汰，如今使用更多的是小口的SATA串口硬盘，SATA已发展到3代，其理论传输速度最高可达500M/s，目前不管是服务器还是个人机基本使用的都是SATA硬盘接口。</p>
</blockquote>
<p>需要留意的是，逻辑分区永远都是从5开始的</p>
<h2 id="挂载"><a href="#挂载" class="headerlink" title="挂载"></a>挂载</h2><p>挂载实际上就是Windows中分配盘符的过程，盘符则被相应地称为挂载点，必须分区的分区有以下两种：</p>
<ol>
<li>根分区：/</li>
<li>swap分区（交换分区）：可以理解为虚拟内存，当真正内存不够用时，可以使用这部分交换分区的硬盘空间来当内存，理论上来说交换分区应该是内存的两倍，但最大不超过2GB</li>
</ol>
<p>若无这两个分区，Linux不能正常使用，但我们还推荐把/boot单独分区，这是为了防止Linux系统启动不起来，一般200MB即可。</p>
<h1 id="远程登陆管理工具"><a href="#远程登陆管理工具" class="headerlink" title="远程登陆管理工具"></a>远程登陆管理工具</h1><h2 id="网络连接"><a href="#网络连接" class="headerlink" title="网络连接"></a>网络连接</h2><p>网络连接从虚拟机设置中可以看到，一共有三种：桥接、NAT和Host-only，下面讲解其区别：</p>
<ul>
<li>桥接：桥接意味着虚拟机如同一个单独的主机一样访问Wifi等，也可以和其他机器通信</li>
<li>NAT：虚拟机仅能和主机通信，但若主机可以访问互联网，虚拟机也可以访问互联网</li>
<li>Host-only：虚拟机仅能和主机本机通信，不能访问互联网</li>
</ul>
<h2 id="网络配置"><a href="#网络配置" class="headerlink" title="网络配置"></a>网络配置</h2><ol>
<li>首先调成Host-only模式，使得虚拟机仅与主机连接</li>
<li>在主机上找到VMware Network Adapter VMnet1的IP地址，我本地地址为192.168.19.1</li>
<li>在虚拟机上使用<code>ishw -c netwowrk</code>命令找到logical name，此即为虚拟机的网卡名称，我的虚拟网卡名称为ens33</li>
<li>使用命令ifconfig  [不等于IP地址]  logical name，例如我使用的是<code>ifconfig ens33 192.168.19.2</code></li>
<li>此时再ifconfig即可看到我们设置的已生效</li>
<li>我们可以在主机ping这个IP地址看到生效</li>
<li>使用secureCRT连接即可</li>
</ol>
<p>需要注意的是，以上方法配置IP地址时不是永久生效的，也就是重新启动电脑时就失效了，若想永久生效需要改变配置文件</p>
<p>若使用NAT模式，则步骤简单很多，只需要ifconfig获得IP地址之后直接用secureCRT连接即可</p>
<h2 id="WinSCP"><a href="#WinSCP" class="headerlink" title="WinSCP"></a>WinSCP</h2><p>另外推荐一个Windows主机与Linux虚拟机进行文件传输的工具——WinSCP，操作方法与上面类似，只需输入对应的IP地址即可连接。</p>
<h2 id="安装linux系统（以ubuntu为例）"><a href="#安装linux系统（以ubuntu为例）" class="headerlink" title="安装linux系统（以ubuntu为例）"></a>安装linux系统（以ubuntu为例）</h2><ul>
<li><p>使用vmware虚拟机安装</p>
<p><a href="https://zhuanlan.zhihu.com/p/38797088" target="_blank" rel="noopener">参考此博客：VMware安装Ubuntu18.04</a></p>
</li>
<li><p>使用win10子系统安装</p>
<p><a href="https://zhuanlan.zhihu.com/p/76032647" target="_blank" rel="noopener">参考此博客：在 win10 下使用 ubuntu 子系统</a></p>
</li>
</ul>
<h1 id="Linux常用命令"><a href="#Linux常用命令" class="headerlink" title="Linux常用命令"></a>Linux常用命令</h1><h2 id="一、最常用命令"><a href="#一、最常用命令" class="headerlink" title="一、最常用命令"></a>一、最常用命令</h2><p>这是我们<strong>使用得最多</strong>的命令了，<strong>Linux最基础的命令</strong>！</p>
<ul>
<li>可用 <code>pwd</code>命令查看用户的当前目录</li>
<li>可用 <code>cd</code> 命令来切换目录</li>
<li><code>.</code>表示当前目录</li>
<li><code>..</code> 表示当前目录的上一级目录（父目录）</li>
<li><code>-</code>表示用 cd 命令切换目录<strong>前</strong>所在的目录</li>
<li><code>~</code> 表示<strong>用户主目录</strong>的绝对路径名</li>
</ul>
<p><strong>绝对路径：</strong></p>
<ul>
<li>以斜线（/）开头 ，描述到文件位置的<strong>完整说明</strong> ，任何时候你想指定文件名的时候都可以使用</li>
</ul>
<p><strong>相对路径 ：</strong></p>
<ul>
<li>不以斜线（/）开头 ，指定<strong>相对于你的当前工作目录而言的位置</strong> ，可以被用作指定文件名的简捷方式</li>
</ul>
<h2 id="二、文件处理命令"><a href="#二、文件处理命令" class="headerlink" title="二、文件处理命令"></a>二、文件处理命令</h2><h3 id="1-命令格式与目录处理命令ls"><a href="#1-命令格式与目录处理命令ls" class="headerlink" title="1. 命令格式与目录处理命令ls"></a>1. 命令格式与目录处理命令<code>ls</code></h3><p><strong>命令格式</strong>：<code>命令[-选项][-参数]</code>，例：<code>ls -la /etc</code></p>
<p><strong>说明</strong>：</p>
<ol>
<li>个别命令使用不遵循此格式</li>
<li>当有多个选项时，可以写在一起</li>
<li>简化选项与完整选项：<code>-a</code> 等于 <code>--all</code></li>
</ol>
<p><code>ls</code>命令的语法：</p>
<ol>
<li><p><code>ls -a</code>可以显示所有文件，包括隐藏文件（以点.开头的文件是隐藏文件）</p>
</li>
<li><p>若希望查询的不是当前目录，可以使用<code>ls+其他目录</code>进行查询</p>
</li>
<li><p><code>ls -l</code>可以显示更多属性（long），属性阐述如下：</p>
</li>
<li><p>第一列分为三个部分，第一部分（如d告诉我们文件的类型是一个目录，-为二进制文件，1为软链接文件），drwx表示该文件支持读写和执行操作，r,w,x分别对应读、写、执行三个权限，三列分别对应所有者，所属组，其他人的权限</p>
</li>
<li><p>第二列的2、2、3等表示调用次数</p>
<ol>
<li>第三列表示所有者，也就是这个文件的总负责人（拥有文件的所有权，可转让）</li>
</ol>
</li>
<li>第四列表示所属组，也就是可以操作这个文件的人<ol>
<li>第五列表示文件大小，默认单位是字节（很反Windows）</li>
</ol>
</li>
<li><p>最后一个是文件的最后一次修改时间（Linux没有创建时间这个概念）</p>
</li>
<li><p><code>ls -lh</code>比原先的更人性化（humanitarian），它将对应的单位也显示了出来，<code>-h</code>实际上是一个通用选项，很多命令都可以加</p>
</li>
<li><p><code>-d</code>显示当前目录本身而不显示目录下的数据，一般与<code>-l</code>结合使用，如<code>ls -ld /etc</code></p>
</li>
<li><p><code>ls -id</code>可以查看当前目录对应的文件ID</p>
</li>
</ol>
<h3 id="2-目录处理命令"><a href="#2-目录处理命令" class="headerlink" title="2. 目录处理命令"></a>2. 目录处理命令</h3><h5 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir"></a><code>mkdir</code></h5><p><strong>语法</strong>：<code>mkdir -p [目录名]</code></p>
<p><strong>功能描述</strong>：创建新目录，<code>-p</code>递归创建（若一个目录本身不存在，可以在创建这个目录的同时创建子目录），也可以同时创建多个目录</p>
<h5 id="cd"><a href="#cd" class="headerlink" title="cd"></a><code>cd</code></h5><p><strong>语法</strong>：<code>cd directory</code></p>
<p><strong>功能描述</strong>：改变当前目录</p>
<h5 id="pwd"><a href="#pwd" class="headerlink" title="pwd"></a><code>pwd</code></h5><p><strong>语法</strong>：<code>pwd</code></p>
<p><strong>功能描述</strong>：显示当前目录（print working directory）</p>
<h5 id="rmdir"><a href="#rmdir" class="headerlink" title="rmdir"></a><code>rmdir</code></h5><p><strong>语法</strong>：<code>rmdir [目录名]</code></p>
<p><strong>功能描述</strong>：删除空目录（若目录非空则不能删除）</p>
<h5 id="cp"><a href="#cp" class="headerlink" title="cp"></a><code>cp</code></h5><p><strong>语法</strong>：<code>cp -rf [源文件或目录] [目标目录] -r 复制目录 -p 保留文件属性（文件创建时间等不发生变化）</code></p>
<p><strong>功能描述</strong>：复制文件或目录</p>
<h5 id="mv"><a href="#mv" class="headerlink" title="mv"></a><code>mv</code></h5><p><strong>语法</strong>：<code>mv [源文件或目录] [目标目录]</code></p>
<p><strong>功能描述</strong>：剪切文件、改名</p>
<h5 id="rm"><a href="#rm" class="headerlink" title="rm"></a><code>rm</code></h5><p><strong>语法</strong>：<code>rm -rf [文件或目录] -r 删除目录 -f 强制执行</code></p>
<p><strong>功能描述</strong>：删除文件</p>
<h3 id="3-文件处理命令"><a href="#3-文件处理命令" class="headerlink" title="3. 文件处理命令"></a>3. 文件处理命令</h3><h5 id="touch"><a href="#touch" class="headerlink" title="touch"></a><code>touch</code></h5><p><strong>语法</strong>：<code>touch [文件名]</code></p>
<p><strong>功能描述</strong>：创建空文件</p>
<h5 id="cat"><a href="#cat" class="headerlink" title="cat"></a><code>cat</code></h5><p><strong>语法</strong>：<code>cat [文件名]</code></p>
<p><strong>功能描述</strong>：显示文件内容  <code>-n</code>可显示行号</p>
<h5 id="tac"><a href="#tac" class="headerlink" title="tac"></a><code>tac</code></h5><p>与<code>cat</code>相反，可以倒着显示</p>
<h5 id="more"><a href="#more" class="headerlink" title="more"></a><code>more</code></h5><p><code>cat</code>命令显示的往往过多，若希望分页显示可以使用<code>more</code>，用法与<code>cat</code>相同，使用时按空格可以一页页往后翻，使用q或Q退出</p>
<h5 id="less"><a href="#less" class="headerlink" title="less"></a><code>less</code></h5><p>由于<code>more</code>无法向上翻，我们可以使用<code>less</code>命令，可以使用page up一页页往上翻，也可以使用上箭头一行行往上翻，其他操作与<code>more</code>相同。另外<code>less</code>还可以进行搜索，比如想要搜索关键词service，可以输入/service进行检索，页面会对这些关键词进行高亮，可以使用<code>n</code>找到其他关键词位置</p>
<h5 id="head"><a href="#head" class="headerlink" title="head"></a><code>head</code></h5><p>若只想要看文件的前几行，可以使用<code>head -n</code>加指定行数，若不加则默认显示前10行</p>
<h5 id="tail"><a href="#tail" class="headerlink" title="tail"></a><code>tail</code></h5><p>与<code>head</code>类似 ，但是显示后面几行。</p>
<p>常用搭配为：<code>tail -f</code>，该命令会动态显示文件末尾内容</p>
<h2 id="三、链接命令ln"><a href="#三、链接命令ln" class="headerlink" title="三、链接命令ln"></a>三、链接命令<code>ln</code></h2><p><strong>语法</strong>：<code>ln -s [原文件] [目标文件] -s 创建软链接</code></p>
<p><strong>功能描述</strong>：生成链接文件</p>
<p><strong>示例</strong>：</p>
<ul>
<li><code>ln -s /etc/issue issue.soft</code>：生成软链接</li>
<li><code>ln /etc/issue issue.hard</code>：生成硬链接</li>
</ul>
<p><strong>软链接和硬链接的区别</strong></p>
<p>我们使用<code>ls -l</code>查看这两个文件的信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-rw-r--r-- 2 root root 26 Jul 15  2020 issue.hard</span><br><span class="line">lrwxrwxrwx 1 root root 10 Jan 31 04:55 issue.soft -&gt; &#x2F;etc&#x2F;issue</span><br></pre></td></tr></table></figure>
<p>我们会发现这两个文件的信息相差的非常多，软链接文件开头的文件类型是<code>l(link)</code>，三个权限都是<code>rwx</code>，即可读可写可执行，软链接文件就类似于Windows的快捷方式，用处是便于做管理，我们可以看到最后有一个箭头指向<code>/etc/issue</code>。另外我们看到这个文件只有31个字节，因为它只是一个符号链接。我们可以总结得出软链接的三个特点：</p>
<ol>
<li>权限是<code>rwx</code></li>
<li>文件很小，只是符号链接</li>
<li>箭头指向源文件</li>
</ol>
<p>下面我们看硬链接的特点，我们首先分别查看 这两个文件的信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ls -l issue.hard</span><br><span class="line">ls -l &#x2F;etc&#x2F;issue</span><br></pre></td></tr></table></figure>
<p>我们可以看到这两个文件的所有信息一模一样，包括文件的大小，这类似于拷贝，似乎相当于<code>cp -p</code>，而硬链接和<code>cp -p</code>的最大不同就是硬链接可以实现同步更新，我们可以做一个简单的实验，我们先查看硬链接文件，然后往源文件中写入文件，可以发现硬链接文件也被同时修改了，当然软链接也会同步修改。</p>
<p>但当我们将源文件复制到另一个位置并删除原位置文件之后，再试图打开软链接会提示“没有那个文件或目录”，而且再显示这个目录软链接会标红并一直闪，而硬链接可以正常访问，没有影响，这就是硬链接和软连接的不同之处。</p>
<p>实际上我们可以通过命令<code>ls -i</code>来识别其<code>i</code>节点以辨别出是硬链接还是软链接，硬链接和源文件的<code>i</code>节点相同，软链接则不同。</p>
<p>硬链接相当于一个同步文件，但可以做实时备份（一个文件删了不会影响另一个文件），硬链接有两个限制，这也是硬链接和软链接的区别：</p>
<ol>
<li>不能跨分区</li>
<li>不能针对目录使用</li>
</ol>
<h2 id="四、权限管理命令"><a href="#四、权限管理命令" class="headerlink" title="四、权限管理命令"></a>四、权限管理命令</h2><p>Linux用户一共分成三类，分别是所有者（U），所属组（G）和其他人（O），权限也分成三类，分别是<code>r</code>，<code>w</code>，<code>x</code>，对应读、写、执行，我们首先学习如何更改权限。</p>
<h4 id="chmod"><a href="#chmod" class="headerlink" title="chmod"></a><code>chmod</code></h4><p>更改文件的人只能是文件所有者或者管理员root用户，更改文件权限有两种方式，第一种方式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod [&#123;ugoa&#125;&#123;+-&#x3D;&#125;&#123;rwx&#125;][文件或目录]</span><br></pre></td></tr></table></figure>
<p>其中第一个花括号里<code>u</code>，<code>g</code>，<code>o</code>，<code>a</code>分别表示所有者，所属组，其他人和所有人，第二个花括号<code>+</code>和<code>-</code>分别表示增加和减少权限，<code>=</code>表示成为后面的权限。第二种方式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod [mod&#x3D;421][文件或目录] -R 递归修改</span><br></pre></td></tr></table></figure>
<p>数字的意思只是将三个权限位分别用数字来表示，比如<code>r</code>用4表示，<code>w</code>用2表示，<code>x</code>用1表示，则若要表示<code>rwxrw-r--</code>则记为<code>764</code></p>
<h4 id="chown"><a href="#chown" class="headerlink" title="chown"></a><code>chown</code></h4><p>命令英文原意是<code>change file ownership</code>，作用是改变文件或目录的所有者，改变文件file的所有者为user的具体用法为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chown user file</span><br></pre></td></tr></table></figure>
<p>要注意只有root和文件的所有者可以改变文件的权限</p>
<h4 id="chgrp"><a href="#chgrp" class="headerlink" title="chgrp"></a><code>chgrp</code></h4><p>命令英文原意是<code>change file  group ownership</code>，作用是改变文件或目录的所属组，若具体用法和前面<code>chown</code>相同。我们可以使用<code>groupadd</code>命令添加组（使用<code>useradd</code>命令添加用户）</p>
<h4 id="umask"><a href="#umask" class="headerlink" title="umask"></a><code>umask</code></h4><p>命令英文原意是<code>the user file-creation mask</code>，作用是显示、设置文件的缺省权限，语法是：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">umask [-S]</span><br></pre></td></tr></table></figure>
<p>其中<code>-S</code>的作用是显示新建文件的缺省权限，但需要注意的是缺省创建文件时不可以有可执行权限的，所以当<code>touch</code>创建文件时会发现所有权限都少了<code>x</code>。</p>
<p>当我们直接使用<code>umask</code>时，比如显示0022，第一个0是特殊权限，我们暂时不涉及，第二只第四位分别是所有者、所属组和其他人，我们的最终权限实际上是<code>777-022=755</code>，也就是<code>rwx r-x r-x</code>，当然这指的是目录，如果是文件由于没有可执行权限，文件权限应当是<code>rw- r-- r--</code>，当然缺省创建的权限可以更改，直接使用<code>umask 077</code>即可将文件缺省权限更改为<code>rwx --- ---</code>，但不推荐做这种更改</p>
]]></content>
      <categories>
        <category>Blockchain</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫（六）多进程基本原理</title>
    <url>/posts/2a78ce87.html</url>
    <content><![CDATA[<p>前面我们说到Python 中的多线程是不能很好发挥多核优势的，如果想要发挥多核优势，最好还是使用多进程。那么现在我们就来了解下多进程的基本概念和用 Python 实现多进程的方法。</p>
<h1 id="多进程的含义"><a href="#多进程的含义" class="headerlink" title="多进程的含义"></a>多进程的含义</h1><p>进程（Process）是具有一定独立功能的程序关于某个数据集合上的一次运行活动，是系统进行资源分配和调度的一个独立单位。</p>
<p>顾名思义，多进程就是启用多个进程同时运行。由于进程是线程的集合，而且进程是由一个或多个线程构成的，所以多进程的运行意味着有大于或等于进程数量的线程在运行。</p>
<h1 id="Python-多进程的优势"><a href="#Python-多进程的优势" class="headerlink" title="Python 多进程的优势"></a>Python 多进程的优势</h1><p>前面我们说到由于进程中 GIL 的存在，Python 中的多线程并不能很好地发挥多核优势，一个进程中的多个线程，在同一时刻只能有一个线程运行。</p>
<p>而对于多进程来说，每个进程都有属于自己的 GIL，所以，在多核处理器下，多进程的运行是不会受 GIL 的影响的。因此，多进程能更好地发挥多核的优势。</p>
<p>当然，对于爬虫这种 IO 密集型任务来说，多线程和多进程影响差别并不大。对于计算密集型任务来说，Python 的多进程相比多线程，其多核运行效率会有成倍的提升。</p>
<p>总的来说，Python 的多进程整体来看是比多线程更有优势的。所以，在条件允许的情况下，能用多进程就尽量用多进程。</p>
<p>不过值得注意的是，由于进程是系统进行资源分配和调度的一个独立单位，所以各个进程之间的数据是无法共享的，如多个进程无法共享一个全局变量，进程之间的数据共享需要有单独的机制来实现，这在后面也会讲到。</p>
<h1 id="多进程的实现"><a href="#多进程的实现" class="headerlink" title="多进程的实现"></a>多进程的实现</h1><p>在 Python 中也有内置的库来实现多进程，它就是 <code>multiprocessing</code>。</p>
<p><code>multiprocessing</code>提供了一系列的组件，如<code>Process</code>（进程）、<code>Queue</code>（队列）、<code>Semaphore</code>（信号量）、<code>Pipe</code>（管道）、Lock（锁）、Pool（进程池）等，接下来让我们来了解下它们的使用方法。</p>
<h2 id="直接使用Process类"><a href="#直接使用Process类" class="headerlink" title="直接使用Process类"></a>直接使用<code>Process</code>类</h2><p>在<code>multiprocessing</code>中，每一个进程都用一个<code>Process</code>类来表示。它的 API 调用如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Process([group [, target [, name [, args [, kwargs]]]]])</span><br></pre></td></tr></table></figure>
<ul>
<li><code>target</code> 表示调用对象，你可以传入方法的名字。</li>
<li><code>args</code> 表示被调用对象的位置参数元组，比如 <code>target</code> 是函数 <code>func</code>，他有两个参数 <code>m</code>，<code>n</code>，那么 <code>args</code> 就传入 <code>[m, n]</code> 即可。</li>
<li><code>kwargs</code> 表示调用对象的字典。</li>
<li><code>name</code> 是别名，相当于给这个进程取一个名字。</li>
<li><code>group</code> 分组。</li>
</ul>
<p>我们先用一个实例来感受一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(index)</span>:</span></span><br><span class="line">    print(<span class="string">f'Process: <span class="subst">&#123;index&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        p = multiprocessing.Process(target=process, args=(i,))</span><br><span class="line">        p.start()</span><br></pre></td></tr></table></figure>
<p>这是一个实现多进程最基础的方式：通过创建 Process 来新建一个子进程，其中 target 参数传入方法名，args 是方法的参数，是以元组的形式传入，其和被调用的方法 process 的参数是一一对应的。</p>
<blockquote>
<p>注意：这里 args 必须要是一个元组，如果只有一个参数，那也要在元组第一个元素后面加一个逗号，如果没有逗号则和单个元素本身没有区别，无法构成元组，导致参数传递出现问题。</p>
</blockquote>
<p>创建完进程之后，我们通过调用 start 方法即可启动进程了。运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Process: 0</span><br><span class="line">Process: 1</span><br><span class="line">Process: 2</span><br><span class="line">Process: 3</span><br><span class="line">Process: 4</span><br></pre></td></tr></table></figure>
<p>可以看到，我们运行了 5 个子进程，每个进程都调用了 process 方法。process 方法的 index 参数通过 Process 的 args 传入，分别是 0~4 这 5 个序号，最后打印出来，5 个子进程运行结束。</p>
<p>由于进程是 Python 中最小的资源分配单元，因此这些进程和线程不同，各个进程之间的数据是不会共享的，每启动一个进程，都会独立分配资源。</p>
<p>另外，在当前 CPU 核数足够的情况下，这些不同的进程会分配给不同的 CPU 核来运行，实现真正的并行执行。</p>
<p>multiprocessing 还提供了几个比较有用的方法，如我们可以通过 cpu_count 的方法来获取当前机器 CPU 的核心数量，通过 active_children 方法获取当前还在运行的所有进程。</p>
<p>下面通过一个实例来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(index)</span>:</span></span><br><span class="line">    time.sleep(index)</span><br><span class="line">    print(<span class="string">f'Process: <span class="subst">&#123;index&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        p = multiprocessing.Process(target=process, args=[i])</span><br><span class="line">        p.start()</span><br><span class="line">    print(<span class="string">f'CPU number: <span class="subst">&#123;multiprocessing.cpu_count()&#125;</span>'</span>)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> multiprocessing.active_children():</span><br><span class="line">        print(<span class="string">f'Child process name: <span class="subst">&#123;p.name&#125;</span> id: <span class="subst">&#123;p.pid&#125;</span>'</span>)</span><br><span class="line">    print(<span class="string">'Process Ended'</span>)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Process: 0</span><br><span class="line">CPU number: 8</span><br><span class="line">Child process name: Process-5 id: 73595</span><br><span class="line">Child process name: Process-2 id: 73592</span><br><span class="line">Child process name: Process-3 id: 73593</span><br><span class="line">Child process name: Process-4 id: 73594</span><br><span class="line">Process Ended</span><br><span class="line">Process: 1</span><br><span class="line">Process: 2</span><br><span class="line">Process: 3</span><br><span class="line">Process: 4</span><br></pre></td></tr></table></figure>
<p>在上面的例子中我们通过 cpu_count 成功获取了 CPU 核心的数量：8 个，当然不同的机器结果可能不同。</p>
<p>另外我们还通过 active_children 获取到了当前正在活跃运行的进程列表。然后我们遍历了每个进程，并将它们的名称和进程号打印出来了，这里进程号直接使用 pid 属性即可获取，进程名称直接通过 name 属性即可获取。</p>
<p>以上我们就完成了多进程的创建和一些基本信息的获取。</p>
<h2 id="继承-Process-类"><a href="#继承-Process-类" class="headerlink" title="继承 Process 类"></a>继承 Process 类</h2><p>在上面的例子中，我们创建进程是直接使用 Process 这个类来创建的，这是一种创建进程的方式。不过，创建进程的方式不止这一种，同样，我们也可以像线程 Thread 一样来通过继承的方式创建一个进程类，进程的基本操作我们在子类的 run 方法中实现即可。</p>
<p>通过一个实例来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyProcess</span><span class="params">(Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, loop)</span>:</span></span><br><span class="line">        Process.__init__(self)</span><br><span class="line">        self.loop = loop</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> count <span class="keyword">in</span> range(self.loop):</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            print(<span class="string">f'Pid: <span class="subst">&#123;self.pid&#125;</span> LoopCount: <span class="subst">&#123;count&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">5</span>):</span><br><span class="line">        p = MyProcess(i)</span><br><span class="line">        p.start()</span><br></pre></td></tr></table></figure>
<p>我们首先声明了一个构造方法，这个方法接收一个 loop 参数，代表循环次数，并将其设置为全局变量。在 run 方法中，又使用这个 loop 变量循环了 loop 次并打印了当前的进程号和循环次数。</p>
<p>在调用时，我们用 range 方法得到了 2、3、4 三个数字，并把它们分别初始化了 MyProcess 进程，然后调用 start 方法将进程启动起来。</p>
<blockquote>
<p>注意：这里进程的执行逻辑需要在 run 方法中实现，启动进程需要调用 start 方法，调用之后 run 方法便会执行。</p>
</blockquote>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Pid: 73667 LoopCount: 0</span><br><span class="line">Pid: 73668 LoopCount: 0</span><br><span class="line">Pid: 73669 LoopCount: 0</span><br><span class="line">Pid: 73667 LoopCount: 1</span><br><span class="line">Pid: 73668 LoopCount: 1</span><br><span class="line">Pid: 73669 LoopCount: 1</span><br><span class="line">Pid: 73668 LoopCount: 2</span><br><span class="line">Pid: 73669 LoopCount: 2</span><br><span class="line">Pid: 73669 LoopCount: 3</span><br></pre></td></tr></table></figure>
<p>可以看到，三个进程分别打印出了 2、3、4 条结果，即进程 73667 打印了 2 次 结果，进程 73668 打印了 3 次结果，进程 73669 打印了 4 次结果。</p>
<blockquote>
<p>注意，这里的进程 pid 代表进程号，不同机器、不同时刻运行结果可能不同。</p>
</blockquote>
<p>通过上面的方式，我们也非常方便地实现了一个进程的定义。为了复用方便，我们可以把一些方法写在每个进程类里封装好，在使用时直接初始化一个进程类运行即可。</p>
<h2 id="守护进程"><a href="#守护进程" class="headerlink" title="守护进程"></a>守护进程</h2><p>在多进程中，同样存在守护进程的概念，如果一个进程被设置为守护进程，当父进程结束后，子进程会自动被终止，我们可以通过设置 daemon 属性来控制是否为守护进程。</p>
<p>还是原来的例子，增加了 deamon 属性的设置：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyProcess</span><span class="params">(Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, loop)</span>:</span></span><br><span class="line">        Process.__init__(self)</span><br><span class="line">        self.loop = loop</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> count <span class="keyword">in</span> range(self.loop):</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            print(<span class="string">f'Pid: <span class="subst">&#123;self.pid&#125;</span> LoopCount: <span class="subst">&#123;count&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">5</span>):</span><br><span class="line">        p = MyProcess(i)</span><br><span class="line">        p.daemon = <span class="literal">True</span></span><br><span class="line">        p.start()</span><br><span class="line">print(<span class="string">'Main Process ended'</span>)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Main Process ended</span><br></pre></td></tr></table></figure>
<p>结果很简单，因为主进程没有做任何事情，直接输出一句话结束，所以在这时也直接终止了子进程的运行。</p>
<p>这样可以有效防止无控制地生成子进程。这样的写法可以让我们在主进程运行结束后无需额外担心子进程是否关闭，避免了独立子进程的运行。</p>
<h2 id="进程等待"><a href="#进程等待" class="headerlink" title="进程等待"></a>进程等待</h2><p>上面的运行效果其实不太符合我们预期：主进程运行结束时，子进程（守护进程）也都退出了，子进程什么都没来得及执行。</p>
<p>能不能让所有子进程都执行完了然后再结束呢？当然是可以的，只需要加入 join 方法即可，我们可以将代码改写如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">processes = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">5</span>):</span><br><span class="line">    p = MyProcess(i)</span><br><span class="line">    processes.append(p)</span><br><span class="line">    p.daemon = <span class="literal">True</span></span><br><span class="line">    p.start()</span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> processes:</span><br><span class="line">    p.join()</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Pid: 40866 LoopCount: 0</span><br><span class="line">Pid: 40867 LoopCount: 0</span><br><span class="line">Pid: 40868 LoopCount: 0</span><br><span class="line">Pid: 40866 LoopCount: 1</span><br><span class="line">Pid: 40867 LoopCount: 1</span><br><span class="line">Pid: 40868 LoopCount: 1</span><br><span class="line">Pid: 40867 LoopCount: 2</span><br><span class="line">Pid: 40868 LoopCount: 2</span><br><span class="line">Pid: 40868 LoopCount: 3</span><br><span class="line">Main Process ended</span><br></pre></td></tr></table></figure>
<p>在调用 start 和 join 方法后，父进程就可以等待所有子进程都执行完毕后，再打印出结束的结果。</p>
<p>默认情况下，join 是无限期的。也就是说，如果有子进程没有运行完毕，主进程会一直等待。这种情况下，如果子进程出现问题陷入了死循环，主进程也会无限等待下去。怎么解决这个问题呢？可以给 join 方法传递一个超时参数，代表最长等待秒数。如果子进程没有在这个指定秒数之内完成，会被强制返回，主进程不再会等待。也就是说这个参数设置了主进程等待该子进程的最长时间。</p>
<p>例如这里我们传入 1，代表最长等待 1 秒，代码改写如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">processes = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>, <span class="number">5</span>):</span><br><span class="line">    p = MyProcess(i)</span><br><span class="line">    processes.append(p)</span><br><span class="line">    p.daemon = <span class="literal">True</span></span><br><span class="line">    p.start()</span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> processes:</span><br><span class="line">    p.join(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Pid: 40970 LoopCount: 0</span><br><span class="line">Pid: 40971 LoopCount: 0</span><br><span class="line">Pid: 40970 LoopCount: 1</span><br><span class="line">Pid: 40971 LoopCount: 1</span><br><span class="line">Main Process ended</span><br></pre></td></tr></table></figure>
<p>可以看到，有的子进程本来要运行 3 秒，结果运行 1 秒就被强制返回了，由于是守护进程，该子进程被终止了。</p>
<p>到这里，我们就了解了守护进程、进程等待和超时设置的用法。</p>
<h2 id="终止进程"><a href="#终止进程" class="headerlink" title="终止进程"></a>终止进程</h2><p>当然，终止进程不止有守护进程这一种做法，我们也可以通过 terminate 方法来终止某个子进程，另外我们还可以通过 is_alive 方法判断进程是否还在运行。</p>
<p>下面我们来看一个实例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'Starting'</span>)</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br><span class="line">    print(<span class="string">'Finished'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    p = multiprocessing.Process(target=process)</span><br><span class="line">    print(<span class="string">'Before:'</span>, p, p.is_alive())</span><br><span class="line">    p.start()</span><br><span class="line">    print(<span class="string">'During:'</span>, p, p.is_alive())</span><br><span class="line">    p.terminate()</span><br><span class="line">    print(<span class="string">'Terminate:'</span>, p, p.is_alive())</span><br><span class="line">    p.join()</span><br><span class="line">    print(<span class="string">'Joined:'</span>, p, p.is_alive())</span><br></pre></td></tr></table></figure>
<p>在上面的例子中，我们用 Process 创建了一个进程，接着调用 start 方法启动这个进程，然后调用 terminate 方法将进程终止，最后调用 join 方法。</p>
<p>另外，在进程运行不同的阶段，我们还通过 is_alive 方法判断当前进程是否还在运行。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Before: &lt;Process(Process-1, initial)&gt; False</span><br><span class="line">During: &lt;Process(Process-1, started)&gt; True</span><br><span class="line">Terminate: &lt;Process(Process-1, started)&gt; True</span><br><span class="line">Joined: &lt;Process(Process-1, stopped[SIGTERM])&gt; False</span><br></pre></td></tr></table></figure>
<p>这里有一个值得注意的地方，在调用 terminate 方法之后，我们用 is_alive 方法获取进程的状态发现依然还是运行状态。在调用 join 方法之后，is_alive 方法获取进程的运行状态才变为终止状态。</p>
<p>所以，在调用 terminate 方法之后，记得要调用一下 join 方法，这里调用 join 方法可以为进程提供时间来更新对象状态，用来反映出最终的进程终止效果。</p>
<h2 id="进程互斥锁"><a href="#进程互斥锁" class="headerlink" title="进程互斥锁"></a>进程互斥锁</h2><p>在上面的一些实例中，我们可能会遇到如下的运行结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Pid: 73993 LoopCount: 0</span><br><span class="line">Pid: 73993 LoopCount: 1</span><br><span class="line">Pid: 73994 LoopCount: 0Pid: 73994 LoopCount: 1</span><br><span class="line"></span><br><span class="line">Pid: 73994 LoopCount: 2</span><br><span class="line">Pid: 73995 LoopCount: 0</span><br><span class="line">Pid: 73995 LoopCount: 1</span><br><span class="line">Pid: 73995 LoopCount: 2</span><br><span class="line">Pid: 73995 LoopCount: 3</span><br><span class="line">Main Process ended</span><br></pre></td></tr></table></figure>
<p>我们发现，有的输出结果没有换行。这是什么原因造成的呢？</p>
<p>这种情况是由多个进程并行执行导致的，两个进程同时进行了输出，结果第一个进程的换行没有来得及输出，第二个进程就输出了结果，导致最终输出没有换行。</p>
<p>那如何来避免这种问题？如果我们能保证，多个进程运行期间的任一时间，只能一个进程输出，其他进程等待，等刚才那个进程输出完毕之后，另一个进程再进行输出，这样就不会出现输出没有换行的现象了。</p>
<p>这种解决方案实际上就是实现了进程互斥，避免了多个进程同时抢占临界区（输出）资源。我们可以通过 multiprocessing 中的 Lock 来实现。Lock，即锁，在一个进程输出时，加锁，其他进程等待。等此进程执行结束后，释放锁，其他进程可以进行输出。</p>
<p>我们首先实现一个不加锁的实例，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Lock</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyProcess</span><span class="params">(Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, loop, lock)</span>:</span></span><br><span class="line">        Process.__init__(self)</span><br><span class="line">        self.loop = loop</span><br><span class="line">        self.lock = lock</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> count <span class="keyword">in</span> range(self.loop):</span><br><span class="line">            time.sleep(<span class="number">0.1</span>)</span><br><span class="line">            <span class="comment"># self.lock.acquire()</span></span><br><span class="line">            print(<span class="string">f'Pid: <span class="subst">&#123;self.pid&#125;</span> LoopCount: <span class="subst">&#123;count&#125;</span>'</span>)</span><br><span class="line">            <span class="comment"># self.lock.release()</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    lock = Lock()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>, <span class="number">15</span>):</span><br><span class="line">        p = MyProcess(i, lock)</span><br><span class="line">        p.start()</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Pid: 74030 LoopCount: 0</span><br><span class="line">Pid: 74031 LoopCount: 0</span><br><span class="line">Pid: 74032 LoopCount: 0</span><br><span class="line">Pid: 74033 LoopCount: 0</span><br><span class="line">Pid: 74034 LoopCount: 0</span><br><span class="line">Pid: 74030 LoopCount: 1</span><br><span class="line">Pid: 74031 LoopCount: 1</span><br><span class="line">Pid: 74032 LoopCount: 1Pid: 74033 LoopCount: 1</span><br><span class="line"></span><br><span class="line">Pid: 74034 LoopCount: 1</span><br><span class="line">Pid: 74030 LoopCount: 2</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>可以看到运行结果中有些输出已经出现了不换行的问题。</p>
<p>我们对其加锁，取消掉刚才代码中的两行注释，重新运行，运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Pid: 74061 LoopCount: 0</span><br><span class="line">Pid: 74062 LoopCount: 0</span><br><span class="line">Pid: 74063 LoopCount: 0</span><br><span class="line">Pid: 74064 LoopCount: 0</span><br><span class="line">Pid: 74065 LoopCount: 0</span><br><span class="line">Pid: 74061 LoopCount: 1</span><br><span class="line">Pid: 74062 LoopCount: 1</span><br><span class="line">Pid: 74063 LoopCount: 1</span><br><span class="line">Pid: 74064 LoopCount: 1</span><br><span class="line">Pid: 74065 LoopCount: 1</span><br><span class="line">Pid: 74061 LoopCount: 2</span><br><span class="line">Pid: 74062 LoopCount: 2</span><br><span class="line">Pid: 74064 LoopCount: 2</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>这时输出效果就正常了。</p>
<p>所以，在访问一些临界区资源时，使用 Lock 可以有效避免进程同时占用资源而导致的一些问题。</p>
<h2 id="信号量"><a href="#信号量" class="headerlink" title="信号量"></a>信号量</h2><p>进程互斥锁可以使同一时刻只有一个进程能访问共享资源，如上面的例子所展示的那样，在同一时刻只能有一个进程输出结果。但有时候我们需要允许多个进程来访问共享资源，同时还需要限制能访问共享资源的进程的数量。</p>
<p>这种需求该如何实现呢？可以用信号量，信号量是进程同步过程中一个比较重要的角色。它可以控制临界资源的数量，实现多个进程同时访问共享资源，限制进程的并发量。</p>
<p>如果你学过操作系统，那么一定对这方面非常了解，如果你还不了解信号量是什么，可以先熟悉一下这个概念。</p>
<p>我们可以用 multiprocessing 库中的 Semaphore 来实现信号量。</p>
<p>那么接下来我们就用一个实例来演示一下进程之间利用 Semaphore 做到多个进程共享资源，同时又限制同时可访问的进程数量，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Semaphore, Lock, Queue</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">buffer = Queue(<span class="number">10</span>)</span><br><span class="line">empty = Semaphore(<span class="number">2</span>)</span><br><span class="line">full = Semaphore(<span class="number">0</span>)</span><br><span class="line">lock = Lock()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Consumer</span><span class="params">(Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">global</span> buffer, empty, full, lock</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            full.acquire()</span><br><span class="line">            lock.acquire()</span><br><span class="line">            buffer.get()</span><br><span class="line">            print(<span class="string">'Consumer pop an element'</span>)</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            lock.release()</span><br><span class="line">            empty.release()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Producer</span><span class="params">(Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">global</span> buffer, empty, full, lock</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            empty.acquire()</span><br><span class="line">            lock.acquire()</span><br><span class="line">            buffer.put(<span class="number">1</span>)</span><br><span class="line">            print(<span class="string">'Producer append an element'</span>)</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            lock.release()</span><br><span class="line">            full.release()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    p = Producer()</span><br><span class="line">    c = Consumer()</span><br><span class="line">    p.daemon = c.daemon = <span class="literal">True</span></span><br><span class="line">    p.start()</span><br><span class="line">    c.start()</span><br><span class="line">    p.join()</span><br><span class="line">    c.join()</span><br><span class="line">    print(<span class="string">'Main Process Ended'</span>)</span><br></pre></td></tr></table></figure>
<p>如上代码实现了经典的生产者和消费者问题。它定义了两个进程类，一个是消费者，一个是生产者。</p>
<p>另外，这里使用 multiprocessing 中的 Queue 定义了一个共享队列，然后定义了两个信号量 Semaphore，一个代表缓冲区空余数，一个表示缓冲区占用数。</p>
<p>生产者 Producer 使用 acquire 方法来占用一个缓冲区位置，缓冲区空闲区大小减 1，接下来进行加锁，对缓冲区进行操作，然后释放锁，最后让代表占用的缓冲区位置数量加 1，消费者则相反。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Producer append an element</span><br><span class="line">Producer append an element</span><br><span class="line">Consumer pop an element</span><br><span class="line">Consumer pop an element</span><br><span class="line">Producer append an element</span><br><span class="line">Producer append an element</span><br><span class="line">Consumer pop an element</span><br><span class="line">Consumer pop an element</span><br><span class="line">Producer append an element</span><br><span class="line">Producer append an element</span><br><span class="line">Consumer pop an element</span><br><span class="line">Consumer pop an element</span><br><span class="line">Producer append an element</span><br><span class="line">Producer append an element</span><br></pre></td></tr></table></figure>
<p>我们发现两个进程在交替运行，生产者先放入缓冲区物品，然后消费者取出，不停地进行循环。 你可以通过上面的例子来体会信号量 Semaphore 的用法，通过 Semaphore 我们很好地控制了进程对资源的并发访问数量。</p>
<h2 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h2><p>在上面的例子中我们使用 Queue 作为进程通信的共享队列使用。</p>
<p>而如果我们把上面程序中的 Queue 换成普通的 list，是完全起不到效果的，因为进程和进程之间的资源是不共享的。即使在一个进程中改变了这个 list，在另一个进程也不能获取到这个 list 的状态，所以声明全局变量对多进程是没有用处的。</p>
<p>那进程如何共享数据呢？可以用 Queue，即队列。当然这里的队列指的是 multiprocessing 里面的 Queue。</p>
<p>依然用上面的例子，我们一个进程向队列中放入随机数据，然后另一个进程取出数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Semaphore, Lock, Queue</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">buffer = Queue(<span class="number">10</span>)</span><br><span class="line">empty = Semaphore(<span class="number">2</span>)</span><br><span class="line">full = Semaphore(<span class="number">0</span>)</span><br><span class="line">lock = Lock()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Consumer</span><span class="params">(Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">global</span> buffer, empty, full, lock</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            full.acquire()</span><br><span class="line">            lock.acquire()</span><br><span class="line">            print(<span class="string">f'Consumer get <span class="subst">&#123;buffer.get()&#125;</span>'</span>)</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            lock.release()</span><br><span class="line">            empty.release()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Producer</span><span class="params">(Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">global</span> buffer, empty, full, lock</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            empty.acquire()</span><br><span class="line">            lock.acquire()</span><br><span class="line">            num = random()</span><br><span class="line">            print(<span class="string">f'Producer put <span class="subst">&#123;num&#125;</span>'</span>)</span><br><span class="line">            buffer.put(num)</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            lock.release()</span><br><span class="line">            full.release()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    p = Producer()</span><br><span class="line">    c = Consumer()</span><br><span class="line">    p.daemon = c.daemon = <span class="literal">True</span></span><br><span class="line">    p.start()</span><br><span class="line">    c.start()</span><br><span class="line">    p.join()</span><br><span class="line">    c.join()</span><br><span class="line">    print(<span class="string">'Main Process Ended'</span>)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Producer put  0.719213647437</span><br><span class="line">Producer put  0.44287326683</span><br><span class="line">Consumer get 0.719213647437</span><br><span class="line">Consumer get 0.44287326683</span><br><span class="line">Producer put  0.722859424381</span><br><span class="line">Producer put  0.525321338921</span><br><span class="line">Consumer get 0.722859424381</span><br><span class="line">Consumer get 0.525321338921</span><br></pre></td></tr></table></figure>
<p>在上面的例子中我们声明了两个进程，一个进程为生产者 Producer，另一个为消费者 Consumer，生产者不断向 Queue 里面添加随机数，消费者不断从队列里面取随机数。</p>
<p>生产者在放数据的时候调用了 Queue 的 put 方法，消费者在取的时候使用了 get 方法，这样我们就通过 Queue 实现两个进程的数据共享了。</p>
<h2 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h2><p>刚才我们使用 Queue 实现了进程间的数据共享，那么进程之间直接通信，如收发信息，用什么比较好呢？可以用 Pipe，管道。</p>
<p>管道，我们可以把它理解为两个进程之间通信的通道。管道可以是单向的，即 half-duplex：一个进程负责发消息，另一个进程负责收消息；也可以是双向的 duplex，即互相收发消息。</p>
<p>默认声明 Pipe 对象是双向管道，如果要创建单向管道，可以在初始化的时候传入 deplex 参数为 False。</p>
<p>我们用一个实例来感受一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Pipe</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Consumer</span><span class="params">(Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, pipe)</span>:</span></span><br><span class="line">        Process.__init__(self)</span><br><span class="line">        self.pipe = pipe</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.pipe.send(<span class="string">'Consumer Words'</span>)</span><br><span class="line">        print(<span class="string">f'Consumer Received: <span class="subst">&#123;self.pipe.recv()&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Producer</span><span class="params">(Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, pipe)</span>:</span></span><br><span class="line">        Process.__init__(self)</span><br><span class="line">        self.pipe = pipe</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">f'Producer Received: <span class="subst">&#123;self.pipe.recv()&#125;</span>'</span>)</span><br><span class="line">        self.pipe.send(<span class="string">'Producer Words'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    pipe = Pipe()</span><br><span class="line">    p = Producer(pipe[<span class="number">0</span>])</span><br><span class="line">    c = Consumer(pipe[<span class="number">1</span>])</span><br><span class="line">    p.daemon = c.daemon = <span class="literal">True</span></span><br><span class="line">    p.start()</span><br><span class="line">    c.start()</span><br><span class="line">    p.join()</span><br><span class="line">    c.join()</span><br><span class="line">    print(<span class="string">'Main Process Ended'</span>)</span><br></pre></td></tr></table></figure>
<p>在这个例子里我们声明了一个默认为双向的管道，然后将管道的两端分别传给两个进程。两个进程互相收发。观察一下结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Producer Received: Consumer Words</span><br><span class="line">Consumer Received: Producer Words</span><br><span class="line">Main Process Ended</span><br></pre></td></tr></table></figure>
<p>管道 Pipe 就像进程之间搭建的桥梁，利用它我们就可以很方便地实现进程间通信了。</p>
<h2 id="进程池"><a href="#进程池" class="headerlink" title="进程池"></a>进程池</h2><p>在前面，我们讲了可以使用 Process 来创建进程，同时也讲了如何用 Semaphore 来控制进程的并发执行数量。</p>
<p>假如现在我们遇到这么一个问题，我有 10000 个任务，每个任务需要启动一个进程来执行，并且一个进程运行完毕之后要紧接着启动下一个进程，同时我还需要控制进程的并发数量，不能并发太高，不然 CPU 处理不过来（如果同时运行的进程能维持在一个最高恒定值当然利用率是最高的）。</p>
<p>那么我们该如何来实现这个需求呢？</p>
<p>用 Process 和 Semaphore 可以实现，但是实现起来比较我们可以用 Process 和 Semaphore 解决问题，但是实现起来比较烦琐。而这种需求在平时又是非常常见的。此时，我们就可以派上进程池了，即 multiprocessing 中的 Pool。</p>
<p>Pool 可以提供指定数量的进程，供用户调用，当有新的请求提交到 pool 中时，如果池还没有满，就会创建一个新的进程用来执行该请求；但如果池中的进程数已经达到规定最大值，那么该请求就会等待，直到池中有进程结束，才会创建新的进程来执行它。</p>
<p>我们用一个实例来实现一下，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function</span><span class="params">(index)</span>:</span></span><br><span class="line">    print(<span class="string">f'Start process: <span class="subst">&#123;index&#125;</span>'</span>)</span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line">    print(<span class="string">f'End process <span class="subst">&#123;index&#125;</span>'</span>, )</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    pool = Pool(processes=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        pool.apply_async(function, args=(i,))</span><br><span class="line">    print(<span class="string">'Main Process started'</span>)</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br><span class="line">    print(<span class="string">'Main Process ended'</span>)</span><br></pre></td></tr></table></figure>
<p>在这个例子中我们声明了一个大小为 3 的进程池，通过 processes 参数来指定，如果不指定，那么会自动根据处理器内核来分配进程数。接着我们使用 apply_async 方法将进程添加进去，args 可以用来传递参数。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Main Process started</span><br><span class="line">Start process: 0</span><br><span class="line">Start process: 1</span><br><span class="line">Start process: 2</span><br><span class="line">End process 0</span><br><span class="line">End process 1</span><br><span class="line">End process 2</span><br><span class="line">Start process: 3</span><br><span class="line">End process 3</span><br><span class="line">Main Process ended</span><br></pre></td></tr></table></figure>
<p>进程池大小为 3，所以最初可以看到有 3 个进程同时执行，第4个进程在等待，在有进程运行完毕之后，第4个进程马上跟着运行，出现了如上的运行效果。</p>
<p>最后，我们要记得调用 close 方法来关闭进程池，使其不再接受新的任务，然后调用 join 方法让主进程等待子进程的退出，等子进程运行完毕之后，主进程接着运行并结束。</p>
<p>不过上面的写法多少有些烦琐，这里再介绍进程池一个更好用的 map 方法，可以将上述写法简化很多。</p>
<p>map 方法是怎么用的呢？第一个参数就是要启动的进程对应的执行方法，第 2 个参数是一个可迭代对象，其中的每个元素会被传递给这个执行方法。</p>
<p>举个例子：现在我们有一个 list，里面包含了很多 URL，另外我们也定义了一个方法用来抓取每个 URL 内容并解析，那么我们可以直接在 map 的第一个参数传入方法名，第 2 个参数传入 URL 数组。</p>
<p>我们用一个实例来感受一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        urllib.request.urlopen(url)</span><br><span class="line">        print(<span class="string">f'URL <span class="subst">&#123;url&#125;</span> Scraped'</span>)</span><br><span class="line">    <span class="keyword">except</span> (urllib.error.HTTPError, urllib.error.URLError):</span><br><span class="line">        print(<span class="string">f'URL <span class="subst">&#123;url&#125;</span> not Scraped'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    pool = Pool(processes=<span class="number">3</span>)</span><br><span class="line">    urls = [</span><br><span class="line">        <span class="string">'https://www.baidu.com'</span>,</span><br><span class="line">        <span class="string">'http://www.meituan.com/'</span>,</span><br><span class="line">        <span class="string">'http://blog.csdn.net/'</span>,</span><br><span class="line">        <span class="string">'http://xxxyxxx.net'</span></span><br><span class="line">    ]</span><br><span class="line">    pool.map(scrape, urls)</span><br><span class="line">    pool.close()</span><br></pre></td></tr></table></figure>
<p>这个例子中我们先定义了一个 scrape 方法，它接收一个参数 url，这里就是请求了一下这个链接，然后输出爬取成功的信息，如果发生错误，则会输出爬取失败的信息。</p>
<p>首先我们要初始化一个 Pool，指定进程数为 3。然后我们声明一个 urls 列表，接着我们调用了 map 方法，第 1 个参数就是进程对应的执行方法，第 2 个参数就是 urls 列表，map 方法会依次将 urls 的每个元素作为 scrape 的参数传递并启动一个新的进程，加到进程池中执行。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">URL https:&#x2F;&#x2F;www.baidu.com Scraped</span><br><span class="line">URL http:&#x2F;&#x2F;xxxyxxx.net not Scraped</span><br><span class="line">URL http:&#x2F;&#x2F;blog.csdn.net&#x2F; Scraped</span><br><span class="line">URL http:&#x2F;&#x2F;www.meituan.com&#x2F; Scraped</span><br></pre></td></tr></table></figure>
<p>这样，我们就可以实现 3 个进程并行运行。不同的进程相互独立地输出了对应的爬取结果。</p>
<p>可以看到，我们利用 Pool 的 map 方法非常方便地实现了多进程的执行。后面我们也会在实战案例中结合进程池来实现数据的爬取。</p>
<p>以上便是 Python 中多进程的基本用法，本节内容比较多，后面的实战案例也会用到这些内容，需要好好掌握。</p>
]]></content>
  </entry>
</search>
