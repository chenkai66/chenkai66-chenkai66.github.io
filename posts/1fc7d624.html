<!DOCTYPE html>


<html lang="zh-CN">


<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    Python深度学习（二）深度学习用于计算机视觉 |  言念君子
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
  
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

  
  

  

</head>

</html>

<body>
  <div id="app">
    
      
      <canvas width="1777" height="841"
        style="position: fixed; left: 0px; top: 0px; z-index: 99999; pointer-events: none;"></canvas>
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-Python深度学习（二）深度学习用于计算机视觉"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  Python深度学习（二）深度学习用于计算机视觉
</h1>
 

    </header>
     
    <div class="article-meta">
      <a href="/posts/1fc7d624.html" class="article-date">
  <time datetime="2021-04-05T15:27:25.089Z" itemprop="datePublished">2021-04-05</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/DeepLearning/">DeepLearning</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">21.4k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">94 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <p>本节将介绍卷积神经网络，也叫<code>convnet</code>，它是计算机视觉应用几乎都在使用的一种深度学习模型。你将学到将卷积神经网络应用于图像分类问题，特别是那些训练数据集较小的问题。如果你工作的地方并非大型科技公司，这也将是你最常见的使用场景。</p>
<h1 id="一、卷积神经网络简介"><a href="#一、卷积神经网络简介" class="headerlink" title="一、卷积神经网络简介"></a>一、卷积神经网络简介</h1><p>我们将深入讲解卷积神经网络的原理，以及它在计算机视觉任务上为什么如此成功。但在此之前，我们先来看一个简单的卷积神经网络示例，即使用卷积神经网络对MNIST 数字进行分类，这个任务我们在第2 章用密集连接网络做过（当时的测试精度为97.8%）。虽然本例中的卷积神经网络很简单，但其精度肯定会超过前面的密集连接网络。</p>
<p>下列代码将会展示一个简单的卷积神经网络。它是Conv2D 层和MaxPooling2D 层的堆叠，很快你就会知道这些层的作用。</p>
<h2 id="1-实例化一个小型的卷积神经网络"><a href="#1-实例化一个小型的卷积神经网络" class="headerlink" title="1. 实例化一个小型的卷积神经网络"></a>1. 实例化一个小型的卷积神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Using TensorFlow backend.
</code></pre><p>重要的是，卷积神经网络接收形状为<code>(image_height, image_width, image_channels)</code>的输入张量（不包括批量维度）。本例中设置卷积神经网络处理大小为(28, 28, 1) 的输入张量，这正是MNIST 图像的格式。我们向第一层传入参数<code>input_shape=(28, 28, 1)</code> 来完成此设置。</p>
<p>我们来看一下目前卷积神经网络的架构。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     
=================================================================
Total params: 55,744
Trainable params: 55,744
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>可以看到，每个Conv2D 层和MaxPooling2D 层的输出都是一个形状为(height, width,channels) 的3D 张量。宽度和高度两个维度的尺寸通常会随着网络加深而变小，通道数量由传<br>入Conv2D 层的第一个参数所控制（32 或64）。</p>
<p>下一步是将最后的输出张量［大小为(3, 3, 64)］输入到一个密集连接分类器网络中，即Dense 层的堆叠，你已经很熟悉了。这些分类器可以处理1D 向量，而当前的输出是3D 张量。首先，我们需要将3D 输出展平为1D，然后在上面添加几个Dense 层。</p>
<h2 id="2-在卷积神经网络上添加分类器"><a href="#2-在卷积神经网络上添加分类器" class="headerlink" title="2. 在卷积神经网络上添加分类器"></a>2. 在卷积神经网络上添加分类器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br></pre></td></tr></table></figure>
<p>我们将进行10 类别分类，最后一层使用带10 个输出的softmax 激活。现在网络的架构如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     
_________________________________________________________________
flatten_1 (Flatten)          (None, 576)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                36928     
_________________________________________________________________
dense_2 (Dense)              (None, 10)                650       
=================================================================
Total params: 93,322
Trainable params: 93,322
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>如你所见，在进入两个Dense层之前，形状(3, 3, 64) 的输出被展平为形状(576,) 的向量。</p>
<p>下面我们在MNIST数字图像上训练这个卷积神经网络。我们将复用MNIST示例中的很多代码。</p>
<h2 id="3-在MINST图像上训练卷积神经网络"><a href="#3-在MINST图像上训练卷积神经网络" class="headerlink" title="3. 在MINST图像上训练卷积神经网络"></a>3. 在MINST图像上训练卷积神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br><span class="line">train_images = train_images.reshape((<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line">train_images = train_images.astype(<span class="string">'float32'</span>) / <span class="number">255</span></span><br><span class="line">test_images = test_images.reshape((<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line">test_images = test_images.astype(<span class="string">'float32'</span>) / <span class="number">255</span></span><br><span class="line">train_labels = to_categorical(train_labels)</span><br><span class="line">test_labels = to_categorical(test_labels)</span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">            loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(train_images, train_labels, epochs=<span class="number">5</span>, batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/5
60000/60000 [==============================] - 25s 419us/step - loss: 0.1652 - accuracy: 0.9488
Epoch 2/5
60000/60000 [==============================] - 26s 429us/step - loss: 0.0458 - accuracy: 0.9864
Epoch 3/5
60000/60000 [==============================] - 24s 400us/step - loss: 0.0320 - accuracy: 0.9897
Epoch 4/5
60000/60000 [==============================] - 24s 396us/step - loss: 0.0247 - accuracy: 0.9924
Epoch 5/5
60000/60000 [==============================] - 24s 393us/step - loss: 0.0196 - accuracy: 0.9940
</code></pre><p>我们在测试数据上对模型进行评估。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_loss, test_acc = model.evaluate(test_images, test_labels)</span><br><span class="line">test_acc</span><br></pre></td></tr></table></figure>
<pre><code>10000/10000 [==============================] - 1s 112us/step


0.9923999905586243
</code></pre><p>密集连接网络的测试精度为97.8%，但这个简单卷积神经网络的测试精度达到了99.1%，我们将错误率降低了68%（相对比例）。相当不错！与密集连接模型相比，为什么这个简单卷积神经网络的效果这么好？要回答这个问题，我们来深入了解Conv2D 层和MaxPooling2D 层的作用。</p>
<h2 id="4-卷积神经网络"><a href="#4-卷积神经网络" class="headerlink" title="4. 卷积神经网络"></a>4. 卷积神经网络</h2><p>密集连接层和卷积层的根本区别在于，Dense 层从输入特征空间中学到的是全局模式（比如对于MNIST 数字，全局模式就是涉及所有像素的模式），而卷积层学到的是局部模式。对于图像来说，学到的就是在输入图像的二维小窗口中发现的模式。在上面的例子中，这些窗口的大小都是3×3。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\conv_1.png" width="300" height="300" alt="图像可以被分解为局部模式，如边缘、纹理等" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图像可以被分解为局部模式，如边缘、纹理等</div>
</center>

<p>这个重要特性使卷积神经网络具有以下两个有趣的性质。</p>
<ul>
<li>卷积神经网络学到的模式具有平移不变性（translation invariant）。卷积神经网络在图像右下角学到某个模式之后，它可以在任何地方识别这个模式，比如左上角。对于密集连接网络来说，如果模式出现在新的位置，它只能重新学习这个模式。这使得卷积神经网络在处理图像时可以高效利用数据（因为视觉世界从根本上具有平移不变性），它只需要更少的训练样本就可以学到具有泛化能力的数据表示。</li>
<li>卷积神经网络可以学到模式的空间层次结构（spatial hierarchies of patterns）。第一个卷积层将学习较小的局部模式（比如边缘），第二个卷积层将学习由第一层特征组成的更大的模式，以此类推。这使得卷积神经网络可以有效地学习越来越复杂、越来越抽象的视觉概念（因为视觉世界从根本上具有空间层次结构）。</li>
</ul>
<p>对于包含两个空间轴（高度和宽度）和一个深度轴（也叫通道轴）的3D 张量，其卷积也叫特征图（feature map）。对于RGB 图像，深度轴的维度大小等于3，因为图像有3 个颜色通道：红色、绿色和蓝色。对于黑白图像（比如MNIST 数字图像），深度等于1（表示灰度等级）。卷积运算从输入特征图中提取图块，并对所有这些图块应用相同的变换，生成输出特征图（output feature map）。该输出特征图仍是一个3D 张量，具有宽度和高度，其深度可以任意取值，因为输出深度是层的参数，深度轴的不同通道不再像RGB 输入那样代表特定颜色，而是代表过滤器（filter）。过滤器对输入数据的某一方面进行编码，比如，单个过滤器可以从更高层次编码这样一个概念：“输入中包含一张脸。”</p>
<center>
    <img src="\Pic\DeepLearning_Pic\cat.png" width="300" height="300" alt="cat" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">视觉世界形成了视觉模块的空间层次结构：超局部的边缘组合成局部的对象，比如眼睛或耳朵，这些局部对象又组合成高级概念，比如“猫”</div>
</center>

<p>在MNIST示例中，第一个卷积层接收一个大小为<code>(28, 28, 1)</code>的特征图，并输出一个大小为<code>(26, 26, 32)</code>的特征图，即它在输入上计算32个过滤器。对于这32个输出通道，每个通道都包含一个26×26的数值网格，它是过滤器对输入的响应图（response map），表示这个过滤器模式在输入中不同位置的响应。这也是特征图这一术语的含义：深度轴的每个维度都是一个特征（或过滤器），而2D 张量<code>output[:, :, n]</code>是这个过滤器在输入上的响应的二维空间图（map）。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\响应图.png" width="300" height="300" alt="响应图" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">响应图的概念：某个模式在输入中的不同位置是否存在的二维图</div>
</center>

<p>卷积由以下两个关键参数所定义：</p>
<ul>
<li>从输入中提取的图块尺寸：这些图块的大小通常是 3×3 或 5×5。本例中为 3×3，这是很常见的选择。</li>
<li>输出特征图的深度：卷积所计算的过滤器的数量。本例第一层的深度为32，最后一层的深度是64。</li>
</ul>
<p>对于Keras 的Conv2D 层，这些参数都是向层传入的前几个参数：<code>Conv2D(output_depth,(window_height, window_width))</code>。</p>
<p>卷积的工作原理：在3D 输入特征图上滑动（slide）这些3×3 或5×5 的窗口，在每个可能的位置停止并提取周围特征的3D图块［形状为<code>(window_height, window_width, input_depth)</code>］。然后每个3D 图块与学到的同一个权重矩阵［叫作卷积核（convolution kernel）］做张量积，转换成形状为<code>(output_depth,)</code> 的1D 向量。然后对所有这些向量进行空间重组，使其转换为形状为<code>(height, width, output_depth)</code>的3D 输出特征图。输出特征图中的每个空间位置都对应于输入特征图中的相同位置（比如输出的右下角包含了输入右下角的信息）。举个例子，利用3×3的窗口，向量<code>output[i, j, :]</code>来自3D 图块<code>input[i-1:i+1,j-1:j+1, :]</code>。整个过程详见下图：</p>
<center>
    <img src="\Pic\DeepLearning_Pic\卷积的工作原理.png" width="300" height="300" alt="卷积的工作原理" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">卷积的工作原理</div>
</center>

<p>注意，输出的宽度和高度可能与输入的宽度和高度不同，不同的原因可能有两点。</p>
<ul>
<li>边界效应，可以通过对输入特征图进行填充来抵消。</li>
<li>使用了步幅（stride），稍后会给出其定义。</li>
</ul>
<h2 id="5-最大池化运算"><a href="#5-最大池化运算" class="headerlink" title="5. 最大池化运算"></a>5. 最大池化运算</h2><p>在卷积神经网络示例中，你可能注意到，在每个MaxPooling2D层之后，特征图的尺寸都会减半。例如，在第一个MaxPooling2D层之前，特征图的尺寸是26×26，但最大池化运算将其减半为13×13。这就是最大池化的作用：<strong>对特征图进行下采样，与步进卷积类似。最大池化是从输入特征图中提取窗口，并输出每个通道的最大值。</strong>它的概念与卷积类似，但是最大池化使用硬编码的<code>max</code>张量运算对局部图块进行变换，而不是使用学到的线性变换（卷积核）。最大池化与卷积的最大不同之处在于，最大池化通常使用2×2的窗口和步幅2，其目的是将特征图下采样2倍。与此相对的是，卷积通常使用3×3 窗口和步幅1。为什么要用这种方式对特征图下采样？为什么不删除最大池化层，一直保留较大的特征图？我们来这么做试一下。这时模型的卷积基（convolutional base）如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model_no_max_pool = models.Sequential()</span><br><span class="line">model_no_max_pool.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)))</span><br><span class="line">model_no_max_pool.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model_no_max_pool.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model_no_max_pool.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_2&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 24, 24, 64)        18496     
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 22, 22, 64)        36928     
=================================================================
Total params: 55,744
Trainable params: 55,744
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>这种架构有什么问题？有如下两点问题：</p>
<ul>
<li>这种架构不利于学习特征的空间层级结构。第三层的 3×3 窗口中只包含初始输入的 7×7 窗口中所包含的信息。卷积神经网络学到的高级模式相对于初始输入来说仍然很小，这可能不足以学会对数字进行分类（你可以试试仅通过7 像素×7 像素的窗口观察图像来识别其中的数字）。我们需要让最后一个卷积层的特征包含输入的整体信息。</li>
<li>最后一层的特征图对每个样本共有 22×22×64=30 976 个元素。这太多了。如果你将其展平并在上面添加一个大小为512 的Dense 层，那一层将会有1580 万个参数。这对于这样一个小模型来说太多了，会导致严重的过拟合。</li>
</ul>
<p>简而言之，使用下采样的原因，一是减少需要处理的特征图的元素个数，二是通过让连续卷积层的观察窗口越来越大（即窗口覆盖原始输入的比例越来越大），从而引入空间过滤器的层级结构。</p>
<p>注意，最大池化不是实现这种下采样的唯一方法。你已经知道，还可以在前一个卷积层中使用步幅来实现。此外，你还可以使用平均池化来代替最大池化，其方法是将每个局部输入图块变换为取该图块各通道的平均值，而不是最大值。但最大池化的效果往往比这些替代方法更好。</p>
<p>简而言之，原因在于特征中往往编码了某种模式或概念在特征图的不同位置是否存在（因此得名特征图），而观察不同特征的最大值而不是平均值能够给出更多的信息。因此，最合理的子采样策略是<strong>首先生成密集的特征图（通过无步进的卷积），然后观察特征每个小图块上的最大激活</strong>，而不是查看输入的稀疏窗口（通过步进卷积）或对输入图块取平均，因为后两种方法可能导致错过或淡化特征是否存在的信息。</p>
<p>现在你应该已经理解了卷积神经网络的基本概念，即特征图、卷积和最大池化，并且也知道如何构建一个小型卷积神经网络来解决简单问题，比如MNIST 数字分类。下面我们将介绍更加实用的应用。</p>
<h1 id="二、在小型数据集上从头开始训练一个卷积神经网络"><a href="#二、在小型数据集上从头开始训练一个卷积神经网络" class="headerlink" title="二、在小型数据集上从头开始训练一个卷积神经网络"></a>二、在小型数据集上从头开始训练一个卷积神经网络</h1><p>使用很少的数据来训练一个图像分类模型，这是很常见的情况，如果你要从事计算机视觉方面的职业，很可能会在实践中遇到这种情况。“很少的”样本可能是几百张图像，也可能是几万张图像。来看一个实例，我们将重点讨论猫狗图像分类，数据集中包含4000 张猫和狗的图像（2000 张猫的图像，2000 张狗的图像）。我们将2000 张图像用于训练，1000 张用于验证，1000张用于测试。</p>
<p>本节将介绍解决这一问题的基本策略，即使用已有的少量数据从头开始训练一个新模型。首先，在2000 个训练样本上训练一个简单的小型卷积神经网络，不做任何正则化，为模型目标设定一个基准。这会得到71% 的分类精度。此时主要的问题在于过拟合。然后，我们会介绍数据增强（data augmentation），它在计算机视觉领域是一种非常强大的降低过拟合的技术。使用数据增强之后，网络精度将提高到82%。随后我们会介绍将深度学习应用于小型数据集的另外两个重要技巧：用预训练的网络做特征提取（得到的精度范围在90%~96%），对预训练的网络进行微调（最终精度为97%）。总而言之，这三种策略——从头开始训练一个小型模型、使用预训练的网络做特征提取、对预训练的网络进行微调——构成了你的工具箱，未来可用于解决小型数据集的图像分类问题。</p>
<h2 id="1-深度学习与小数据问题的相关性"><a href="#1-深度学习与小数据问题的相关性" class="headerlink" title="1. 深度学习与小数据问题的相关性"></a>1. 深度学习与小数据问题的相关性</h2><p>有时你会听人说，仅在有大量数据可用时，深度学习才有效。这种说法部分正确：深度学习的一个基本特性就是能够独立地在训练数据中找到有趣的特征，无须人为的特征工程，而这只在拥有大量训练样本时才能实现。对于输入样本的维度非常高（比如图像）的问题尤其如此。</p>
<p>但对于初学者来说，所谓“大量”样本是相对的，即相对于你所要训练网络的大小和深度而言。只用几十个样本训练卷积神经网络就解决一个复杂问题是不可能的，但如果模型很小，并做了很好的正则化，同时任务非常简单，那么几百个样本可能就足够了。由于卷积神经网络学到的是局部的、平移不变的特征，它对于感知问题可以高效地利用数据。虽然数据相对较少，但在非常小的图像数据集上从头开始训练一个卷积神经网络，仍然可以得到不错的结果，而且无须任何自定义的特征工程。</p>
<p>此外，深度学习模型本质上具有高度的可复用性，比如，已有一个在大规模数据集上训练的图像分类模型或语音转文本模型，你只需做很小的修改就能将其复用于完全不同的问题。特别是在计算机视觉领域，许多预训练的模型（通常都是在ImageNet 数据集上训练得到的）现在都可以公开下载，并可以用于在数据很少的情况下构建强大的视觉模型。我们先来看一下数据。</p>
<h2 id="2-下载数据"><a href="#2-下载数据" class="headerlink" title="2. 下载数据"></a>2. 下载数据</h2><p>本节用到的猫狗分类数据集不包含在Keras 中。它由Kaggle 在2013 年末公开并作为一项计算视觉竞赛的一部分，当时卷积神经网络还不是主流算法。你可以从<a href="https://www.kaggle.com/c/dogs-vs-cats/data" target="_blank" rel="noopener">https://www.kaggle.com/c/dogs-vs-cats/data</a> 下载原始数据集。</p>
<p>这些图像都是中等分辨率的彩色JPEG 图像:</p>
<center>
    <img src="\Pic\DeepLearning_Pic\cat_dog_1.png" width="300" height="300" alt="猫狗分类数据集的一些样本。没有修改尺寸：样本在尺寸、外观等方面是不一样的" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">猫狗分类数据集的一些样本。没有修改尺寸：样本在尺寸、外观等方面是不一样的</div>
</center>

<p>不出所料，2013 年的猫狗分类Kaggle 竞赛的优胜者使用的是卷积神经网络。最佳结果达到了95% 的精度。本例中，虽然你只在不到参赛选手所用的10% 的数据上训练模型，但结果也和这个精度相当接近。</p>
<p>这个数据集包含25 000 张猫狗图像（每个类别都有12 500 张），大小为543MB（压缩后）。下载数据并解压之后，你需要创建一个新数据集，其中包含三个子集：每个类别各1000 个样本的训练集、每个类别各500 个样本的验证集和每个类别各500 个样本的测试集。创建新数据集的代码如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将图像复制到训练、验证和测试的目录</span></span><br><span class="line"><span class="keyword">import</span> os, shutil</span><br><span class="line">original_dataset_dir = <span class="string">'data/cat_dog/kaggle_original_data'</span></span><br><span class="line">base_dir = <span class="string">'data/cat_dog/cats_and_dogs_small'</span></span><br><span class="line">os.mkdir(base_dir)</span><br><span class="line">train_dir = os.path.join(base_dir, <span class="string">'train'</span>)</span><br><span class="line">os.mkdir(train_dir)</span><br><span class="line">validation_dir = os.path.join(base_dir, <span class="string">'validation'</span>)</span><br><span class="line">os.mkdir(validation_dir)</span><br><span class="line">test_dir = os.path.join(base_dir, <span class="string">'test'</span>)</span><br><span class="line">os.mkdir(test_dir)</span><br><span class="line">train_cats_dir = os.path.join(train_dir, <span class="string">'cats'</span>)</span><br><span class="line">os.mkdir(train_cats_dir)</span><br><span class="line">train_dogs_dir = os.path.join(train_dir, <span class="string">'dogs'</span>)</span><br><span class="line">os.mkdir(train_dogs_dir)</span><br><span class="line">validation_cats_dir = os.path.join(validation_dir, <span class="string">'cats'</span>)</span><br><span class="line">os.mkdir(validation_cats_dir)</span><br><span class="line">validation_dogs_dir = os.path.join(validation_dir, <span class="string">'dogs'</span>)</span><br><span class="line">os.mkdir(validation_dogs_dir)</span><br><span class="line">test_cats_dir = os.path.join(test_dir, <span class="string">'cats'</span>)</span><br><span class="line">os.mkdir(test_cats_dir)</span><br><span class="line">test_dogs_dir = os.path.join(test_dir, <span class="string">'dogs'</span>)</span><br><span class="line">os.mkdir(test_dogs_dir)</span><br><span class="line">fnames = [<span class="string">'cat.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">dst = os.path.join(train_cats_dir, fname)</span><br><span class="line">shutil.copyfile(src, dst)</span><br><span class="line">fnames = [<span class="string">'cat.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>, <span class="number">1500</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">dst = os.path.join(validation_cats_dir, fname)</span><br><span class="line">shutil.copyfile(src, dst)</span><br><span class="line">fnames = [<span class="string">'cat.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1500</span>, <span class="number">2000</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">dst = os.path.join(test_cats_dir, fname)</span><br><span class="line">shutil.copyfile(src, dst)</span><br><span class="line">fnames = [<span class="string">'dog.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">dst = os.path.join(train_dogs_dir, fname)</span><br><span class="line">shutil.copyfile(src, dst)</span><br><span class="line">fnames = [<span class="string">'dog.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>, <span class="number">1500</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">dst = os.path.join(validation_dogs_dir, fname)</span><br><span class="line">shutil.copyfile(src, dst)</span><br><span class="line">fnames = [<span class="string">'dog.&#123;&#125;.jpg'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1500</span>, <span class="number">2000</span>)]</span><br><span class="line"><span class="keyword">for</span> fname <span class="keyword">in</span> fnames:</span><br><span class="line">src = os.path.join(original_dataset_dir, fname)</span><br><span class="line">dst = os.path.join(test_dogs_dir, fname)</span><br><span class="line">shutil.copyfile(src, dst)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os, shutil</span><br><span class="line">base_dir = <span class="string">'data/cat_dog/cats_and_dogs_small'</span></span><br><span class="line">train_dir = os.path.join(base_dir, <span class="string">'train'</span>)</span><br><span class="line">test_dir = os.path.join(base_dir, <span class="string">'test'</span>)</span><br><span class="line">validation_dir = os.path.join(base_dir, <span class="string">'validation'</span>)</span><br><span class="line">train_cats_dir = os.path.join(train_dir, <span class="string">'cats'</span>)</span><br><span class="line">test_cats_dir = os.path.join(test_dir, <span class="string">'cats'</span>)</span><br><span class="line">validation_cats_dir = os.path.join(validation_dir, <span class="string">'cats'</span>)</span><br><span class="line">train_dogs_dir = os.path.join(train_dir, <span class="string">'dogs'</span>)</span><br><span class="line">test_dogs_dir = os.path.join(test_dir, <span class="string">'dogs'</span>)</span><br><span class="line">validation_dogs_dir = os.path.join(validation_dir, <span class="string">'dogs'</span>)</span><br><span class="line">print(<span class="string">'total training cat images:'</span>, len(os.listdir(train_cats_dir)))</span><br><span class="line">print(<span class="string">'total training dog images:'</span>, len(os.listdir(train_dogs_dir)))</span><br><span class="line">print(<span class="string">'total validation cat images:'</span>, len(os.listdir(validation_cats_dir)))</span><br><span class="line">print(<span class="string">'total validation dog images:'</span>, len(os.listdir(validation_dogs_dir)))</span><br><span class="line">print(<span class="string">'total test cat images:'</span>, len(os.listdir(test_cats_dir)))</span><br><span class="line">print(<span class="string">'total test dog images:'</span>, len(os.listdir(test_dogs_dir)))</span><br></pre></td></tr></table></figure>
<pre><code>total training cat images: 1000
total training dog images: 1000
total validation cat images: 500
total validation dog images: 500
total test cat images: 500
total test dog images: 500
</code></pre><p>所以我们的确有2000 张训练图像、1000 张验证图像和1000 张测试图像。每个分组中两个类别的样本数相同，这是一个平衡的二分类问题，分类精度可作为衡量成功的指标。</p>
<h2 id="3-构建网络"><a href="#3-构建网络" class="headerlink" title="3. 构建网络"></a>3. 构建网络</h2><p>在前一个MNIST示例中，我们构建了一个小型卷积神经网络，所以你应该已经熟悉这种网络。我们将复用相同的总体结构，即卷积神经网络由<code>Conv2D</code>层（使用<code>relu</code>激活）和<code>MaxPooling2D</code>层交替堆叠构成。</p>
<p>但由于这里要处理的是更大的图像和更复杂的问题，你需要相应地增大网络，即再增加一个<code>Conv2D+MaxPooling2D</code>的组合。这既可以增大网络容量，也可以进一步减小特征图的尺寸，使其在连接<code>Flatten</code>层时尺寸不会太大。本例中初始输入的尺寸为150×150（有些随意的选择），所以最后在<code>Flatten</code>层之前的特征图大小为7×7。</p>
<p>你面对的是一个二分类问题，所以网络最后一层是使用<code>sigmoid</code>激活的单一单元（大小为 1 的<code>Dense</code>层）。这个单元将对某个类别的概率进行编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将猫狗分类的小型卷积神经网络实例化</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">"relu"</span>, input_shape=(<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>)))</span><br><span class="line">model.add(layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_4&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_8 (Conv2D)            (None, 148, 148, 32)      896       
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 74, 74, 32)        0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 72, 72, 64)        18496     
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 17, 17, 128)       0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 36992)             0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               18940416  
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 513       
=================================================================
Total params: 19,034,177
Trainable params: 19,034,177
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>在编译这一步，和前面一样，我们将使用RMSprop 优化器。因为网络最后一层是单一sigmoid单元，所以我们将使用二元交叉熵作为损失函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置模型用于训练</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,optimizer=optimizers.RMSprop(lr=<span class="number">1e-4</span>),metrics=[<span class="string">'acc'</span>])</span><br></pre></td></tr></table></figure>
<h2 id="4-数据预处理"><a href="#4-数据预处理" class="headerlink" title="4. 数据预处理"></a>4. 数据预处理</h2><p>你现在已经知道，将数据输入神经网络之前，应该将数据格式化为经过预处理的浮点数张量。现在，数据以 JPEG 文件的形式保存在硬盘中，所以数据预处理步骤大致如下：</p>
<ol>
<li>读取图像文件</li>
<li>将JPEG文件解码为RGB像素网格</li>
<li>将这些像素网格转换为浮点数张量</li>
<li>将像素值（0~255 范围内）缩放到 <code>[0, 1]</code> 区间（正如你所知，神经网络喜欢处理较小的输<br>入值）</li>
</ol>
<p>这些步骤可能看起来有点吓人，但幸运的是，Keras 拥有自动完成这些步骤的工具。Keras有一个图像处理辅助工具的模块，位于<code>keras.preprocessing.image</code>。特别地，它包含<code>ImageDataGenerator</code>类，可以快速创建Python生成器，能够将硬盘上的图像文件自动转换为预处理好的张量批量。下面我们将用到这个类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用ImageDataGenerator 从目录中读取图像</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line">train_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line">test_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">        train_dir,</span><br><span class="line">        target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">        batch_size=<span class="number">20</span>,</span><br><span class="line">        class_mode=<span class="string">'binary'</span>)</span><br><span class="line">validation_generator = test_datagen.flow_from_directory(</span><br><span class="line">        validation_dir,</span><br><span class="line">        target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">        batch_size=<span class="number">20</span>,</span><br><span class="line">        class_mode=<span class="string">'binary'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
</code></pre><p>我们来看一下其中一个生成器的输出：它生成了150×150 的RGB 图像［形状为(20,150, 150, 3)］与二进制标签［形状为(20,)］组成的批量。每个批量中包含20 个样本（批量大小）。注意，生成器会不停地生成这些批量，它会不断循环目标文件夹中的图像。因此，你需要在某个时刻终止（break）迭代循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data_batch, labels_batch <span class="keyword">in</span> train_generator:</span><br><span class="line">    print(<span class="string">'data batch shape:'</span>, data_batch.shape)</span><br><span class="line">    print(<span class="string">'labels batch shape:'</span>, labels_batch.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<pre><code>data batch shape: (20, 150, 150, 3)
labels batch shape: (20,)
</code></pre><p>利用生成器，我们让模型对数据进行拟合。我们将使用<code>fit_generator</code>方法来拟合，它在数据生成器上的效果和fit 相同。它的第一个参数应该是一个Python生成器，可以不停地生成输入和目标组成的批量，比如<code>train_generator</code>。因为数据是不断生成的，所以Keras模型要知道每一轮需要从生成器中抽取多少个样本。这是<code>steps_per_epoch</code>参数的作用：从生成器中抽取<code>steps_per_epoch</code>个批量后（即运行了<code>steps_per_epoch</code>次梯度下降），拟合过程将进入下一个轮次。本例中，每个批量包含20个样本，所以读取完所有2000 个样本需要100个批量。</p>
<p>使用<code>fit_generator</code>时，你可以传入一个<code>validation_data</code>参数，其作用和在<code>fit</code>方法中类似。值得注意的是，这个参数可以是一个数据生成器，但也可以是Numpy数组组成的元组。如果向<code>validation_data</code>传入一个生成器，那么这个生成器应该能够不停地生成验证数据批量，因此你还需要指定<code>validation_steps</code>参数，说明需要从验证生成器中抽取多少个批次用于评估。</p>
<h2 id="5-拟合模型"><a href="#5-拟合模型" class="headerlink" title="5. 拟合模型"></a>5. 拟合模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit_generator(</span><br><span class="line">    train_generator,</span><br><span class="line">    steps_per_epoch=<span class="number">100</span>,</span><br><span class="line">    epochs=<span class="number">30</span>,</span><br><span class="line">    validation_data=validation_generator,</span><br><span class="line">    validation_steps=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/30
100/100 [==============================] - 71s 709ms/step - loss: 0.6845 - acc: 0.5520 - val_loss: 0.6738 - val_acc: 0.5850
Epoch 2/30
100/100 [==============================] - 65s 645ms/step - loss: 0.6184 - acc: 0.6405 - val_loss: 0.5264 - val_acc: 0.6730
Epoch 3/30
100/100 [==============================] - 83s 830ms/step - loss: 0.5660 - acc: 0.7160 - val_loss: 0.4590 - val_acc: 0.6360
Epoch 4/30
100/100 [==============================] - 74s 739ms/step - loss: 0.5222 - acc: 0.7400 - val_loss: 0.6281 - val_acc: 0.6680
Epoch 5/30
100/100 [==============================] - 70s 699ms/step - loss: 0.4870 - acc: 0.7635 - val_loss: 0.5049 - val_acc: 0.7010
Epoch 6/30
100/100 [==============================] - 70s 700ms/step - loss: 0.4575 - acc: 0.7800 - val_loss: 0.6481 - val_acc: 0.7110
Epoch 7/30
100/100 [==============================] - 70s 698ms/step - loss: 0.4273 - acc: 0.8025 - val_loss: 0.7052 - val_acc: 0.7060
Epoch 8/30
100/100 [==============================] - 70s 697ms/step - loss: 0.3928 - acc: 0.8265 - val_loss: 0.4466 - val_acc: 0.7050
Epoch 9/30
100/100 [==============================] - 70s 700ms/step - loss: 0.3731 - acc: 0.8485 - val_loss: 0.5841 - val_acc: 0.7180
Epoch 10/30
100/100 [==============================] - 71s 708ms/step - loss: 0.3347 - acc: 0.8575 - val_loss: 0.5448 - val_acc: 0.6970
Epoch 11/30
100/100 [==============================] - 71s 706ms/step - loss: 0.3212 - acc: 0.8655 - val_loss: 0.6970 - val_acc: 0.7150
Epoch 12/30
100/100 [==============================] - 70s 703ms/step - loss: 0.2870 - acc: 0.8830 - val_loss: 0.4558 - val_acc: 0.7210
Epoch 13/30
100/100 [==============================] - 70s 704ms/step - loss: 0.2593 - acc: 0.8990 - val_loss: 0.4447 - val_acc: 0.6940
Epoch 14/30
100/100 [==============================] - 70s 704ms/step - loss: 0.2326 - acc: 0.9165 - val_loss: 0.5825 - val_acc: 0.7220
Epoch 15/30
100/100 [==============================] - 71s 706ms/step - loss: 0.2018 - acc: 0.9325 - val_loss: 0.1710 - val_acc: 0.7170
Epoch 16/30
100/100 [==============================] - 71s 708ms/step - loss: 0.1785 - acc: 0.9390 - val_loss: 0.7726 - val_acc: 0.7160
Epoch 17/30
100/100 [==============================] - 71s 708ms/step - loss: 0.1579 - acc: 0.9480 - val_loss: 0.4888 - val_acc: 0.7150
Epoch 18/30
100/100 [==============================] - 71s 709ms/step - loss: 0.1399 - acc: 0.9545 - val_loss: 0.7219 - val_acc: 0.7130
Epoch 19/30
100/100 [==============================] - 71s 708ms/step - loss: 0.1229 - acc: 0.9580 - val_loss: 0.3776 - val_acc: 0.7180
Epoch 20/30
100/100 [==============================] - 71s 708ms/step - loss: 0.1002 - acc: 0.9735 - val_loss: 1.0687 - val_acc: 0.7240
Epoch 21/30
100/100 [==============================] - 71s 707ms/step - loss: 0.0841 - acc: 0.9785 - val_loss: 1.6117 - val_acc: 0.7270
Epoch 22/30
100/100 [==============================] - 71s 710ms/step - loss: 0.0753 - acc: 0.9840 - val_loss: 0.3944 - val_acc: 0.7240
Epoch 23/30
100/100 [==============================] - 71s 707ms/step - loss: 0.0649 - acc: 0.9825 - val_loss: 1.1620 - val_acc: 0.7240
Epoch 24/30
100/100 [==============================] - 71s 708ms/step - loss: 0.0503 - acc: 0.9905 - val_loss: 0.4844 - val_acc: 0.7260
Epoch 25/30
100/100 [==============================] - 71s 707ms/step - loss: 0.0464 - acc: 0.9875 - val_loss: 0.4670 - val_acc: 0.7080
Epoch 26/30
100/100 [==============================] - 71s 706ms/step - loss: 0.0362 - acc: 0.9945 - val_loss: 0.4580 - val_acc: 0.7140
Epoch 27/30
100/100 [==============================] - 71s 708ms/step - loss: 0.0324 - acc: 0.9920 - val_loss: 0.6673 - val_acc: 0.7240
Epoch 28/30
100/100 [==============================] - 71s 708ms/step - loss: 0.0211 - acc: 0.9970 - val_loss: 0.7170 - val_acc: 0.7260
Epoch 29/30
100/100 [==============================] - 71s 709ms/step - loss: 0.0195 - acc: 0.9965 - val_loss: 1.4715 - val_acc: 0.7250
Epoch 30/30
100/100 [==============================] - 71s 708ms/step - loss: 0.0137 - acc: 0.9965 - val_loss: 1.1098 - val_acc: 0.7010
</code></pre><p>始终在训练完成后保存模型，这是一种良好实践。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">model.save(<span class="string">'model/ComputerVersion/cats_and_dogs_small_1.h5'</span>)</span><br></pre></td></tr></table></figure>
<p>我们来分别绘制训练过程中模型在训练数据和验证数据上的损失和精度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">acc = history.history[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_acc'</span>]</span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line">epochs = range(<span class="number">1</span>, len(acc) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_29_0.png" width="400" height="400" alt="训练损失和验证损失" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练损失和验证损失</div>
</center>


<center>
    <img src="\Pic\DeepLearning_Pic\output_29_1.png" width="400" height="400" alt="训练精度和验证精度" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练精度和验证精度</div>
</center>



<p>从这些图像中都能看出过拟合的特征。训练精度随着时间线性增加，直到接近100%，而验证精度则停留在70%~72%。验证损失仅在5 轮后就达到最小值，然后保持不变，而训练损失则一直线性下降，直到接近于0。</p>
<p>因为训练样本相对较少（2000 个），所以过拟合是你最关心的问题。前面已经介绍过几种降低过拟合的技巧，比如dropout 和权重衰减（L2 正则化）。现在我们将使用一种针对于计算机视觉领域的新方法，在用深度学习模型处理图像时几乎都会用到这种方法，它就是数据增强（data augmentation）。</p>
<h2 id="6-使用数据增强"><a href="#6-使用数据增强" class="headerlink" title="6. 使用数据增强"></a>6. 使用数据增强</h2><p>过拟合的原因是学习样本太少，导致无法训练出能够泛化到新数据的模型。如果拥有无限的数据，那么模型能够观察到数据分布的所有内容，这样就永远不会过拟合。数据增强是从现有的训练样本中生成更多的训练数据，其方法是利用多种能够生成可信图像的随机变换来增加（augment）样本。其目标是，模型在训练时不会两次查看完全相同的图像。这让模型能够观察到数据的更多内容，从而具有更好的泛化能力。</p>
<p>在Keras 中，这可以通过对<code>ImageDataGenerator</code>实例读取的图像执行多次随机变换来实现。我们先来看一个例子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用ImageDataGenerator 来设置数据增强</span></span><br><span class="line">datagen = ImageDataGenerator(</span><br><span class="line">    rotation_range=<span class="number">40</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    shear_range=<span class="number">0.2</span>,</span><br><span class="line">    zoom_range=<span class="number">0.2</span>,</span><br><span class="line">    horizontal_flip=<span class="literal">True</span>,</span><br><span class="line">    fill_mode=<span class="string">'nearest'</span>)</span><br></pre></td></tr></table></figure>
<p>这里只选择了几个参数（想了解更多参数，请查阅Keras 文档）。我们来快速介绍一下这些<br>参数的含义。</p>
<ul>
<li><code>rotation_range</code>是角度值（在 0~180 范围内），表示图像随机旋转的角度范围。</li>
<li><code>width_shift</code> 和 <code>height_shift</code> 是图像在水平或垂直方向上平移的范围（相对于总宽度或总高度的比例）。</li>
<li><code>shear_range</code>是随机错切变换的角度。</li>
<li><code>zoom_range</code>是图像随机缩放的范围。</li>
<li><code>horizontal_flip</code> 是随机将一半图像水平翻转。如果没有水平不对称的假设（比如真实世界的图像），这种做法是有意义的。</li>
<li><code>fill_mode</code>是用于填充新创建像素的方法，这些新像素可能来自于旋转或宽度/高度平移。</li>
</ul>
<p>我们来看一下增强后的图像</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示几个随机增强后的训练图像</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line">fnames = [os.path.join(train_cats_dir, fname) <span class="keyword">for</span> fname <span class="keyword">in</span> os.listdir(train_cats_dir)]</span><br><span class="line">img_path = fnames[<span class="number">3</span>]</span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">150</span>, <span class="number">150</span>))</span><br><span class="line">x = image.img_to_array(img)</span><br><span class="line">x = x.reshape((<span class="number">1</span>,) + x.shape)</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> datagen.flow(x, batch_size=<span class="number">1</span>):</span><br><span class="line">    plt.figure(i)</span><br><span class="line">    imgplot = plt.imshow(image.array_to_img(batch[<span class="number">0</span>]))</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">4</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_33_0.png" width="400" height="400" alt="增强后的图像" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">增强后的图像（1）</div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_33_1.png" width="400" height="400" alt="增强后的图像" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">增强后的图像（2）</div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_33_2.png" width="400" height="400" alt="增强后的图像" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">增强后的图像（3）</div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_33_3.png" width="400" height="400" alt="增强后的图像" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">增强后的图像（4）</div>
</center>




<p>如果你使用这种数据增强来训练一个新网络，那么网络将不会两次看到同样的输入。但网络看到的输入仍然是高度相关的，因为这些输入都来自于少量的原始图像。你无法生成新信息，而只能混合现有信息。因此，这种方法可能不足以完全消除过拟合。为了进一步降低过拟合，你还需要向模型中添加一个<code>Dropout</code>层，添加到密集连接分类器之前。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>,input_shape=(<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>)))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,optimizer=optimizers.RMSprop(lr=<span class="number">1e-4</span>),metrics=[<span class="string">'acc'</span>])</span><br></pre></td></tr></table></figure>
<p>我们来训练这个使用了数据增强和dropout 的网络。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用数据增强生成器训练卷积神经网络</span></span><br><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">    rescale=<span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">    rotation_range=<span class="number">40</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    shear_range=<span class="number">0.2</span>,</span><br><span class="line">    zoom_range=<span class="number">0.2</span>,</span><br><span class="line">    horizontal_flip=<span class="literal">True</span>,)</span><br><span class="line"></span><br><span class="line">test_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">    train_dir,</span><br><span class="line">    target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">    batch_size=<span class="number">32</span>,</span><br><span class="line">    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">validation_generator = test_datagen.flow_from_directory(</span><br><span class="line">    validation_dir,</span><br><span class="line">    target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">    batch_size=<span class="number">32</span>,</span><br><span class="line">    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit_generator(</span><br><span class="line">    train_generator,</span><br><span class="line">    steps_per_epoch=<span class="number">100</span>,</span><br><span class="line">    epochs=<span class="number">100</span>,</span><br><span class="line">    validation_data=validation_generator,</span><br><span class="line">    validation_steps=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
Epoch 1/100
100/100 [==============================] - 76s 765ms/step - loss: 0.6914 - acc: 0.5183 - val_loss: 0.7206 - val_acc: 0.5032
Epoch 2/100
100/100 [==============================] - 72s 724ms/step - loss: 0.6739 - acc: 0.5729 - val_loss: 0.6612 - val_acc: 0.5921
Epoch 3/100
100/100 [==============================] - 72s 723ms/step - loss: 0.6692 - acc: 0.5701 - val_loss: 0.7034 - val_acc: 0.5374
Epoch 4/100
100/100 [==============================] - 72s 724ms/step - loss: 0.6496 - acc: 0.6080 - val_loss: 0.6402 - val_acc: 0.6269
Epoch 5/100
100/100 [==============================] - 75s 749ms/step - loss: 0.6313 - acc: 0.6408 - val_loss: 0.6454 - val_acc: 0.6650
Epoch 6/100
100/100 [==============================] - 77s 774ms/step - loss: 0.6112 - acc: 0.6555 - val_loss: 0.6192 - val_acc: 0.6823
Epoch 7/100
100/100 [==============================] - 76s 761ms/step - loss: 0.6064 - acc: 0.6585 - val_loss: 0.6323 - val_acc: 0.6853
Epoch 8/100
100/100 [==============================] - 77s 775ms/step - loss: 0.5988 - acc: 0.6724 - val_loss: 0.5059 - val_acc: 0.6804
Epoch 9/100
100/100 [==============================] - 75s 753ms/step - loss: 0.5842 - acc: 0.6926 - val_loss: 0.5870 - val_acc: 0.7139
Epoch 10/100
100/100 [==============================] - 77s 771ms/step - loss: 0.5765 - acc: 0.6954 - val_loss: 0.5300 - val_acc: 0.7157
Epoch 11/100
100/100 [==============================] - 77s 772ms/step - loss: 0.5766 - acc: 0.6869 - val_loss: 0.6192 - val_acc: 0.7004
Epoch 12/100
100/100 [==============================] - 74s 737ms/step - loss: 0.5625 - acc: 0.7048 - val_loss: 0.6041 - val_acc: 0.6916
Epoch 13/100
100/100 [==============================] - 73s 733ms/step - loss: 0.5667 - acc: 0.7014 - val_loss: 0.6984 - val_acc: 0.6849
Epoch 14/100
100/100 [==============================] - 80s 800ms/step - loss: 0.5551 - acc: 0.7142 - val_loss: 0.5477 - val_acc: 0.6859
Epoch 15/100
100/100 [==============================] - 74s 745ms/step - loss: 0.5481 - acc: 0.7263 - val_loss: 0.6152 - val_acc: 0.7139
Epoch 16/100
100/100 [==============================] - 75s 752ms/step - loss: 0.5450 - acc: 0.7155 - val_loss: 0.4643 - val_acc: 0.7088
Epoch 17/100
100/100 [==============================] - 75s 751ms/step - loss: 0.5368 - acc: 0.7225 - val_loss: 0.5045 - val_acc: 0.7259
Epoch 18/100
100/100 [==============================] - 77s 772ms/step - loss: 0.5334 - acc: 0.7305 - val_loss: 0.5234 - val_acc: 0.7384
Epoch 19/100
100/100 [==============================] - 73s 728ms/step - loss: 0.5276 - acc: 0.7446 - val_loss: 0.3467 - val_acc: 0.7284
Epoch 20/100
100/100 [==============================] - 72s 722ms/step - loss: 0.5252 - acc: 0.7346 - val_loss: 0.5238 - val_acc: 0.7506
Epoch 21/100
100/100 [==============================] - 73s 734ms/step - loss: 0.5319 - acc: 0.7304 - val_loss: 0.6062 - val_acc: 0.7189
Epoch 22/100
100/100 [==============================] - 74s 737ms/step - loss: 0.5140 - acc: 0.7434 - val_loss: 0.5258 - val_acc: 0.7500
Epoch 23/100
100/100 [==============================] - 76s 759ms/step - loss: 0.5001 - acc: 0.7538 - val_loss: 0.5850 - val_acc: 0.6681
Epoch 24/100
100/100 [==============================] - 75s 752ms/step - loss: 0.5181 - acc: 0.7345 - val_loss: 0.5819 - val_acc: 0.7429
Epoch 25/100
100/100 [==============================] - 73s 726ms/step - loss: 0.5130 - acc: 0.7421 - val_loss: 0.5566 - val_acc: 0.7519
Epoch 26/100
100/100 [==============================] - 74s 736ms/step - loss: 0.4970 - acc: 0.7513 - val_loss: 0.5302 - val_acc: 0.7468
Epoch 27/100
100/100 [==============================] - 75s 750ms/step - loss: 0.5017 - acc: 0.7617 - val_loss: 0.7022 - val_acc: 0.7597
Epoch 28/100
100/100 [==============================] - 74s 740ms/step - loss: 0.4944 - acc: 0.7601 - val_loss: 0.5136 - val_acc: 0.7602
Epoch 29/100
100/100 [==============================] - 76s 760ms/step - loss: 0.4971 - acc: 0.7575 - val_loss: 0.5346 - val_acc: 0.7539
Epoch 30/100
100/100 [==============================] - 73s 732ms/step - loss: 0.4934 - acc: 0.7582 - val_loss: 0.5304 - val_acc: 0.7253
Epoch 31/100
100/100 [==============================] - 75s 746ms/step - loss: 0.4920 - acc: 0.7557 - val_loss: 0.5527 - val_acc: 0.7410
Epoch 32/100
100/100 [==============================] - 74s 736ms/step - loss: 0.4881 - acc: 0.7676 - val_loss: 0.4412 - val_acc: 0.7622
Epoch 33/100
100/100 [==============================] - 74s 741ms/step - loss: 0.4879 - acc: 0.7594 - val_loss: 0.4859 - val_acc: 0.7773
Epoch 34/100
100/100 [==============================] - 73s 731ms/step - loss: 0.4791 - acc: 0.7775 - val_loss: 0.5089 - val_acc: 0.7809
Epoch 35/100
100/100 [==============================] - 76s 762ms/step - loss: 0.4783 - acc: 0.7718 - val_loss: 0.4133 - val_acc: 0.7862
Epoch 36/100
100/100 [==============================] - 75s 750ms/step - loss: 0.4723 - acc: 0.7754 - val_loss: 0.7177 - val_acc: 0.7371
Epoch 37/100
100/100 [==============================] - 72s 722ms/step - loss: 0.4684 - acc: 0.7705 - val_loss: 0.2175 - val_acc: 0.8014
Epoch 38/100
100/100 [==============================] - 74s 743ms/step - loss: 0.4617 - acc: 0.7784 - val_loss: 0.5956 - val_acc: 0.7655
Epoch 39/100
100/100 [==============================] - 74s 743ms/step - loss: 0.4610 - acc: 0.7811 - val_loss: 0.4567 - val_acc: 0.7970
Epoch 40/100
100/100 [==============================] - 76s 756ms/step - loss: 0.4573 - acc: 0.7828 - val_loss: 0.6099 - val_acc: 0.7816
Epoch 41/100
100/100 [==============================] - 73s 732ms/step - loss: 0.4521 - acc: 0.7883 - val_loss: 0.5290 - val_acc: 0.7809
Epoch 42/100
100/100 [==============================] - 72s 722ms/step - loss: 0.4563 - acc: 0.7854 - val_loss: 0.5905 - val_acc: 0.7938
Epoch 43/100
100/100 [==============================] - 75s 752ms/step - loss: 0.4538 - acc: 0.7822 - val_loss: 0.3387 - val_acc: 0.7919
Epoch 44/100
100/100 [==============================] - 76s 758ms/step - loss: 0.4508 - acc: 0.7893 - val_loss: 0.3386 - val_acc: 0.7766
Epoch 45/100
100/100 [==============================] - 76s 764ms/step - loss: 0.4392 - acc: 0.7981 - val_loss: 0.4522 - val_acc: 0.7539
Epoch 46/100
100/100 [==============================] - 75s 750ms/step - loss: 0.4415 - acc: 0.7904 - val_loss: 0.4479 - val_acc: 0.8039
Epoch 47/100
100/100 [==============================] - 75s 746ms/step - loss: 0.4431 - acc: 0.7996 - val_loss: 0.6296 - val_acc: 0.8015
Epoch 48/100
100/100 [==============================] - 74s 735ms/step - loss: 0.4337 - acc: 0.7842 - val_loss: 0.7442 - val_acc: 0.7899
Epoch 49/100
100/100 [==============================] - 73s 728ms/step - loss: 0.4492 - acc: 0.7895 - val_loss: 0.3889 - val_acc: 0.8090
Epoch 50/100
100/100 [==============================] - 73s 734ms/step - loss: 0.4369 - acc: 0.8011 - val_loss: 0.3906 - val_acc: 0.7719
Epoch 51/100
100/100 [==============================] - 73s 728ms/step - loss: 0.4203 - acc: 0.8106 - val_loss: 0.3646 - val_acc: 0.7303
Epoch 52/100
100/100 [==============================] - 75s 754ms/step - loss: 0.4243 - acc: 0.8056 - val_loss: 0.3277 - val_acc: 0.7957
Epoch 53/100
100/100 [==============================] - 74s 736ms/step - loss: 0.4288 - acc: 0.7986 - val_loss: 0.3046 - val_acc: 0.8096
Epoch 54/100
100/100 [==============================] - 73s 734ms/step - loss: 0.4203 - acc: 0.8046 - val_loss: 0.1641 - val_acc: 0.8189
Epoch 55/100
100/100 [==============================] - 73s 732ms/step - loss: 0.4296 - acc: 0.8065 - val_loss: 0.4410 - val_acc: 0.7963
Epoch 56/100
100/100 [==============================] - 74s 736ms/step - loss: 0.4264 - acc: 0.8112 - val_loss: 0.7351 - val_acc: 0.7751
Epoch 57/100
100/100 [==============================] - 74s 742ms/step - loss: 0.4208 - acc: 0.8081 - val_loss: 0.4422 - val_acc: 0.7899
Epoch 58/100
100/100 [==============================] - 73s 731ms/step - loss: 0.4186 - acc: 0.8059 - val_loss: 0.3010 - val_acc: 0.8230
Epoch 59/100
100/100 [==============================] - 72s 724ms/step - loss: 0.4123 - acc: 0.8078 - val_loss: 0.2892 - val_acc: 0.8061
Epoch 60/100
100/100 [==============================] - 74s 736ms/step - loss: 0.4048 - acc: 0.8211 - val_loss: 0.4229 - val_acc: 0.8039
Epoch 61/100
100/100 [==============================] - 72s 724ms/step - loss: 0.4085 - acc: 0.8147 - val_loss: 0.6638 - val_acc: 0.7932
Epoch 62/100
100/100 [==============================] - 74s 736ms/step - loss: 0.3915 - acc: 0.8229 - val_loss: 0.4043 - val_acc: 0.7824
Epoch 63/100
100/100 [==============================] - 73s 725ms/step - loss: 0.4047 - acc: 0.8112 - val_loss: 0.3251 - val_acc: 0.8035
Epoch 64/100
100/100 [==============================] - 72s 722ms/step - loss: 0.3958 - acc: 0.8185 - val_loss: 0.1415 - val_acc: 0.8125
Epoch 65/100
100/100 [==============================] - 73s 730ms/step - loss: 0.4035 - acc: 0.8021 - val_loss: 0.4303 - val_acc: 0.8103
Epoch 66/100
100/100 [==============================] - 72s 719ms/step - loss: 0.3885 - acc: 0.8141 - val_loss: 0.6505 - val_acc: 0.8054
Epoch 67/100
100/100 [==============================] - 73s 729ms/step - loss: 0.4019 - acc: 0.8169 - val_loss: 0.3303 - val_acc: 0.8122
Epoch 68/100
100/100 [==============================] - 72s 720ms/step - loss: 0.4013 - acc: 0.8147 - val_loss: 0.4759 - val_acc: 0.8247
Epoch 69/100
100/100 [==============================] - 74s 743ms/step - loss: 0.3903 - acc: 0.8178 - val_loss: 0.5856 - val_acc: 0.8128
Epoch 70/100
100/100 [==============================] - 72s 724ms/step - loss: 0.3819 - acc: 0.8292 - val_loss: 0.5976 - val_acc: 0.8086
Epoch 71/100
100/100 [==============================] - 72s 722ms/step - loss: 0.3844 - acc: 0.8229 - val_loss: 0.1998 - val_acc: 0.8185
Epoch 72/100
100/100 [==============================] - 75s 746ms/step - loss: 0.3849 - acc: 0.8251 - val_loss: 0.3054 - val_acc: 0.8325
Epoch 73/100
100/100 [==============================] - 73s 731ms/step - loss: 0.3930 - acc: 0.8207 - val_loss: 0.5404 - val_acc: 0.8138
Epoch 74/100
100/100 [==============================] - 74s 735ms/step - loss: 0.3792 - acc: 0.8314 - val_loss: 0.7166 - val_acc: 0.7919
Epoch 75/100
100/100 [==============================] - 73s 729ms/step - loss: 0.3747 - acc: 0.8273 - val_loss: 0.3519 - val_acc: 0.8099
Epoch 76/100
100/100 [==============================] - 72s 717ms/step - loss: 0.3782 - acc: 0.8323 - val_loss: 0.3318 - val_acc: 0.8109
Epoch 77/100
100/100 [==============================] - 74s 741ms/step - loss: 0.3732 - acc: 0.8336 - val_loss: 0.4652 - val_acc: 0.8247
Epoch 78/100
100/100 [==============================] - 72s 720ms/step - loss: 0.3614 - acc: 0.8386 - val_loss: 0.4755 - val_acc: 0.8325
Epoch 79/100
100/100 [==============================] - 75s 747ms/step - loss: 0.3750 - acc: 0.8330 - val_loss: 0.2174 - val_acc: 0.8086
Epoch 80/100
100/100 [==============================] - 73s 727ms/step - loss: 0.3593 - acc: 0.8386 - val_loss: 0.1871 - val_acc: 0.8286
Epoch 81/100
100/100 [==============================] - 75s 754ms/step - loss: 0.3712 - acc: 0.8365 - val_loss: 0.2686 - val_acc: 0.8128
Epoch 82/100
100/100 [==============================] - 75s 746ms/step - loss: 0.3682 - acc: 0.8387 - val_loss: 0.3900 - val_acc: 0.8138
Epoch 83/100
100/100 [==============================] - 74s 737ms/step - loss: 0.3644 - acc: 0.8403 - val_loss: 0.3761 - val_acc: 0.8306
Epoch 84/100
100/100 [==============================] - 77s 766ms/step - loss: 0.3391 - acc: 0.8488 - val_loss: 0.3467 - val_acc: 0.8318
Epoch 85/100
100/100 [==============================] - 75s 752ms/step - loss: 0.3486 - acc: 0.8482 - val_loss: 0.3446 - val_acc: 0.8261
Epoch 86/100
100/100 [==============================] - 76s 757ms/step - loss: 0.3519 - acc: 0.8483 - val_loss: 0.7064 - val_acc: 0.8293
Epoch 87/100
100/100 [==============================] - 76s 764ms/step - loss: 0.3520 - acc: 0.8381 - val_loss: 0.4178 - val_acc: 0.8242
Epoch 88/100
100/100 [==============================] - 73s 728ms/step - loss: 0.3543 - acc: 0.8453 - val_loss: 0.2981 - val_acc: 0.8267
Epoch 89/100
100/100 [==============================] - 75s 752ms/step - loss: 0.3465 - acc: 0.8527 - val_loss: 0.4845 - val_acc: 0.8363
Epoch 90/100
100/100 [==============================] - 73s 729ms/step - loss: 0.3451 - acc: 0.8387 - val_loss: 0.3519 - val_acc: 0.8319
Epoch 91/100
100/100 [==============================] - 74s 739ms/step - loss: 0.3562 - acc: 0.8401 - val_loss: 0.3915 - val_acc: 0.8299
Epoch 92/100
100/100 [==============================] - 77s 773ms/step - loss: 0.3336 - acc: 0.8516 - val_loss: 0.3190 - val_acc: 0.8344
Epoch 93/100
100/100 [==============================] - 81s 813ms/step - loss: 0.3436 - acc: 0.8427 - val_loss: 0.5416 - val_acc: 0.7938
Epoch 94/100
100/100 [==============================] - 80s 800ms/step - loss: 0.3318 - acc: 0.8535 - val_loss: 0.3949 - val_acc: 0.7912
Epoch 95/100
100/100 [==============================] - 78s 780ms/step - loss: 0.3330 - acc: 0.8589 - val_loss: 0.4805 - val_acc: 0.8222
Epoch 96/100
100/100 [==============================] - 83s 827ms/step - loss: 0.3341 - acc: 0.8533 - val_loss: 0.2260 - val_acc: 0.8273
Epoch 97/100
100/100 [==============================] - 78s 783ms/step - loss: 0.3212 - acc: 0.8621 - val_loss: 0.4926 - val_acc: 0.8338
Epoch 98/100
100/100 [==============================] - 83s 829ms/step - loss: 0.3245 - acc: 0.8526 - val_loss: 0.5733 - val_acc: 0.8177
Epoch 99/100
100/100 [==============================] - 87s 872ms/step - loss: 0.3156 - acc: 0.8628 - val_loss: 0.5779 - val_acc: 0.8204
Epoch 100/100
100/100 [==============================] - 84s 843ms/step - loss: 0.3205 - acc: 0.8589 - val_loss: 0.5135 - val_acc: 0.8479
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">model.save(<span class="string">'model/ComputerVersion/cats_and_dogs_small_2.h5'</span>)</span><br></pre></td></tr></table></figure>
<p>我们再次绘制结果，使用了数据增强和dropout 之后，模型不再过拟合：训练曲线紧紧跟随着验证曲线。现在的精度为82%，比未正则化的模型提高了15%（相对比例）。</p>
<p>通过进一步使用正则化方法以及调节网络参数（比如每个卷积层的过滤器个数或网络中的层数），你可以得到更高的精度，可以达到86%或87%。但只靠从头开始训练自己的卷积神经网络，再想提高精度就十分困难，因为可用的数据太少。想要在这个问题上进一步提高精度，下一步需要使用预训练的模型，这是接下来两节的重点。</p>
<h1 id="三、使用预训练的卷积神经网络"><a href="#三、使用预训练的卷积神经网络" class="headerlink" title="三、使用预训练的卷积神经网络"></a>三、使用预训练的卷积神经网络</h1><p>想要将深度学习应用于小型图像数据集，一种常用且非常高效的方法是使用预训练网络。预训练网络（pretrained network）是一个保存好的网络，之前已在大型数据集（通常是大规模图像分类任务）上训练好。如果这个原始数据集足够大且足够通用，那么预训练网络学到的特征的空间层次结构可以有效地作为视觉世界的通用模型，因此这些特征可用于各种不同的计算机视觉问题，即使这些新问题涉及的类别和原始任务完全不同。举个例子，你在ImageNet 上训练了一个网络（其类别主要是动物和日常用品），然后将这个训练好的网络应用于某个不相干的任务，比如在图像中识别家具。这种学到的特征在不同问题之间的可移植性，是深度学习与许多早期浅层学习方法相比的重要优势，它使得深度学习对小数据问题非常有效。</p>
<p>本例中，假设有一个在ImageNet 数据集（140 万张标记图像，1000 个不同的类别）上训练好的大型卷积神经网络。ImageNet 中包含许多动物类别，其中包括不同种类的猫和狗，因此可以认为它在猫狗分类问题上也能有良好的表现。我们将使用VGG16 架构，它由Karen Simonyan 和Andrew Zisserman 在2014 年开发a。对于ImageNet，它是一种简单而又广泛使用的卷积神经网络架构。虽然VGG16 是一个比较旧的模型，性能远比不了当前最先进的模型，而且还比许多新模型更为复杂，但我之所以选择它，是因为它的架构与你已经熟悉的架构很相似，因此无须引入新概念就可以很好地理解。这可能是你第一次遇到这种奇怪的模型名称——VGG、ResNet、Inception、Inception-ResNet、Xception 等。你会习惯这些名称的，因为如果你一直用深度学习做计算机视觉的话，它们会频繁出现。使用预训练网络有两种方法：<strong>特征提取</strong>（feature extraction）和<strong>微调模型</strong>（fine-tuning）。两种方法我们都会介绍。首先来看特征提取。</p>
<h2 id="1-特征提取"><a href="#1-特征提取" class="headerlink" title="1. 特征提取"></a>1. 特征提取</h2><p>特征提取是使用之前网络学到的表示来从新样本中提取出有趣的特征。然后将这些特征输入一个新的分类器，从头开始训练。</p>
<p>如前所述，用于图像分类的卷积神经网络包含两部分：首先是一系列池化层和卷积层，最后是一个密集连接分类器。第一部分叫作模型的卷积基（convolutional base）。对于卷积神经网络而言，特征提取就是取出之前训练好的网络的卷积基，在上面运行新数据，然后在输出上面训练一个新的分类器</p>
<center>
    <img src="\Pic\DeepLearning_Pic\预训练模型1.png" width="300" height="300" alt="保持卷积基不变，改变分类器" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">保持卷积基不变，改变分类器</div>
</center>

<p>为什么仅重复使用卷积基？我们能否也重复使用密集连接分类器？一般来说，应该避免这么做。原因在于卷积基学到的表示可能更加通用，因此更适合重复使用。卷积神经网络的特征图表示通用概念在图像中是否存在，无论面对什么样的计算机视觉问题，这种特征图都可能很有用。但是，分类器学到的表示必然是针对于模型训练的类别，其中仅包含某个类别出现在整张图像中的概率信息。此外，密集连接层的表示不再包含物体在输入图像中的位置信息。密集连接层舍弃了空间的概念，而物体位置信息仍然由卷积特征图所描述。如果物体位置对于问题很重要，那么密集连接层的特征在很大程度上是无用的。</p>
<p>注意，某个卷积层提取的表示的通用性（以及可复用性）取决于该层在模型中的深度。模型中更靠近底部的层提取的是局部的、高度通用的特征图（比如视觉边缘、颜色和纹理），而更靠近顶部的层提取的是更加抽象的概念（比如“猫耳朵”或“狗眼睛”）。因此，如果你的新数据集与原始模型训练的数据集有很大差异，那么最好只使用模型的前几层来做特征提取，而不是使用整个卷积基。</p>
<p>本例中，由于ImageNet的类别中包含多种狗和猫的类别，所以重复使用原始模型密集连接层中所包含的信息可能很有用。但我们选择不这么做，以便涵盖新问题的类别与原始模型的类别不一致的更一般情况。我们来实践一下，使用在ImageNet上训练的VGG16 网络的卷积基从猫狗图像中提取有趣的特征，然后在这些特征上训练一个猫狗分类器。VGG16 等模型内置于Keras 中。你可以从<code>keras.applications</code>模块中导入。下面是<code>keras.applications</code>中的一部分图像分类模型（都是在ImageNet数据集上预训练得到的）：</p>
<ul>
<li>Xception</li>
<li>Inception V3</li>
<li>ResNet50</li>
<li>VGG16</li>
<li>VGG19</li>
<li>MobileNet</li>
</ul>
<p>我们将<code>VGG16</code>模型实例化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将 VGG16 卷积基实例化</span></span><br><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> VGG16</span><br><span class="line">conv_base = VGG1616(weights=<span class="string">'imagenet'</span>, include_top=<span class="literal">False</span>, input_shape=(<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<p>这里向构造函数中传入了三个参数。</p>
<ul>
<li>weights指定模型初始化的权重检查点。</li>
<li>include_top 指定模型最后是否包含密集连接分类器。默认情况下，这个密集连接分类器对应于ImageNet的1000个类别。因为我们打算使用自己的密集连接分类器（只有两个类别：cat和dog），所以不需要包含它。</li>
<li>input_shape是输入到网络中的图像张量的形状。这个参数完全是可选的，如果不传入这个参数，那么网络能够处理任意形状的输入。VGG16卷积基的详细架构如下所示。它和你已经熟悉的简单卷积神经网络很相似。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv_base.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;vgg16&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>最后的特征图形状为(4, 4, 512)。我们将在这个特征上添加一个密集连接分类器。接下来，下一步有两种方法可供选择：</p>
<ul>
<li>在你的数据集上运行卷积基，将输出保存成硬盘中的Numpy数组，然后用这个数据作为输入，输入到独立的密集连接分类器中。这种方法速度快，计算代价低，因为对于每个输入图像只需运行一次卷积基，而卷积基是目前流程中计算代价最高的。但出于同样的原因，这种方法不允许你使用数据增强。</li>
<li>在顶部添加<code>Dense</code>层来扩展已有模型（即<code>conv_base</code>），并在输入数据上端到端地运行整个模型。这样你可以使用数据增强，因为每个输入图像进入模型时都会经过卷积基。但出于同样的原因，这种方法的计算代价比第一种要高很多。</li>
</ul>
<h2 id="2-不使用数据增强的快速特征提取"><a href="#2-不使用数据增强的快速特征提取" class="headerlink" title="2. 不使用数据增强的快速特征提取"></a>2. 不使用数据增强的快速特征提取</h2><p>首先，运行<code>ImageDataGenerator</code>实例，将图像及其标签提取为<code>Numpy</code>数组。我们需要调用<code>conv_base</code>模型的<code>predict</code>方法来从这些图像中提取特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用预训练的卷积基提取特征</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line">base_dir = <span class="string">'data/cat_dog/cats_and_dogs_small'</span></span><br><span class="line">train_dir = os.path.join(base_dir, <span class="string">'train'</span>)</span><br><span class="line">validation_dir = os.path.join(base_dir, <span class="string">'validation'</span>)</span><br><span class="line">test_dir = os.path.join(base_dir, <span class="string">'test'</span>)</span><br><span class="line">datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_features</span><span class="params">(directory, sample_count)</span>:</span></span><br><span class="line">    features = np.zeros(shape=(sample_count, <span class="number">4</span>, <span class="number">4</span>, <span class="number">512</span>))</span><br><span class="line">    labels = np.zeros(shape=(sample_count))</span><br><span class="line">    generator = datagen.flow_from_directory(</span><br><span class="line">        directory,</span><br><span class="line">        target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        class_mode=<span class="string">'binary'</span>)</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> inputs_batch, labels_batch <span class="keyword">in</span> generator:</span><br><span class="line">        features_batch = conv_base.predict(inputs_batch)</span><br><span class="line">        features[i * batch_size : (i + <span class="number">1</span>) * batch_size] = features_batch</span><br><span class="line">        labels[i * batch_size : (i + <span class="number">1</span>) * batch_size] = labels_batch</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i * batch_size &gt;= sample_count:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line"></span><br><span class="line">train_features, train_labels = extract_features(train_dir, <span class="number">2000</span>)</span><br><span class="line">validation_features, validation_labels = extract_features(validation_dir, <span class="number">1000</span>)</span><br><span class="line">test_features, test_labels = extract_features(test_dir, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
</code></pre><p>目前，提取的特征形状为<code>(samples, 4, 4, 512)</code>。我们要将其输入到密集连接分类器中，所以首先必须将其形状展平为<code>(samples, 8192)</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_features = np.reshape(train_features, (<span class="number">2000</span>, <span class="number">4</span> * <span class="number">4</span> * <span class="number">512</span>))</span><br><span class="line">validation_features = np.reshape(validation_features, (<span class="number">1000</span>, <span class="number">4</span> * <span class="number">4</span> * <span class="number">512</span>))</span><br><span class="line">test_features = np.reshape(test_features, (<span class="number">1000</span>, <span class="number">4</span> * <span class="number">4</span> * <span class="number">512</span>))</span><br></pre></td></tr></table></figure>
<p>现在你可以定义你的密集连接分类器（注意要使用<code>dropout</code>正则化），并在刚刚保存的数据和标签上训练这个分类器。</p>
<h3 id="定义并训练密集连接分类器"><a href="#定义并训练密集连接分类器" class="headerlink" title="定义并训练密集连接分类器"></a>定义并训练密集连接分类器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>, input_dim=<span class="number">4</span> * <span class="number">4</span> * <span class="number">512</span>))</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.compile(optimizer=optimizers.RMSprop(lr=<span class="number">2e-5</span>),</span><br><span class="line">    loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">    metrics=[<span class="string">'acc'</span>])</span><br><span class="line">history = model.fit(train_features, train_labels,</span><br><span class="line">    epochs=<span class="number">30</span>,</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">    validation_data=(validation_features, validation_labels))</span><br></pre></td></tr></table></figure>
<pre><code>Train on 2000 samples, validate on 1000 samples
Epoch 1/30
2000/2000 [==============================] - 4s 2ms/step - loss: 0.6064 - acc: 0.6600 - val_loss: 0.4430 - val_acc: 0.8350
Epoch 2/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.4158 - acc: 0.8210 - val_loss: 0.3588 - val_acc: 0.8610
Epoch 3/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.3449 - acc: 0.8585 - val_loss: 0.3250 - val_acc: 0.8770
Epoch 4/30
2000/2000 [==============================] - 4s 2ms/step - loss: 0.3145 - acc: 0.8645 - val_loss: 0.3024 - val_acc: 0.8840
Epoch 5/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.2900 - acc: 0.8800 - val_loss: 0.2865 - val_acc: 0.8910
Epoch 6/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.2563 - acc: 0.8995 - val_loss: 0.2808 - val_acc: 0.8860
Epoch 7/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.2441 - acc: 0.9035 - val_loss: 0.2654 - val_acc: 0.8950
Epoch 8/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.2302 - acc: 0.9185 - val_loss: 0.2627 - val_acc: 0.8950
Epoch 9/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.2226 - acc: 0.9095 - val_loss: 0.2599 - val_acc: 0.8930
Epoch 10/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1979 - acc: 0.9275 - val_loss: 0.2521 - val_acc: 0.9010
Epoch 11/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1949 - acc: 0.9305 - val_loss: 0.2537 - val_acc: 0.8940
Epoch 12/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1872 - acc: 0.9390 - val_loss: 0.2518 - val_acc: 0.8990
Epoch 13/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1781 - acc: 0.9275 - val_loss: 0.2451 - val_acc: 0.8980
Epoch 14/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1662 - acc: 0.9430 - val_loss: 0.2477 - val_acc: 0.9060
Epoch 15/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1632 - acc: 0.9450 - val_loss: 0.2449 - val_acc: 0.8980
Epoch 16/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1530 - acc: 0.9480 - val_loss: 0.2389 - val_acc: 0.9030
Epoch 17/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1477 - acc: 0.9465 - val_loss: 0.2454 - val_acc: 0.9010
Epoch 18/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1434 - acc: 0.9525 - val_loss: 0.2463 - val_acc: 0.8990
Epoch 19/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1346 - acc: 0.9550 - val_loss: 0.2390 - val_acc: 0.9020
Epoch 20/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1305 - acc: 0.9595 - val_loss: 0.2374 - val_acc: 0.9000
Epoch 21/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1216 - acc: 0.9630 - val_loss: 0.2430 - val_acc: 0.9000
Epoch 22/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1166 - acc: 0.9625 - val_loss: 0.2381 - val_acc: 0.9030
Epoch 23/30
2000/2000 [==============================] - 4s 2ms/step - loss: 0.1103 - acc: 0.9685 - val_loss: 0.2373 - val_acc: 0.9030
Epoch 24/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1112 - acc: 0.9635 - val_loss: 0.2460 - val_acc: 0.9020
Epoch 25/30
2000/2000 [==============================] - 4s 2ms/step - loss: 0.1067 - acc: 0.9675 - val_loss: 0.2380 - val_acc: 0.8990
Epoch 26/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.1004 - acc: 0.9695 - val_loss: 0.2378 - val_acc: 0.9000
Epoch 27/30
2000/2000 [==============================] - 4s 2ms/step - loss: 0.0994 - acc: 0.9680 - val_loss: 0.2450 - val_acc: 0.9030
Epoch 28/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.0947 - acc: 0.9710 - val_loss: 0.2396 - val_acc: 0.9010
Epoch 29/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.0870 - acc: 0.9770 - val_loss: 0.2416 - val_acc: 0.9020
Epoch 30/30
2000/2000 [==============================] - 3s 2ms/step - loss: 0.0870 - acc: 0.9730 - val_loss: 0.2473 - val_acc: 0.9010
</code></pre><p>训练速度非常快，因为你只需处理两个Dense 层。我们来看一下训练期间的损失曲线和精度曲线：</p>
<h3 id="绘制结果"><a href="#绘制结果" class="headerlink" title="绘制结果"></a>绘制结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">acc = history.history[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_acc'</span>]</span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">epochs = range(<span class="number">1</span>, len(acc) + <span class="number">1</span>)</span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_51_0.png" width="400" height="400" alt="训练损失和验证损失" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练损失和验证损失</div>
</center>


<center>
    <img src="\Pic\DeepLearning_Pic\output_51_1.png" width="400" height="400" alt="训练精度和验证精度" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练精度和验证精度</div>
</center>


<p>我们的验证精度达到了约90%，比上一节从头开始训练的小型模型效果要好得多。但从图中也可以看出，虽然<code>dropout</code>比率相当大，但模型几乎从一开始就过拟合。这是因为本方法没有使用数据增强，而数据增强对防止小型图像数据集的过拟合非常重要。</p>
<h2 id="3-使用数据增强的特征提取"><a href="#3-使用数据增强的特征提取" class="headerlink" title="3. 使用数据增强的特征提取"></a>3. 使用数据增强的特征提取</h2><p>下面我们来看一下特征提取的第二种方法，它的速度更慢，计算代价更高，但在训练期间可以使用数据增强。这种方法就是：扩展<code>conv_base</code>模型，然后在输入数据上端到端地运行模型。</p>
<blockquote>
<p>注意 本方法计算代价很高，只在有GPU的情况下才能尝试运行。它在CPU上是绝对难以运行的。如果你无法在GPU上运行代码，那么就采用第一种方法。</p>
</blockquote>
<p>模型的行为和层类似，所以你可以向<code>Sequential</code>模型中添加一个模型（比如<code>conv_base</code>），就像添加一个层一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在卷积基上添加一个密集连接分类器</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(conv_base)</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>
<p>现在模型的架构如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_7&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
vgg16 (Model)                (None, 4, 4, 512)         14714688  
_________________________________________________________________
flatten_4 (Flatten)          (None, 8192)              0         
_________________________________________________________________
dense_9 (Dense)              (None, 256)               2097408   
_________________________________________________________________
dense_10 (Dense)             (None, 1)                 257       
=================================================================
Total params: 16,812,353
Trainable params: 16,812,353
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>如你所见，<code>VGG16</code>的卷积基有14 714 688个参数，非常多。在其上添加的分类器有200万个参数。</p>
<p>在编译和训练模型之前，一定要“冻结”卷积基。冻结（<code>freeze</code>）一个或多个层是指在训练过程中保持其权重不变。如果不这么做，那么卷积基之前学到的表示将会在训练过程中被修改。因为其上添加的<code>Dense</code>层是随机初始化的，所以非常大的权重更新将会在网络中传播，对之前学到的表示造成很大破坏。</p>
<p>在<code>Keras</code>中，冻结网络的方法是将其<code>trainable</code>属性设为<code>False</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'This is the number of trainable weights before freezing the conv base:'</span>, len(model.trainable_weights))</span><br><span class="line">conv_base.trainable = <span class="literal">False</span></span><br><span class="line">print(<span class="string">'This is the number of trainable weights after freezing the conv base:'</span>, len(model.trainable_weights))</span><br></pre></td></tr></table></figure>
<pre><code>This is the number of trainable weights before freezing the conv base: 30
This is the number of trainable weights after freezing the conv base: 4
</code></pre><p>如此设置之后，只有添加的两个Dense 层的权重才会被训练。总共有4 个权重张量，每层 2 个（主权重矩阵和偏置向量）。注意，为了让这些修改生效，你必须先编译模型。如果在编译之后修改了权重的<code>trainable</code>属性，那么应该重新编译模型，否则这些修改将被忽略。</p>
<p>现在你可以开始训练模型了，使用和前一个例子相同的数据增强设置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用冻结的卷积基端到端地训练模型</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line"></span><br><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">    rescale=<span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">    rotation_range=<span class="number">40</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    shear_range=<span class="number">0.2</span>,</span><br><span class="line">    zoom_range=<span class="number">0.2</span>,</span><br><span class="line">    horizontal_flip=<span class="literal">True</span>,</span><br><span class="line">    fill_mode=<span class="string">'nearest'</span>)</span><br><span class="line"></span><br><span class="line">test_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">    train_dir,</span><br><span class="line">    target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">validation_generator = test_datagen.flow_from_directory(</span><br><span class="line">    validation_dir,</span><br><span class="line">    target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">2e-5</span>),</span><br><span class="line">    metrics=[<span class="string">'acc'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit_generator(</span><br><span class="line">    train_generator,</span><br><span class="line">    steps_per_epoch=<span class="number">100</span>,</span><br><span class="line">    epochs=<span class="number">30</span>,</span><br><span class="line">    validation_data=validation_generator,</span><br><span class="line">    validation_steps=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/30
100/100 [==============================] - 154s 2s/step - loss: 0.3479 - acc: 0.8495 - val_loss: 0.1390 - val_acc: 0.8900
Epoch 2/30
100/100 [==============================] - 165s 2s/step - loss: 0.3448 - acc: 0.8560 - val_loss: 0.2075 - val_acc: 0.9010
Epoch 3/30
100/100 [==============================] - 178s 2s/step - loss: 0.3488 - acc: 0.8435 - val_loss: 0.2230 - val_acc: 0.9020
Epoch 4/30
100/100 [==============================] - 176s 2s/step - loss: 0.3232 - acc: 0.8555 - val_loss: 0.1981 - val_acc: 0.9030
Epoch 5/30
100/100 [==============================] - 166s 2s/step - loss: 0.3177 - acc: 0.8700 - val_loss: 0.3433 - val_acc: 0.8960
Epoch 6/30
100/100 [==============================] - 166s 2s/step - loss: 0.3232 - acc: 0.8630 - val_loss: 0.3095 - val_acc: 0.8990
Epoch 7/30
100/100 [==============================] - 170s 2s/step - loss: 0.3226 - acc: 0.8575 - val_loss: 0.1717 - val_acc: 0.9000
Epoch 8/30
100/100 [==============================] - 166s 2s/step - loss: 0.2967 - acc: 0.8735 - val_loss: 0.1980 - val_acc: 0.9020
Epoch 9/30
100/100 [==============================] - 165s 2s/step - loss: 0.3155 - acc: 0.8650 - val_loss: 0.5357 - val_acc: 0.8940
Epoch 10/30
100/100 [==============================] - 164s 2s/step - loss: 0.3109 - acc: 0.8570 - val_loss: 0.1268 - val_acc: 0.9020
Epoch 11/30
100/100 [==============================] - 164s 2s/step - loss: 0.3071 - acc: 0.8620 - val_loss: 0.4099 - val_acc: 0.8990
Epoch 12/30
100/100 [==============================] - 167s 2s/step - loss: 0.3166 - acc: 0.8585 - val_loss: 0.1766 - val_acc: 0.9000
Epoch 13/30
100/100 [==============================] - 163s 2s/step - loss: 0.2989 - acc: 0.8740 - val_loss: 0.1520 - val_acc: 0.9050
Epoch 14/30
100/100 [==============================] - 163s 2s/step - loss: 0.2943 - acc: 0.8730 - val_loss: 0.2935 - val_acc: 0.9010
Epoch 15/30
100/100 [==============================] - 163s 2s/step - loss: 0.2965 - acc: 0.8730 - val_loss: 0.5893 - val_acc: 0.9000
Epoch 16/30
100/100 [==============================] - 164s 2s/step - loss: 0.2848 - acc: 0.8775 - val_loss: 0.1792 - val_acc: 0.9070
Epoch 17/30
100/100 [==============================] - 166s 2s/step - loss: 0.2841 - acc: 0.8810 - val_loss: 0.2951 - val_acc: 0.9040
Epoch 18/30
100/100 [==============================] - 165s 2s/step - loss: 0.3002 - acc: 0.8650 - val_loss: 0.2552 - val_acc: 0.9030
Epoch 19/30
100/100 [==============================] - 165s 2s/step - loss: 0.3013 - acc: 0.8690 - val_loss: 0.4531 - val_acc: 0.8970
Epoch 20/30
100/100 [==============================] - 164s 2s/step - loss: 0.2898 - acc: 0.8750 - val_loss: 0.2342 - val_acc: 0.9070
Epoch 21/30
100/100 [==============================] - 165s 2s/step - loss: 0.2806 - acc: 0.8815 - val_loss: 0.2162 - val_acc: 0.9050
Epoch 22/30
100/100 [==============================] - 165s 2s/step - loss: 0.2854 - acc: 0.8750 - val_loss: 0.3686 - val_acc: 0.9040
Epoch 23/30
100/100 [==============================] - 165s 2s/step - loss: 0.2948 - acc: 0.8635 - val_loss: 0.2450 - val_acc: 0.9110
Epoch 24/30
100/100 [==============================] - 175s 2s/step - loss: 0.2738 - acc: 0.8790 - val_loss: 0.1684 - val_acc: 0.9100
Epoch 25/30
100/100 [==============================] - 170s 2s/step - loss: 0.2792 - acc: 0.8765 - val_loss: 0.1878 - val_acc: 0.9080
Epoch 26/30
100/100 [==============================] - 167s 2s/step - loss: 0.2687 - acc: 0.8855 - val_loss: 0.3175 - val_acc: 0.9080
Epoch 27/30
100/100 [==============================] - 172s 2s/step - loss: 0.2655 - acc: 0.8930 - val_loss: 0.1548 - val_acc: 0.9090
Epoch 28/30
100/100 [==============================] - 179s 2s/step - loss: 0.2628 - acc: 0.8910 - val_loss: 0.1822 - val_acc: 0.9020
Epoch 29/30
100/100 [==============================] - 177s 2s/step - loss: 0.2766 - acc: 0.8720 - val_loss: 0.2272 - val_acc: 0.9110
Epoch 30/30
100/100 [==============================] - 176s 2s/step - loss: 0.2676 - acc: 0.8815 - val_loss: 0.1242 - val_acc: 0.9120
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save(<span class="string">'model/ComputerVersion/cats_and_dogs_small_dataEnforcementFeatureExtraction.h5'</span>)</span><br></pre></td></tr></table></figure>
<p>如你所见，这比从头开始训练的小型卷积神经网络要好得多。</p>
<h2 id="4-微调模型"><a href="#4-微调模型" class="headerlink" title="4. 微调模型"></a>4. 微调模型</h2><p>另一种广泛使用的模型复用方法是模型微调（fine-tuning），与特征提取互为补充。对于用于特征提取的冻结的模型基，微调是指将其顶部的几层“解冻”，并将这解冻的几层和新增加的部分（本例中是全连接分类器）联合训练。之所以叫作微调，是因为它只是略微调整了所复用模型中更加抽象的表示，以便让这些表示与手头的问题更加相关。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\fine_tuning.png" width="200" height="200" alt="微调VGG16网络的最后一个卷积块" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">微调VGG16网络的最后一个卷积块</div>
</center>

<p>前面说过，冻结VGG16的卷积基是为了能够在上面训练一个随机初始化的分类器。同理，只有上面的分类器已经训练好了，才能微调卷积基的顶部几层。如果分类器没有训练好，那么训练期间通过网络传播的误差信号会特别大，微调的几层之前学到的表示都会被破坏。因此，微调网络的步骤如下。</p>
<ol>
<li>在已经训练好的基网络（base network）上添加自定义网络。</li>
<li>冻结基网络。</li>
<li>训练所添加的部分。</li>
<li>解冻基网络的一些层。</li>
<li>联合训练解冻的这些层和添加的部分。</li>
</ol>
<p>你在做特征提取时已经完成了前三个步骤。我们继续进行第四步：先解冻<code>conv_base</code>，然后冻结其中的部分层。提醒一下，卷积基的架构如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv_base.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;vgg16&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         
=================================================================
Total params: 14,714,688
Trainable params: 7,079,424
Non-trainable params: 7,635,264
_________________________________________________________________
</code></pre><p>我们将微调最后三个卷积层，也就是说，直到<code>block4_pool</code>的所有层都应该被冻结，而<code>block5_conv1</code>、<code>block5_conv2</code>和<code>block5_conv3</code>三层应该是可训练的。为什么不微调更多层？为什么不微调整个卷积基？你当然可以这么做，但需要考虑以下几点。</p>
<ul>
<li>卷积基中更靠底部的层编码的是更加通用的可复用特征，而更靠顶部的层编码的是更专业化的特征。微调这些更专业化的特征更加有用，因为它们需要在你的新问题上改变用途。微调更靠底部的层，得到的回报会更少。</li>
<li>训练的参数越多，过拟合的风险越大。卷积基有 1500 万个参数，所以在你的小型数据集上训练这么多参数是有风险的。</li>
</ul>
<p>因此，在这种情况下，一个好策略是仅微调卷积基最后的两三层。我们从上一个例子结束的地方开始，继续实现此方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 冻结直到某一层的所有层</span></span><br><span class="line">conv_base.trainable = <span class="literal">True</span></span><br><span class="line">set_trainable = <span class="literal">False</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> conv_base.layers:</span><br><span class="line">    <span class="keyword">if</span> layer.name == <span class="string">'block5_conv1'</span>:</span><br><span class="line">        set_trainable = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> set_trainable:</span><br><span class="line">        layer.trainable = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        layer.trainable = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>现在你可以开始微调网络。我们将使用学习率非常小的RMSProp优化器来实现。之所以让学习率很小，是因为对于微调的三层表示，我们希望其变化范围不要太大，太大的权重更新可能会破坏这些表示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 微调模型</span></span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">    optimizer=optimizers.RMSprop(lr=<span class="number">1e-5</span>),</span><br><span class="line">    metrics=[<span class="string">'acc'</span>])</span><br><span class="line">history = model.fit_generator(</span><br><span class="line">    train_generator,</span><br><span class="line">    steps_per_epoch=<span class="number">100</span>,</span><br><span class="line">    epochs=<span class="number">100</span>,</span><br><span class="line">    validation_data=validation_generator,</span><br><span class="line">    validation_steps=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save(<span class="string">'model/ComputerVersion/cats_and_dogs_small_dataEnforcementFineTuning.h5'</span>)</span><br></pre></td></tr></table></figure>
<p>我们用和前面一样的绘图代码来绘制结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">acc = history.history[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_acc'</span>]</span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">epochs = range(<span class="number">1</span>, len(acc) + <span class="number">1</span>)</span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_70_0.png" width="400" height="400" alt="训练精度和验证精度" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练精度和验证精度</div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_70_1.png" width="400" height="400" alt="训练损失和验证损失" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练损失和验证损失</div>
</center>




<p>这些曲线看起来包含噪声。为了让图像更具可读性，你可以将每个损失和精度都替换为指数移动平均值，从而让曲线变得平滑。下面用一个简单的实用函数来实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使曲线变得平滑</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smooth_curve</span><span class="params">(points, factor=<span class="number">0.8</span>)</span>:</span></span><br><span class="line">    smoothed_points = []</span><br><span class="line">    <span class="keyword">for</span> point <span class="keyword">in</span> points:</span><br><span class="line">        <span class="keyword">if</span> smoothed_points:</span><br><span class="line">            previous = smoothed_points[<span class="number">-1</span>]</span><br><span class="line">            smoothed_points.append(previous * factor + point * (<span class="number">1</span> - factor))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            smoothed_points.append(point)</span><br><span class="line">    <span class="keyword">return</span> smoothed_points</span><br><span class="line"></span><br><span class="line">plt.plot(epochs,smooth_curve(acc), <span class="string">'bo'</span>, label=<span class="string">'Smoothed training acc'</span>)</span><br><span class="line">plt.plot(epochs,smooth_curve(val_acc), <span class="string">'b'</span>, label=<span class="string">'Smoothed validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(epochs,smooth_curve(loss), <span class="string">'bo'</span>, label=<span class="string">'Smoothed training loss'</span>)</span><br><span class="line">plt.plot(epochs,smooth_curve(val_loss), <span class="string">'b'</span>, label=<span class="string">'Smoothed validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_72_0.png" width="400" height="400" alt="训练精度和验证精度" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">平滑后的训练精度和验证精度</div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_72_1.png" width="400" height="400" alt="训练损失和验证损失" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">平滑后的训练损失和验证损失</div>
</center>


<p>注意，从损失曲线上看不出与之前相比有任何真正的提高（实际上还在变差）。你可能感到奇怪，如果损失没有降低，那么精度怎么能保持稳定或提高呢？答案很简单：图中展示的是逐点（pointwise）损失值的平均值，但影响精度的是损失值的分布，而不是平均值，因为精度是模型预测的类别概率的二进制阈值。即使从平均损失中无法看出，但模型也仍然可能在改进。现在，你可以在测试数据上最终评估这个模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">test_generator = test_datagen.flow_from_directory(</span><br><span class="line">    test_dir,</span><br><span class="line">    target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">    class_mode=<span class="string">'binary'</span>)</span><br><span class="line">test_loss, test_acc = model.evaluate_generator(test_generator, steps=<span class="number">50</span>)</span><br><span class="line">print(<span class="string">'test acc:'</span>, test_acc)</span><br></pre></td></tr></table></figure>
<pre><code>Found 1000 images belonging to 2 classes.
test acc: 0.9399999976158142
</code></pre><h1 id="四、卷积神经网络的可视化"><a href="#四、卷积神经网络的可视化" class="headerlink" title="四、卷积神经网络的可视化"></a>四、卷积神经网络的可视化</h1><p>人们常说，深度学习模型是“黑盒”，即模型学到的表示很难用人类可以理解的方式来提取和呈现。虽然对于某些类型的深度学习模型来说，这种说法部分正确，但对卷积神经网络来说绝对不是这样。卷积神经网络学到的表示非常适合可视化，很大程度上是因为它们是视觉概念的表示。自2013 年以来，人们开发了多种技术来对这些表示进行可视化和解释。我们不会全部介绍，但会介绍三种最容易理解也最有用的方法。</p>
<ul>
<li>可视化卷积神经网络的中间输出（中间激活）：有助于理解卷积神经网络连续的层如何对输入进行变换，也有助于初步了解卷积神经网络每个过滤器的含义。</li>
<li>可视化卷积神经网络的过滤器：有助于精确理解卷积神经网络中每个过滤器容易接受的视觉模式或视觉概念。</li>
<li>可视化图像中类激活的热力图：有助于理解图像的哪个部分被识别为属于某个类别，从而可以定位图像中的物体。</li>
</ul>
<p>对于第一种方法（即激活的可视化），我们将使用猫狗分类问题上从头开始训练的小型卷积神经网络。对于另外两种可视化方法，我们将使用VGG16模型。</p>
<h2 id="1-可视化中间激活"><a href="#1-可视化中间激活" class="headerlink" title="1. 可视化中间激活"></a>1. 可视化中间激活</h2><p>可视化中间激活，是指对于给定输入，展示网络中各个卷积层和池化层输出的特征图（层的输出通常被称为该层的激活，即激活函数的输出）。这让我们可以看到输入如何被分解为网络学到的不同过滤器。我们希望在三个维度对特征图进行可视化：宽度、高度和深度（通道）。每个通道都对应相对独立的特征，所以将这些特征图可视化的正确方法是将每个通道的内容分别绘制成二维图像。我们首先来加载先前保存的模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line">model = load_model(<span class="string">'model/ComputerVersion/cats_and_dogs_small_2.h5'</span>)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_5&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_11 (Conv2D)           (None, 148, 148, 32)      896       
_________________________________________________________________
max_pooling2d_7 (MaxPooling2 (None, 74, 74, 32)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 72, 72, 64)        18496     
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_9 (MaxPooling2 (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_10 (MaxPooling (None, 7, 7, 128)         0         
_________________________________________________________________
flatten_3 (Flatten)          (None, 6272)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 6272)              0         
_________________________________________________________________
dense_5 (Dense)              (None, 512)               3211776   
_________________________________________________________________
dense_6 (Dense)              (None, 1)                 513       
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>接下来，我们需要一张输入图像，即一张猫的图像，它不属于网络的训练图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img_path = <span class="string">"data/cat_dog/cats_and_dogs_small/test/cats/cat.1700.jpg"</span></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">150</span>, <span class="number">150</span>))</span><br><span class="line">img_tensor = image.img_to_array(img)</span><br><span class="line">img_tensor = np.expand_dims(img_tensor, axis=<span class="number">0</span>)</span><br><span class="line">img_tensor /= <span class="number">255.</span></span><br><span class="line">print(img_tensor.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(1, 150, 150, 3)
</code></pre><p>我们来显示这张图像</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.imshow(img_tensor[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_80_0.png" width="400" height="400" alt="a" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<p>为了提取想要查看的特征图，我们需要创建一个<code>Keras</code>模型，以图像批量作为输入，并输出所有卷积层和池化层的激活。为此，我们需要使用<code>Keras</code>的<code>Model</code>类。模型实例化需要两个参数：一个输入张量（或输入张量的列表）和一个输出张量（或输出张量的列表）。得到的类是一个<code>Keras</code>模型，就像你熟悉的<code>Sequential</code>模型一样，将特定输入映射为特定输出。<code>Model</code>类允许模型有多个输出，这一点与<code>Sequential</code>模型不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用一个输入张量和一个输出张量列表将模型实例化</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line">layer_outputs = [layer.output <span class="keyword">for</span> layer <span class="keyword">in</span> model.layers[:<span class="number">8</span>]]</span><br><span class="line">activation_model = models.Model(inputs=model.input, outputs=layer_outputs)</span><br></pre></td></tr></table></figure>
<p>输入一张图像，这个模型将返回原始模型前8 层的激活值。这是第一次遇到的多输出模型，之前的模型都是只有一个输入和一个输出。一般情况下，模型可以有任意个输入和输出。这个模型有一个输入和8 个输出，即每层激活对应一个输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以预测模式运行模型</span></span><br><span class="line">activations = activation_model.predict(img_tensor)</span><br><span class="line"><span class="comment"># 例如，对于输入的猫图像，第一个卷积层的激活如下所示。</span></span><br><span class="line">first_layer_activation = activations[<span class="number">0</span>]</span><br><span class="line">print(first_layer_activation.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(1, 148, 148, 32)
</code></pre><p>它是大小为148×148的特征图，有32个通道。我们来绘制原始模型第一层激活的第4个通道</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将第4个通道可视化</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.matshow(first_layer_activation[<span class="number">0</span>, :, :, <span class="number">4</span>], cmap=<span class="string">'viridis'</span>)</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_86_1.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">通道4激活</div>
</center>


<p>这个通道似乎是对角边缘检测器。我们再看一下第7个通道</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将第7个通道可视化</span></span><br><span class="line">plt.matshow(first_layer_activation[<span class="number">0</span>, :, :, <span class="number">7</span>], cmap=<span class="string">'viridis'</span>)</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_88_1.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">通道7激活</div>
</center>



<p>下面我们来绘制网络中所有激活的完整可视化。我们需要在8个特征图中的每一个中提取并绘制每一个通道，然后将结果叠加在一个大的图像张量中，按通道并排。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将每个中间激活的所有通道可视化</span></span><br><span class="line">layer_names = []</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> model.layers[:<span class="number">8</span>]:</span><br><span class="line">    layer_names.append(layer.name)</span><br><span class="line">images_per_row = <span class="number">16</span></span><br><span class="line"><span class="keyword">for</span> layer_name, layer_activation <span class="keyword">in</span> zip(layer_names, activations):</span><br><span class="line">    n_features = layer_activation.shape[<span class="number">-1</span>]</span><br><span class="line">    size = layer_activation.shape[<span class="number">1</span>]</span><br><span class="line">    n_cols = n_features // images_per_row</span><br><span class="line">    display_grid = np.zeros((size * n_cols, images_per_row * size))</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> range(n_cols):</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> range(images_per_row):</span><br><span class="line">            channel_image = layer_activation[<span class="number">0</span>,:, :,col * images_per_row + row]</span><br><span class="line">            channel_image -= channel_image.mean()</span><br><span class="line">            channel_image /= channel_image.std()</span><br><span class="line">            channel_image *= <span class="number">64</span></span><br><span class="line">            channel_image += <span class="number">128</span></span><br><span class="line">            channel_image = np.clip(channel_image, <span class="number">0</span>, <span class="number">255</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">            display_grid[col * size : (col + <span class="number">1</span>) * size,row * size : (row + <span class="number">1</span>) * size] = channel_image</span><br><span class="line">    scale = <span class="number">1.</span> / size</span><br><span class="line">    plt.figure(figsize=(scale * display_grid.shape[<span class="number">1</span>],</span><br><span class="line">    scale * display_grid.shape[<span class="number">0</span>]))</span><br><span class="line">    plt.title(layer_name)</span><br><span class="line">    plt.grid(<span class="literal">False</span>)</span><br><span class="line">    plt.imshow(display_grid, aspect=<span class="string">'auto'</span>, cmap=<span class="string">'viridis'</span>)</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_90_1.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_90_2.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<center>
    <img src="\Pic\DeepLearning_Pic\output_90_3.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<center>
    <img src="\Pic\DeepLearning_Pic\output_90_4.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<center>
    <img src="\Pic\DeepLearning_Pic\output_90_5.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_90_6.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_90_7.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>

<center>
    <img src="\Pic\DeepLearning_Pic\output_90_8.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>



<p>这里需要注意以下几点。</p>
<ul>
<li>第一层是各种边缘探测器的集合。在这一阶段，激活几乎保留了原始图像中的所有信息。</li>
<li>随着层数的加深，激活变得越来越抽象，并且越来越难以直观地理解。它们开始表示更高层次的概念，比如“猫耳朵”和“猫眼睛”。层数越深，其表示中关于图像视觉内容的信息就越少，而关于类别的信息就越多。</li>
<li>激活的稀疏度（sparsity）随着层数的加深而增大。在第一层里，所有过滤器都被输入图像激活，但在后面的层里，越来越多的过滤器是空白的。也就是说，输入图像中找不到这些过滤器所编码的模式。</li>
</ul>
<p>我们刚刚揭示了深度神经网络学到的表示的一个重要普遍特征：随着层数的加深，层所提取的特征变得越来越抽象。更高的层激活包含关于特定输入的信息越来越少，而关于目标的信息越来越多（本例中即图像的类别：猫或狗）。深度神经网络可以有效地作为信息蒸馏管道（information distillation pipeline），输入原始数据（本例中是RGB 图像），反复对其进行变换，将无关信息过滤掉（比如图像的具体外观），并放大和细化有用的信息（比如图像的类别）。</p>
<p>这与人类和动物感知世界的方式类似：人类观察一个场景几秒钟后，可以记住其中有哪些抽象物体（比如自行车、树），但记不住这些物体的具体外观。事实上，如果你试着凭记忆画一辆普通自行车，那么很可能完全画不出真实的样子，虽然你一生中见过上千辆自行车。你可以现在就试着画一下，这个说法绝对是真实的。你的大脑已经学会将视觉输入完全抽象化，即将其转换为更高层次的视觉概念，同时过滤掉不相关的视觉细节，这使得大脑很难记住周围事物的外观。</p>
<center>
    <img src="\Pic\DeepLearning_Pic\bike.png" width="200" height="200" alt="（左图）试着凭记忆画一辆自行车；（右图）自行车示意图" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">（左图）试着凭记忆画一辆自行车；（右图）自行车示意图</div>
</center>

<h2 id="2-可视化卷积神经网络的过滤器"><a href="#2-可视化卷积神经网络的过滤器" class="headerlink" title="2. 可视化卷积神经网络的过滤器"></a>2. 可视化卷积神经网络的过滤器</h2><p>想要观察卷积神经网络学到的过滤器，另一种简单的方法是显示每个过滤器所响应的视觉模式。这可以通过在输入空间中进行梯度上升来实现：从空白输入图像开始，将梯度下降应用于卷积神经网络输入图像的值，其目的是让某个过滤器的响应最大化。得到的输入图像是选定过滤器具有最大响应的图像。</p>
<p>这个过程很简单：我们需要构建一个损失函数，其目的是让某个卷积层的某个过滤器的值最大化；然后，我们要使用随机梯度下降来调节输入图像的值，以便让这个激活值最大化。例如，对于在<code>ImageNet</code>上预训练的<code>VGG16</code>网络，其<code>block3_conv1</code>层第0 个过滤器激活的损失如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为过滤器的可视化定义损失张量</span></span><br><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> VGG16</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line">model = VGG16(weights=<span class="string">'imagenet'</span>,include_top=<span class="literal">False</span>)</span><br><span class="line">layer_name = <span class="string">'block3_conv1'</span></span><br><span class="line">filter_index = <span class="number">0</span></span><br><span class="line">layer_output = model.get_layer(layer_name).output</span><br><span class="line">loss = K.mean(layer_output[:, :, :, filter_index])</span><br></pre></td></tr></table></figure>
<p>为了实现梯度下降，我们需要得到损失相对于模型输入的梯度。为此，我们需要使用<code>Keras</code>的<code>backend</code>模块内置的<code>gradients</code>函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取损失相对于输入的梯度</span></span><br><span class="line">grads = K.gradients(loss, model.input)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>为了让梯度下降过程顺利进行，一个非显而易见的技巧是将梯度张量除以其L2范数（张量中所有值的平方的平均值的平方根）来标准化。这就确保了输入图像的更新大小始终位于相同的范围。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度标准化技巧</span></span><br><span class="line">grads /= (K.sqrt(K.mean(K.square(grads))) + <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>
<p>现在你需要一种方法：给定输入图像，它能够计算损失张量和梯度张量的值。你可以定义一个Keras后端函数来实现此方法：<code>iterate</code>是一个函数，它将一个Numpy张量（表示为长度为1的张量列表）转换为两个Numpy张量组成的列表，这两个张量分别是损失值和梯度值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 给定Numpy输入值，得到Numpy输出值</span></span><br><span class="line">iterate = K.function([model.input], [loss, grads])</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">loss_value, grads_value = iterate([np.zeros((<span class="number">1</span>, <span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>))])</span><br></pre></td></tr></table></figure>
<p>现在你可以定义一个Python 循环来进行随机梯度下降。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过随机梯度下降让损失最大化</span></span><br><span class="line">input_img_data = np.random.random((<span class="number">1</span>, <span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>)) * <span class="number">20</span> + <span class="number">128.</span></span><br><span class="line">step = <span class="number">1.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">40</span>):</span><br><span class="line">    loss_value, grads_value = iterate([input_img_data])</span><br><span class="line">    input_img_data += grads_value * step</span><br></pre></td></tr></table></figure>
<p>得到的图像张量是形状为<code>(1, 150, 150, 3)</code>的浮点数张量，其取值可能不是<code>[0, 255]</code>区间内的整数。因此，你需要对这个张量进行后处理，将其转换为可显示的图像。下面这个简单的实用函数可以做到这一点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将张量转换为有效图像的实用函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deprocess_image</span><span class="params">(x)</span>:</span></span><br><span class="line">    x -= x.mean()</span><br><span class="line">    x /= (x.std() + <span class="number">1e-5</span>)</span><br><span class="line">    x *= <span class="number">0.1</span></span><br><span class="line">    x += <span class="number">0.5</span></span><br><span class="line">    x = np.clip(x, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    x *= <span class="number">255</span></span><br><span class="line">    x = np.clip(x, <span class="number">0</span>, <span class="number">255</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>接下来，我们将上述代码片段放到一个Python函数中，输入一个层的名称和一个过滤器索引，它将返回一个有效的图像张量，表示能够将特定过滤器的激活最大化的模式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成过滤器可视化的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_pattern</span><span class="params">(layer_name, filter_index, size=<span class="number">150</span>)</span>:</span></span><br><span class="line">    layer_output = model.get_layer(layer_name).output</span><br><span class="line">    loss = K.mean(layer_output[:, :, :, filter_index])</span><br><span class="line">    grads = K.gradients(loss, model.input)[<span class="number">0</span>]</span><br><span class="line">    grads /= (K.sqrt(K.mean(K.square(grads))) + <span class="number">1e-5</span>)</span><br><span class="line">    iterate = K.function([model.input], [loss, grads])</span><br><span class="line">    input_img_data = np.random.random((<span class="number">1</span>, size, size, <span class="number">3</span>)) * <span class="number">20</span> + <span class="number">128.</span></span><br><span class="line">    step = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">40</span>):</span><br><span class="line">        loss_value, grads_value = iterate([input_img_data])</span><br><span class="line">        input_img_data += grads_value * step</span><br><span class="line">    img = input_img_data[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> deprocess_image(img)</span><br></pre></td></tr></table></figure>
<p>我们来试用一下这个函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(generate_pattern(<span class="string">'block3_conv1'</span>, <span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_106_1.png" width="400" height="400" alt="通道激活" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>



<p>看起来，<code>block3_conv1</code>层第0个过滤器响应的是波尔卡点（polka-dot）图案。下面来看有趣的部分：我们可以将每一层的每个过滤器都可视化。为了简单起见，我们只查看每一层的前64 个过滤器，并只查看每个卷积块的第一层（即<code>block1_conv1</code>、<code>block2_conv1</code>、<code>block3_conv1</code>、<code>block4_ conv1</code>、<code>block5_conv1</code>）。我们将输出放在一个8×8的网格中，每个网格是一个64像素×64像素的过滤器模式，两个过滤器模式之间留有一些黑边</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成某一层中所有过滤器响应模式组成的网格</span></span><br><span class="line">layer_name = <span class="string">'block1_conv1'</span></span><br><span class="line">size = <span class="number">64</span></span><br><span class="line">margin = <span class="number">5</span></span><br><span class="line">results = np.zeros((<span class="number">8</span> * size + <span class="number">7</span> * margin, <span class="number">8</span> * size + <span class="number">7</span> * margin, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">        filter_img = generate_pattern(layer_name, i + (j * <span class="number">8</span>), size=size)</span><br><span class="line">        horizontal_start = i * size + i * margin</span><br><span class="line">        horizontal_end = horizontal_start + size</span><br><span class="line">        vertical_start = j * size + j * margin</span><br><span class="line">        vertical_end = vertical_start + size</span><br><span class="line">        results[horizontal_start: horizontal_end, vertical_start: vertical_end, :] = filter_img</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">plt.imshow(results)</span><br></pre></td></tr></table></figure>
<p>这些过滤器可视化包含卷积神经网络的层如何观察世界的很多信息：卷积神经网络中每一层都学习一组过滤器，以便将其输入表示为过滤器的组合。这类似于傅里叶变换将信号分解为一组余弦函数的过程。随着层数的加深，卷积神经网络中的过滤器变得越来越复杂，越来越精细。</p>
<ul>
<li>模型第一层（block1_conv1）的过滤器对应简单的方向边缘和颜色（还有一些是彩色边缘）。</li>
<li>block2_conv1层的过滤器对应边缘和颜色组合而成的简单纹理。</li>
<li>更高层的过滤器类似于自然图像中的纹理：羽毛、眼睛、树叶等。</li>
</ul>
<h2 id="3-可视化类激活的热力图"><a href="#3-可视化类激活的热力图" class="headerlink" title="3. 可视化类激活的热力图"></a>3. 可视化类激活的热力图</h2><p>我还要介绍另一种可视化方法，它有助于了解一张图像的哪一部分让卷积神经网络做出了最终的分类决策。这有助于对卷积神经网络的决策过程进行调试，特别是出现分类错误的情况下。这种方法还可以定位图像中的特定目标。</p>
<p>这种通用的技术叫作类激活图（CAM，class activation map）可视化，它是指对输入图像生成类激活的热力图。类激活热力图是与特定输出类别相关的二维分数网格，对任何输入图像的每个位置都要进行计算，它表示每个位置对该类别的重要程度。举例来说，对于输入到猫狗分类卷积神经网络的一张图像，CAM 可视化可以生成类别“猫”的热力图，表示图像的各个部分与“猫”的相似程度，CAM 可视化也会生成类别“狗”的热力图，表示图像的各个部分与“狗”的相似程度。</p>
<p>我们将使用的具体实现方式是“Grad-CAM: visual explanations from deep networks via gradientbasedlocalization”a 这篇论文中描述的方法。这种方法非常简单：给定一张输入图像，对于一个卷积层的输出特征图，用类别相对于通道的梯度对这个特征图中的每个通道进行加权。直观上来看，理解这个技巧的一种方法是，你是用“每个通道对类别的重要程度”对“输入图像对不同通道的激活强度”的空间图进行加权，从而得到了“输入图像对类别的激活强度”的空间图。</p>
<p>我们再次使用预训练的<code>VGG16</code>网络来演示此方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载带有预训练权重的VGG16网络</span></span><br><span class="line"><span class="keyword">from</span> keras.applications.vgg16 <span class="keyword">import</span> VGG16</span><br><span class="line">model = VGG16(weights=<span class="string">'imagenet'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5
553467904/553467096 [==============================] - 441s 1us/step
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为VGG16模型预处理一张输入图像</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> keras.applications.vgg16 <span class="keyword">import</span> preprocess_input, decode_predictions</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img_path = <span class="string">'data/pic_input/elephant1.jpg'</span></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">x = image.img_to_array(img)</span><br><span class="line">x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line">x = preprocess_input(x)</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\elephant1.jpg" width="200" height="200" alt="非洲象" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">非洲象</div>
</center>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preds = model.predict(x)</span><br><span class="line">print(<span class="string">'Predicted:'</span>, decode_predictions(preds, top=<span class="number">3</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json
40960/35363 [==================================] - 0s 3us/step
Predicted: [(&#39;n02504458&#39;, &#39;African_elephant&#39;, 0.87728226), (&#39;n01871265&#39;, &#39;tusker&#39;, 0.11725453), (&#39;n02504013&#39;, &#39;Indian_elephant&#39;, 0.0054599163)]
</code></pre><p>对这张图像预测的前三个类别分别为：</p>
<ul>
<li>非洲象（African elephant，87.728226% 的概率）</li>
<li>长牙动物（tusker，11.725453% 的概率）</li>
<li>印度象（Indian elephant，0.54599163%的概率）</li>
</ul>
<p>网络识别出图像中包含数量不确定的非洲象。预测向量中被最大激活的元素是对应“非洲象”类别的元素，索引编号为386。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.argmax(preds[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>386
</code></pre><p>为了展示图像中哪些部分最像非洲象，我们来使用<code>Grad-CAM</code>算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">african_elephant_output = model.output[:, <span class="number">386</span>]</span><br><span class="line">last_conv_layer = model.get_layer(<span class="string">'block5_conv3'</span>)</span><br><span class="line">grads = K.gradients(african_elephant_output, last_conv_layer.output)[<span class="number">0</span>]</span><br><span class="line">pooled_grads = K.mean(grads, axis=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">iterate = K.function([model.input],[pooled_grads, last_conv_layer.output[<span class="number">0</span>]])</span><br><span class="line">pooled_grads_value, conv_layer_output_value = iterate([x])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">512</span>):</span><br><span class="line">    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]</span><br><span class="line">heatmap = np.mean(conv_layer_output_value, axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<p>为了便于可视化，我们还需要将热力图标准化到<code>0~1</code>范围内。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">heatmap = np.maximum(heatmap, <span class="number">0</span>)</span><br><span class="line">heatmap /= np.max(heatmap)</span><br><span class="line">plt.matshow(heatmap)</span><br></pre></td></tr></table></figure>
<center>
    <img src="\Pic\DeepLearning_Pic\output_120_1.png" width="400" height="400" alt="b" align="center">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>



<p>最后，我们可以用OpenCV 来生成一张图像，将原始图像叠加在刚刚得到的热力图上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">img = cv2.imread(img_path)</span><br><span class="line">heatmap = cv2.resize(heatmap, (img.shape[<span class="number">1</span>], img.shape[<span class="number">0</span>]))</span><br><span class="line">heatmap = np.uint8(<span class="number">255</span> * heatmap)</span><br><span class="line">heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)</span><br><span class="line">superimposed_img = heatmap * <span class="number">0.4</span> + img</span><br><span class="line">cv2.imwrite(<span class="string">'data/pic_output/elephant_cam.jpg'</span>, superimposed_img)</span><br></pre></td></tr></table></figure>
<pre><code>True
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cv2.imshow(<span class="string">'合并后的图像'</span>, superimposed_img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<pre><code>-1
</code></pre><h1 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h1><ul>
<li>卷积神经网络是解决视觉分类问题的最佳工具。</li>
<li>卷积神经网络通过学习模块化模式和概念的层次结构来表示视觉世界。</li>
<li>卷积神经网络学到的表示很容易可视化，卷积神经网络不是黑盒。</li>
<li>现在你能够从头开始训练自己的卷积神经网络来解决图像分类问题。</li>
<li>你知道了如何使用视觉数据增强来防止过拟合。</li>
<li>你知道了如何使用预训练的卷积神经网络进行特征提取与模型微调。</li>
<li>你可以将卷积神经网络学到的过滤器可视化，也可以将类激活热力图可视化。</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>版权声明： </strong>
          本博客所有文章除特别声明外，著作权归作者所有。转载请注明出处！
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://chenkai66.github.io/posts/1fc7d624.html" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
    </footer>
  </div>

   
  <nav class="article-nav">
    
    
      <a href="/posts/d8ee3b71.html" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Python深度学习（一）神经网络入门</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "TQy5bHTePagP10u5BBsesx61-gzGzoHsz",
    app_key: "O6UyJYxBFgMKQMjktBh4KGad",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020-2021
        <i class="ri-heart-fill heart_icon"></i> chenk
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        由 帅气的CK本尊 强力驱动
        
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="言念君子"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->


<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto'
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true
  }
</script>

<!-- Katex -->

<!-- busuanzi  -->


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->


<script src="/js/clickBoom2.js"></script>


<!-- CodeCopy -->


<link rel="stylesheet" href="/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>


<!-- CanvasBackground -->


<script src="/js/dz.js"></script>



    
  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"live2d-widget-model-haruto"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>